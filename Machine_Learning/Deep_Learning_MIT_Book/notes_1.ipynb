{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview \n",
    "This notebook is created to save notes from the famous ***deep-learning-adaptive-computation-and-machine-learning-series*** book and learnt the concepts long-term."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Humans have always sought creating machines (let's say non-human presences/existences) that can think: or probably simulate the human's intelligence with different species. It turned out the most computationally demanding tasks for humans are among the simplest for computers. The real challenge is to **teach** the computer how to execute tasks that are quite simple and intuitive to humans. Humans function in this world thanks to simple set of unwritten, intuitive, extremely hard to formalize rules. This knowledge as it is hard to formalize seems extremely difficult for a computer to acquire.  \n",
    "\n",
    "In the merge of the Artificial intelligence area, several attempts were made to formalize humans' knowledge, hard coding a set of rules and feeding in it to the computer in the hope that inference rules will enable the computers to infer more complex premises about the world. Nevertheless, none of these attempts led to any major successes.   \n",
    "\n",
    "Representation in computer science and life in general has played a major role in solving complex problems, as one encoding of a problem can simply make the problem so trivial while another can complicate it even further. Machine Learning algorithms can map the given features (the extraction process might generally be guided by domain's knowledge) to the desired output. ML models have no conceptual understanding of the given numbers: are they the dimensions, number of rooms in a hourse, are they the price of a certain stock for the last 24 hours. Thus, the success of a ML model hugely depends on a good set of features as well as human supervision. One possible solution is to design models that extract the features themselves from the very raw data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical foundataions\n",
    "## Linear Algebra\n",
    "I have done a thorough overview of Linear Algebra (can be found [here](https://github.com/ayhem18/Towards_Data_Science/tree/main/Mathematics/Linear%20Algebra)). So I won't probably be writing much here. Nevertheless, the book does a great job by linking the theoretical aspects to their practical implications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms\n",
    "a norm is a function that is generally associated with the length (size, magnitude) of a vector. A famous family of functions is the following:\n",
    "$\n",
    "\\begin{align}\n",
    "L^p = ||x||_p = (\\sum_i |x_i|^p ) ^ {\\frac{1}{p}}\n",
    "\\end{align}\n",
    "$\n",
    "The mathematical definition of a norm is a function satisfying 3 main properties:\n",
    "1. $f(x) = 0 \\implies x = 0$\n",
    "2. $f(x + y) = f(x) + f(y)$\n",
    "3. $\\forall \\alpha \\in \\mathbb{R}, ~ f(\\alpha \\cdot x) = |\\alpha| \\cdot f(x)$\n",
    "\n",
    "* The norm $L^2$ (p = 2) is known as the ***Euclidean norm***. The squared norm is more mathematically convenient to work with.\n",
    "* the square norm $L^2$ might no be desirable when the difference between zero elements and almost-zero elements should be emphasized. As $\\epsilon ^ 2$ is relatively small in comparison to $\\epsilon$ for values in the neighborhood of $\\epsilon$ (say $10 ^ {-5}$). $L^1$ is preferred in such scenarios as the loss function increases with $\\epsilon$ and not by its square.\n",
    "* Another important measure is $||A||_F = \\sqrt{\\sum_{i, j} A^2_{i, j}}$\n",
    "* and another point to keep in mind, the dot product can be seen as:\n",
    "$\\begin{align}\n",
    "x^T \\cdot y = ||x||\\cdot||y||\\cdot cos(\\theta) \n",
    "\\end{align}$  \n",
    "where $\\theta$ is the angle between $x$ and $y$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One significantly influential idea in mathematics is finding a universal representation properties that are representation independent. For instance the number $12$ might look $1100$ in the binary representation. Nevertheless, $12 = 2 ^ 2 \\cdot 3 $ is universally true regardless of the number's representation. Such universal property represents the fundamental fact on which several other properties about the number $12$ (and basically any other number). The same concept can brough up to the word of matrices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb0ce5cb6b092cde9f0ba713d915425207ed6ea08d3ede97530b87c251a3aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

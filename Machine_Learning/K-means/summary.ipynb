{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "## 1. K-Means algorithm\n",
    "### 1.1 Notation\n",
    "* $n$ represents the number of features\n",
    "* $m$ the training dataset size\n",
    "* $x_i$ a vector $\\in \\mathbb{R} ^ n$ representing the $i$-th training sample\n",
    "* $K$ represents the number of clusters (classes) for data classification.\n",
    "* $\\mu_k$ is a vector $\\in \\mathbb{R} ^ n$ representing a cluster's centroid: a point that determines a cluster\n",
    "* $c_i$ the index $\\in [1, 2, .. K]$ of the centroid for which $x_i$ is assigned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 algorithm\n",
    "The algorithm can be expressed through the following piece of [pseudo-code](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/K-means/K-means_algorithm.pdf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Optimization Objective\n",
    "Similarly to the supervised learning algorithm, ***K-means*** algorithm is associated with a cost function defined as follows: \n",
    "$\\begin{aligned}\n",
    "J(c_1, c_2,...c_m, \\mu_1, \\mu_2...,\\mu_k) = \\frac{1}{m} \\sum_{i=1}^{m} ||x_i - \\mu_{c_i}||^2\n",
    "\\end{aligned}$\n",
    "* For given centroids, the centroids assignment step minimizes $J$ with respect to $c_1, c_2,...c_m$. As we have $||x_i - \\mu_k||$ = $min_k ||x_i - \\mu_k||$ and thus the sum is minimal.\n",
    "* For given clusters, the move centroids step minimizes $J$ with respect to $\\mu_1, \\mu_2...,\\mu_k$. The mathematical proof can be found through [link](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/K-means/K-means_algorithm.pdf)\n",
    "* It is note worthy that the algorithm might get stuck in a local optimum. The right choice of initial centroids helps overcome such issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Random initialization\n",
    "The recommended approach would be as follows: \n",
    "* under the assumption that $K < m$, $K$ data points are randomly selected from the training dataset\n",
    "* set $\\mu_1, \\mu_2...,\\mu_k$ to these $K$ examples.\n",
    "* repeat the same steps a large number of time computing the cost function each.\n",
    "* choose the setting with the minimal cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 The choice of K\n",
    "There is no automatic method to find the ***\"right\"*** value of $K$ as there are several clustering possibilites.\n",
    "#### 1.5.1 The Elbow Method\n",
    "The main idea is to plot the variation of the cost function in terms of the number of iterations. The resulting graph might be similar to this [image](https://miro.medium.com/max/1400/1*8wV1j-klQA1xFvfaNXuVzg.png). In analogy with the human's body, the first point would represent the shoulder conjunction, while the last point represents the hands' fingers and the points just before it represent the arm. The chosen $K$ or the ***elbow*** point is the last point before the graph's slop starts to vary quite slowly.\\\n",
    "Yet, Such method is not of guaranteed efficiency as the graph might be similar to this [image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT2uoqcVncQ9w7azC2PdHdvwqIttDAeMoaqkw&usqp=CAU) where there are reasonable possible optimal values of $K$.\n",
    "#### 1.5.2 Other methods\n",
    "In a large number of cases, the criteria for choosing $K$ would depend on the application's purposes and later-use metrics."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e6cc62e2280f752aa7e5178eced3fb8e0cbe7333ea77893546e6db4cefb4290"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

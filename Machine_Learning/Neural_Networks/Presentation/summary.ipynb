{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Representation\n",
    "\n",
    "# 1. Motivation\n",
    "## 1.1 Are not Linear regression and Logistic Regression enough ?\n",
    "The Mechanisms discussed previously can be extended to build more complex models such as polynomial models. Yet, let's consider the following computational issue. In *Computer Vision*, one of the areas tightly related to ML, an image is internally represented as a $m * m$ matrix of color intensity pixels. for orginially, $n$ features, a quadratic model would require $C^2_n = \\frac{n(n-1)}{2}$, $O(n^2)$ new features as combincations of the original ones. A regular image would have $100 * 100 = 10^4$ pixels which means, a quadractic model would require approximately $ 5 * 10^7$ features which is computationally extremely expensive. Let alone a polynomial of degree $k$ would require $C^k_n \\approx n^k$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis Representation\n",
    "### 2.1 Brain Neuron\n",
    "Each neuron has a basic anatomical structure represented as follows:\n",
    "![The structure for a neuron](https://sciencetrends.com/wp-content/uploads/2019/05/neuron-700x376.png)\n",
    "\n",
    "The ***dendrites*** are generally referred to as ***input wires***. The axon is referred to as ***ouput wire*** and the ***nucleus*** is computational unit that transfers the input to output.\n",
    "\n",
    "### 2.2 Artificial Neural Network Representation\n",
    "Each Aritificial neural network constitutes of one ***input layer***, one ***output layer*** and an optional number of layers in between. Those are referred to as ***hidden layers***.\n",
    "\n",
    "Let's consider a simplistic example of a Neural Network with only 3 layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/Neural_Networks/Presentation/simple_NND.png?raw=true\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "  \n",
    "# get the image\n",
    "Image(url=\"https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/Neural_Networks/Presentation/simple_NND.png?raw=true\", width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--![Artificial NN](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/Neural_Networks/Presentation/simple_NND.png)--->\n",
    "This model can be expressed by this system of equations:\n",
    "\n",
    "$\n",
    "\\begin{equation}\\begin{aligned}a_1^{(2)} = g(\\Theta_{10}^1*x_0 + \\Theta_{11}^1*x_1 + \\Theta_{12}^1*x_2 + \\Theta_{13}^1*x_3)\\\\\n",
    "a_2^{(2)} = g(\\Theta_{20}^1*x_0 + \\Theta_{21}^1*x_1 + \\Theta_{22}^1*x_2 + \\Theta_{23}^1*x_3)\\\\\n",
    "a_3^{(2)} = g(\\Theta_{30}^1*x_0 + \\Theta_{31}^1*x_1 + \\Theta_{32}^1*x_2 + \\Theta_{33}^1*x_3)\\\\\n",
    "h_{\\Theta} = a_1^3 = g(\\Theta_{10}^2*a_0^{(2)} + \\Theta_{11}^2*a_1^{(2)} + \\Theta_{12}^2*a_2^{(2)} + \\Theta_{13}^2*a_3^{(2)}) \\end{aligned}\\end{equation}$ \n",
    "The system above can be rewritten in matrix form as follows:\n",
    "$\\begin{equation}\\begin{bmatrix}\n",
    "a_1^{(2)}\\\\\n",
    "a_2^{(2)} \\\\\n",
    "a_3^{(2)}\\end{bmatrix}\n",
    "=  g (\\begin{bmatrix}\n",
    "\\Theta_{10}^{(1)} && \\Theta_{11}^{(1)} && \\Theta_{12}^{(1)} && \\Theta_{13}^{(1)} \\\\\n",
    "\\Theta_{20}^{(1)} && \\Theta_{21}^{(1)} && \\Theta_{22}^{(1)} && \\Theta_{23}^{(1)} \\\\\n",
    "\\Theta_{30}^{(1)} && \\Theta_{31}^{(1)} && \\Theta_{32}^{(1)} && \\Theta_{33}^{(1)}\n",
    "\\end{bmatrix}*\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix})\\end{equation}$\n",
    "More generally\n",
    "$\n",
    "S_{i+1} = g(\\Theta^i * S_{i}) \n",
    "$\n",
    "where\n",
    "* g is the sigmoid function\n",
    "* $S_i$: the vector of values generated in the $i$-th layer with length $s_{i}$ \n",
    "* $S_{i+1}$: vector of values generated in the $(i+1)$-th layer with length $s_{i + 1}$ \n",
    "* $\\Theta^{(i)}$ is the matrix mapping the input of the $i$-th layer to the $(i+1)$-th one.\n",
    "* $\\Theta^{(i)}$ is of dimensions $s_{i + 1} * (s_{i} + 1)$  \n",
    "* a value $S^i_0$ is added to each layer and is denoted as the ***bias unit*** and generally equal to $1$.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e6cc62e2280f752aa7e5178eced3fb8e0cbe7333ea77893546e6db4cefb4290"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

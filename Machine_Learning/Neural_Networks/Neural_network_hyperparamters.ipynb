{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving a Neural Net: Hyperparameters Tuning\n",
    "## 1. Train, dev, test sets\n",
    "### 1.1 Neural Net: Hyperparameters:\n",
    "For a Neural network, there are certain values that affect the performance, yet do not change during the model's training:\n",
    "1. number of hidden layers\n",
    "2. number of units in each layer\n",
    "3. activation functions\n",
    "4. learning rate\n",
    "5. convergence criteria\n",
    "### 1.2 distribution\n",
    "For relatively small $m \\leq 10000$, a suitable divison's rule of thumb would be: $60\\%$, $20\\%$ and $20\\%$ for train, dev (CV) and test sets respectively. Yet, as the main goal of dev set is to determine the most efficient combination of hyperparameters, there is no need for a large training set. Hence, a suggested distribution when $m \\approx 10 ^ 6$ or larger is: $98\\%$, $1\\%$ and $1\\%$.\n",
    "Additionally, it is of extreme importance to have all data sets with the same distribution, or the same quality, speaking more informally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias and Variance\n",
    "### 2.1  Recall:\n",
    "The following [notebook](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/generalities/evaluating_ML_algorithm.ipynb) provides a basic introduction to the notions of ***bias*** and ***variance*** along with additional general tools such as ***learning curves***.\n",
    "### 2.2 Bias and Variance for Neural Net:\n",
    "A Neural Net's low performance can be tackled as follows: \n",
    "1. Consider the high bias possibility (poor performance of training dataset): if it is the case, then:\n",
    "    * bigger network: larger number of hidden units per layer\n",
    "    * deeper network: larger number of hidden layers\n",
    "    * consider a different Neural network architechture\n",
    "2. Consider the high Variance possibility (high performance with train set and poor one with CV set):\n",
    "    * more training data \n",
    "    * regularization\n",
    "    * consider a different Neural network architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Regularization methods\n",
    "#### 2.3.1 L2 method\n",
    "This method is discussed in the following [notebook](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/generalities/evaluating_ML_algorithm.ipynb)\n",
    "#### 2.3.2 DropOut method\n",
    "The main idea is to eliminate certain hidden units in every layer with a certain probability. More formally, assigning $0$ to certain hidden units with a propability $p$. $p$ can vary from one layer to another. Such procedure decreases the weighs as well as distributes them along the different units in that layer.\n",
    "The procedure can be expressed as follows for a say the layer $l$:\n",
    "1. dl = (np.random.rand(al.shape) $< p$): creates a boolean array refleting the inequalities\n",
    "2. al = dl * al (assigning the value $0$ to every hidden unit corresponding to a value larger than $p$ in dl)\n",
    "3. a3 /= p\n",
    "\n",
    "As for the last step, on average (1 - p) of the elements will be set to $0$. Thus, the expected value of all units will be multiplied by $p$. To have the original expected value, we need to divide by $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Early Stopping\n",
    "This method is slightly different from the previously mentioned techniques as it tackles both training the model and overfitting. \n",
    "Assuming the model is not underfitting the train dataset, then the cost function over training data $J_t$ and dev data $J_d$ with respect to number of iterations can have similar graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.30258509  1.93507413  1.66692336 ... -4.38091568 -4.38147131\n",
      " -4.38202663]\n",
      "[ 2.67138538  2.30428414  2.03654355 ... -1.66866358 -1.66620602\n",
      " -1.66374481]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 200 linearly spaced numbers\n",
    "\n",
    "x = np.linspace(1, 800,1800)\n",
    "# x.reshape((1, x.shape[0]))\n",
    "# the sigmoid function\n",
    "J_t = - np.log(x / 10)\n",
    "stopping_point = 400\n",
    "J_d = - np.log(x / 10) + np.exp(np.fmax((x - stopping_point) / stopping_point, np.array([-1])))\n",
    "print(J_t)\n",
    "print(J_d)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(-5, 3)\n",
    "ax.plot(x, J_t , 'b', label='J_t')\n",
    "ax.plot(x, J_d , 'r', label='J_d')\n",
    "leg = ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ps: The functions are chosen for their similarities to the graphs of the actual $J_t$ and $J_d$ functions. No mathematical justification.\n",
    "\n",
    "The main idea is to choose the value where the difference between the two error functions is neither too small nor too large. Such a method might not be optimal as it tries to solve two different issues simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialization to solve vanishing and exploding gradients\n",
    "For deep Neural networks, it is quite possible for the weights to increase and decrease exponentially. Therefore, it slows significantly the learning algorithm. Such a situation can be partially addressed with a careful random initilization:\n",
    "Consider the following single neuron: \n",
    "\n",
    "![single neuron](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/Neural_Networks/single_neuron.png?raw=true)\n",
    "\n",
    "$\\begin{equation}\n",
    "z = w_1 \\cdot x_1 + w_2 \\cdot x_2 ... + w_n\\cdot x_n \n",
    "\\end{equation}$\n",
    "for large values of $n$, it is preferrable for $w_i$ to be small enough not to make $z$ explode and not too small to make it vanish.\n",
    "Thus, variance should be set to \n",
    "$\\begin{equation}\\sigma ^ 2 = \\frac{c}{n}\\end{equation}$\n",
    "where $c$ depends on the activation function associated with the hidden layer. \n",
    "* for Relu, c = 2\n",
    "* for tanh, c = 1\n",
    "\n",
    "The final initialization would be\n",
    "$\\begin{equation}W^{([l])} = np.random.rand(shape) * \\sqrt(\\frac{c}{n^{[l - 1]}})\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Hyperparameters Tuning \n",
    "### 4.1 Methadology\n",
    "Previously, Deep learning practitioners used grid methadology. For $n$ hyperparameters, the engineer in question, would prepare $n$ sets of values. $[a_{11}, a_{12}...a_{1m}]$ where each set reprents possible values for a specific hyperparameter. The engineer experiments with all the combinations.\n",
    "\n",
    "Such approach might no longer be optimal due to the increased number of parameters and the difference in their importance. Alternatively, It is recommended to try random values within the predefined ranges (not sets) for each of the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Coarse to fine\n",
    "Assuming a number of combinations proved to be more efficient than others, it might be worth the time to reduce the search space and retry the same proceedure in the region that acheived satisfying results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Scaling hyperparameters\n",
    "It is note worthy that not all hyperparameters vary within the same range. Some vary from [0.0001 ... 1]. Others from [2..100]. A random linear distribution would be perfectly fine on the second range. Yet, it is not the case for the second one.\n",
    "\n",
    "Hyperparameters in ranges like [0.0001 ... 1] tend to be very insensitive towards end points. In other words even differences estimated in the vecenity of $10^{-4}$ can significantly affect the algorithm's performace. When using a linear random distribution: $10\\%$ of the values would assigned to the range [0.0001 ... 0.001] while the remaining $90 \\%$ would be distributed on the rest of the interval. This is might be harmful for the application taking into consideration that both ranges are of equivalent importance. A more adequate appoach on ranges $[10^{-a} ... 10^{-b}]$ is to consider the random variable $x$ distributed on $[b, a]$ and then consider the values $10 ^ {-x}$ for the hyperparameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08488e93894ea7be7272109919d40edb52233f14daf834f5f2387122a81730e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

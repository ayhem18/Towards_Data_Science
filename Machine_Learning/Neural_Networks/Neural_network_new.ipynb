{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "## 1. Notation\n",
    "The following notation will be used throughout the notebook:\n",
    "* L: number of non-input layers in the network. The input layer will denoted as the $0$ zero layer\n",
    "* $n^{[i]}$ the number of hidden units in the $i$-th layer\n",
    "* $z^{[i]}$ the weighted sum vector in the $i$-th layer\n",
    "* $a^{[i]}$ the activations vector in the $i$-th layer\n",
    "* $w^{[i]}$ the weights used to compute the $i$-th layer\n",
    "* $b^{[i]}$ the bias added in the $i$-th layer\n",
    "* $g^{[i]}$ the activation function used in the $i$-th layer\n",
    "* $a^{[0](i)}$ =  $x^{(i)}$\n",
    "* $n^{[0]}$ = $n$: the number of features.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FeedForward Propagation\n",
    "### 2.1 For a single training sample\n",
    "Given a training sample $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .. \\\\ .. \\\\ x_n\\end{bmatrix}$ it is possible to compute the associated output $\\hat{y}$ as follows:\n",
    "1. $a^{[0]}$ = $x$\n",
    "2. for $l = 1, 2, .. L$\n",
    "$\\begin{equation} \\begin{aligned} \n",
    "z^{[l]} = w^{[l]} \\cdot a^{[l - 1]} + b^{[l]} \\\\\n",
    "a^{[l]} = g^{[l]} (z^{[l]})\n",
    "\\end{aligned}\\end{equation}$\n",
    "3. $\\hat{y} = a^{[L]}$\n",
    "\n",
    "### 2.2 For the entire training dataset\n",
    "This section will extend (or vectorize) the algorithm to apply it for dataset $X$. Hence, the new notation:\n",
    "* $Z^{[i]}$ = $\\begin{bmatrix} \n",
    "z^{[i](1)} && z^{[i](2)} && .. && .. && z^{[i](m)}\n",
    "\\end{bmatrix}$\n",
    "* $A^{[i]}$ = $\\begin{bmatrix} \n",
    "a^{[i](1)} && a^{[i](2)} && .. && .. && a^{[i](m)}\n",
    "\\end{bmatrix}$\n",
    "* $W^{[i]}$ = $\\begin{bmatrix} \n",
    "w^{[i](1)} && w^{[i](2)} && .. && .. && w^{[i](m)}\n",
    "\\end{bmatrix}$\n",
    "* $B^{[i]}$ = $\\begin{bmatrix} \n",
    "b^{[i](1)} && b^{[i](2)} && .. && .. && b^{[i](m)}\n",
    "\\end{bmatrix}$.\n",
    "The final vectorized version would be:\n",
    "1. $A^{[0]}$ = $X$\n",
    "2. for $l = 1, 2, .. L$\n",
    "$\\begin{equation} \\begin{aligned} \n",
    "Z^{[l]} = W^{[l]} \\cdot A^{[l - 1]} + B^{[l]} \\\\\n",
    "A^{[l]} = g^{[l]} (Z^{[l]})\n",
    "\\end{aligned}\\end{equation}$\n",
    "3. $\\hat{Y} = A^{[L]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backward Propagation\n",
    "Due to the hyper complex nature of the mathematics involved, only the final algorithm will be provided as a part of the code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08488e93894ea7be7272109919d40edb52233f14daf834f5f2387122a81730e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

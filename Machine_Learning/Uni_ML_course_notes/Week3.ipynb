{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week3\n",
    "This notebook is created to track important notes on the material discussed in the 3rd week of the ML course at my university.\n",
    "\n",
    "* $F_1$ score is a more reliable metric mainly for class-unbalanced datasets where one class is much heavily represented than others: it can be interprested as teh model's ability to capture both postive and negative classes.\n",
    "\n",
    "\n",
    "* The effect of moving the threshold in  Logistic Regression:\n",
    "* One versus All: consider $n$ binary classificaiton problems where at each problem we consider the class $i$ and other classes as not $i$. \n",
    "* One versus One: take all $C^{2}_{n}$ combincations and make a hyperplane for each of them. The final target is determined by some aggregation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* any preprocessing module should be run only on the training dataset. because in real world situations, we do not have access to the test dataset.\n",
    "* Precision, recall are used as performance metrics depending mainly on the model's usage. If we need to be sure about the negative class, then we use recall, if we need to sure about the positive class, it makes sense to use precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Lasso: a lot of features and most of them are irrelevant\n",
    "* Ridge: most of them are good features, however, each of them might contribute a little bit to the target value.\n",
    "\n",
    "* Lasso is less stable regualizer as with different splitting of the same datasets, one of two correlated features might be dropped (but not consistently)\n",
    "* Ridge is more stable as it decreases the norm of correlated features together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* naive Bayes pros: categorial works, online learning, interpretability\n",
    "* naive Bayes cons: independence assumption, zero frequency problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "1. SVM is less affected by a small dataset, and umbalenced class distribution.\n",
    "2. the points that define the margin are called support vectors.\n",
    "3. with rbf: the higher the gamma value, the more likely the model is to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "* feature learning is better than hand crafted features: they require domain expertise and are time-consuming.\n",
    "* features are generally represented hierarchly. In other words, features are built on top of each other to make more complex, composed features\n",
    "* the significance and meaning of complex features is a direct result of non-linear relations between feature levels.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

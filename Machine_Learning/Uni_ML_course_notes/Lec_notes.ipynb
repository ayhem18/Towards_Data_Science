{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Machine Learning algorithms are computer programs that improve performance at some task through experience.\n",
    "* The Experience presented in form of datasets and performance measured through a performance metric usually tailored to the problem at hand.\n",
    "* given $x_0, y_0$ an observation from the test datasets, we can evaluate the error of the prediction as follows:\n",
    "    $\\begin{align}E(y_0 - \\hat f (x_0))^2 = Var(\\hat f (x_0)) + Bias(\\hat f (x_0))^2 + Var(\\epsilon)\\end{align}$\n",
    "where $Var(\\epsilon)$ represents the irreducible error.\n",
    "As Variance and Bias and inheritedly negatively correlated, the best result that can be achieved is generally a tradeoff between both quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2\n",
    "The Linear Regression model is a simple, yet powerful model that can be used as a step towards more complicated and complex models. Given a dataset:\n",
    "$\\begin{align} \n",
    "X = \\begin{bmatrix} \n",
    "1 & x_{11} & x_{12} & ... & x_{1n} \\\\\n",
    "1 & x_{21} & x_{22} & ... & x_{2n} \\\\\n",
    "1 &  .. & .. & ... & .. \\\\\n",
    "1 &  x_{m1} & x_{m2} & ... & x_{mn}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$\n",
    "with a set of predictors\n",
    "$\\begin{align} Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ .. \\\\ y_m\\end{bmatrix}\\end{align}$\n",
    "and denoting \n",
    "$\\begin{align} \n",
    "\\Theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ .. \\\\ \\theta_n \\end{bmatrix}\n",
    "\\end{align}$\n",
    "The best coefficients: minimizing the squared error are expressed as:\n",
    "$\\begin{align} \n",
    "\\Theta = (X^T \\cdot X) ^ {-1} \\cdot X^T \\cdot Y \n",
    "\\end{align}$\n",
    "\n",
    "Assuming the relation between the predictors and the target variable is not linear, the same linear model can be fed new features: polynomial combinations of the initial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing the sigmoid function introduces non-linearity as well as projects the whole Real line to the interval $[0, 1]$. This extends the linear regrssion to solve classification problems (with the help of treshhold). The non-linearity introduced by sigmoid, makes **MSE** non-convex function. A new convex function (convexity can be proved) was introduced to ensure better results in general:\n",
    "$\\begin{align}\n",
    "L(w) = -\\frac{1}{n} \\sum_{i=1}^n y^i \\log(p(x^i)) + (1 - y^i) \\cdot (\\log(1 - p(x^i))) \n",
    "\\end{align}$\n",
    "where $p(x^i) = \\frac{1}{1 + e^{-w^{T}\\cdot x^i}}$\n",
    "\n",
    "Solving the system of equations written in vector form:\n",
    "$\\begin{align}\n",
    "    \\frac{\\delta L}{\\delta \\Theta} = 0\n",
    "\\end{align}$\n",
    "Thus we use numerical optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Bayes Classifier\n",
    "Naive Bayes classifier is a classifier built upon the infamous Bayes Equation:\n",
    "$\\begin{align} P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\end{align}$\n",
    "Assuming a dataset represented as $X$, a set of classes $C$, and a set of labels denoted by $Y$, and given a new data point $X_{new}$.\n",
    "$\\begin{align}\n",
    "p(y_{new} = c_i|x_{new}) = \\frac{p(x_{new} | c) \\cdot p(c_i)}{p(x_{new})}\n",
    "\\end{align}$\n",
    "Before expanding the formula further. Let's recall the Bayes' rule: if the universal set can be divided into $k$ disjoint events $H_1, H_2, ..., H_k$\n",
    "$\\begin{align}\n",
    "P(H_i | A) = \\frac{P(A|H_i) \\cdot P(H_i)}{\\sum_{j=1}^{n} P(A|H_j) \\cdot P(H_j)}\n",
    "\\end {align}$\n",
    "In the case of the Bayes's classifier, the partition is $H_i: y_{new} = c_i$ as all classes are disjoints. Thus,\n",
    "$\\begin{align}\n",
    "p(y_{new} = c_i|x_{new}) = \\frac{p(x_{new} | c_i) \\cdot p(c_i)}{\\sum_{j=1}^{k}p(x_{new} | c_j) \\cdot p(c_j)}\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive ? Bayes\n",
    "The naivety in the classifier is explained by the following assumption:\n",
    "<p style=\"text-align: center;\">The features are independent</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the probability:\n",
    "$\\begin{align}\n",
    "p(x | c) = p(x_1, x_2, ..., x_n|c) = \\prod_{i=1}^n p(x_i|c)\n",
    "\\end{align}$\n",
    "$p(x_i|c)$ represents a simple quantity to compute. \n",
    "1. The class with the largest probability is the one assigned to the new instance.  \n",
    "2. It is necessary to note that features are generally considered categorial. Yet, by choosing a certain hyperparameter, the continous, numerical features can be divided into ranges and the resulting column is practically a categorical feature.\n",
    "3. Underflow technical issue, with a large number of classes and / or features, the probabilities can get pretty low. As the $\\log$ function is increasing, we consider the logarithm of probabilities and the products turn to sum reducing the issues raised by float-point numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons:\n",
    "### Ads\n",
    "1. Naive Bayes represents a fast and easy way to predict new test data. Its performance can be easily extended to multi class issues.\n",
    "2. With a data where the assumption of independe indeed holds, the performance can be quite high.\n",
    "3. Allows online learning: where the data does not need to saved at memory. The classifier can predict based on a stream of incoming data and not necessarily a static set.\n",
    "### DisAds\n",
    "1. Most often, the independence assumption does not hold and thus the model is relatively too simple for the problem at hand\n",
    "2. a first time encountered predictor will be assigned $0$ which requires smoothing techniques are required. Among the most famous techniques is the **Laplace Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "Given the initial dataset $X$, a model will classify a certain data point depending on the classes of the $k$ nearest / most similar points from the dataset.\n",
    "The model is simple as the data is indeed the model. Given a new data point $X_{new}$, we consider a similarity measure $f$, each point in the training dataset is considered to find the nearest $k$ points. The new class is the majority vote among the classes of the neighbors of the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and cons\n",
    "### Pros\n",
    "1. Intuitive, easy to understand and interpret and can capture quite complex non-linear interactions within the data.\n",
    "2. Can be easily extended to Regression by again finding the nearest $k$ points and considering a statistics of these $k$ values: median, mean, weighted mean..\n",
    "### Cons\n",
    "1. At its basic level implementation, the test time is generally too expensive\n",
    "    * assuming $n$ points in the training dataset and $d$ dimensions\n",
    "    * computing d-dimensional distance for $n$ points: $O(n\\cdot d)$\n",
    "    * sorting the distances: $O(n\\cdot \\log(n))$\n",
    "2. data management (storage) is needed.\n",
    "3. More advanced implementations are needed.\n",
    "4. normalizing and scaling data is necessary.\n",
    "\n",
    "## Impact of $K$\n",
    "1. small $K$:\n",
    "    * catpure relations better\n",
    "    * higher risk of overfitting\n",
    "2. Larger $k$:\n",
    "    * more stable, yet not as refined\n",
    "    * higher risk of underfitting\n",
    "The best value of $k$ should be obtained by cross validation techniques. A general rule of thumb set by researchers\n",
    "$\\begin{align} \n",
    "k < \\sqrt(n)\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "With the right values of $\\lambda$, regularization can help tackle overfitting. The main techniques are: 'l1' and 'l2'.\n",
    "## l1\n",
    "* The cost function can be written in general as:\n",
    "$\\begin{align}\n",
    "L = cost + \\lambda \\sum_{i=1}^{n} |w_i|\n",
    "\\end{align}$\n",
    "* L1 Linear Regression is known as Lasso Regression\n",
    "* L1 can be also through of a feature selection mechanism as $L1$ sets features of low relevance/contribution to $0$ which results in sparse models\n",
    "\n",
    "## l2\n",
    "* The cost function can be written in general as:\n",
    "$\\begin{align}\n",
    "L = cost + \\lambda \\sum_{i=1}^{n} w_i ^ 2\n",
    "\\end{align}$\n",
    "* L2 Linear Regression is known as Ridge Regression\n",
    "* L2, unlike L1, sets the values of low-relevance features to low values and not necessarily zero. This is generally done when we are sure that most of our features are indeed relevant (to a certain extent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization problems are generally associated with constraints. Assuming a function $f$ that accepts $w$ as arguments.\n",
    "Then the optimization problem is generally formulated as:\n",
    "find \n",
    "$\\begin{align} argmin f(w), ~ g(w) < a\n",
    "\\end{align}$\n",
    "which is converted using Lagrange mutlipliers to:\n",
    "$\\begin{align} \n",
    "f(w) - \\alpha \\cdot (g(w) - a)\n",
    "\\end{align}$ \n",
    "with $\\alpha > 0$\n",
    "The shape of the function $|w| < s$ represents a square. A square intersecting another function generates sparse values. While the expression $w^2 < s$ generates a disque which generates small values when intersecting another function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA: Principle Components Analysis is one of the most popular dimensions reduction techniques. The mathematics behind it is not too complicated and can be found in most machine learning books / courses. As for the number of dimensions, it can be chosen either based on the task at hand: for visualization purposes, a $d=3$ would be the maximum value. As for general training, we can consider the projection as a hyperparameter determined by cross validation, or using the eigen-spectrum: The sum of values of the chosen eigenvalues on the sum of all eigenvalues represented how much variance is perserved out of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correlation is a mathematical quantity that can express how tight is the relation between two random variable generally or in our case two features. Correlation might indicate a casual relationship. Basic quantity in modelling techniques.\n",
    "* Multicollinearity means that there a certain number of features that highly correlated to each other. In other words, knowing one of them gives us a fair or a clear understandin of the rest of the features which means that with only one feature we can expect similar performance as with all of them. Reducing dimensionality generally leads to a boost in training speed, simplicity and interpretability: a better generalization to new unseen data.\n",
    "* Pearson's measure the linear correlation between two random variables. it a mathematical quantity between -1 and 1. The higher the correlation in absolute value the better. If it is positive, it means that the data changes in the same direction, if it is negeative then it changes in different / opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

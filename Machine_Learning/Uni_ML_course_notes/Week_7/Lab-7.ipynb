{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab-7: ANN in Pytorch\n",
    "\n",
    "In this lab, you will practice simple deep learning model in Pytorch.\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "1. Theoretical issues\n",
    "2. Get starting in Pytorch\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Theoretical issues\n",
    "Ordinary fully connected neural nets consists of Dense layers, activations, and output layer.\n",
    "\n",
    "1. What's the difference between deep learning and normal machine learning?\n",
    "2. How does a neural network with no hidden layers and one output neuron compare to a logistic/linear regression?\n",
    "3. How does a neural network with multiple hidden layers but with linear activation and one output neuron compared to logistic/linear regression?\n",
    "4. Can the perceptron find a non-linear decision boundary?\n",
    "5. In multi-hidden layers network, what's the need of non-linear activation function?\n",
    "6. Is random weight assignment better than assigning same weights to the units in the hidden layer. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch: Getting started \n",
    "\n",
    "### Feed Forward Neural Network\n",
    "An artificial neural network wherein connections between the nodes do not form a cycle.\n",
    "It consists of : \n",
    "\n",
    "- Input Layer  \n",
    "- Hidden Layer(s)\n",
    "- Output Layer\n",
    "\n",
    "![alt text](https://images.deepai.org/django-summernote/2019-06-06/5c17d9c2-0ad4-474c-be8d-d6ae9b094e74.png)\n",
    "\n",
    "\n",
    "The neural network contents units, typically called \"neurons\".  Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.\n",
    "\n",
    "<center>\n",
    "<img src=\"./assets/simple_neuron.png\" alt=\"drawing\" style=\"width:400px;\"/> \n",
    "</center>\n",
    "\n",
    "Mathematically this equivalent to:\n",
    "<center>\n",
    "\n",
    "$ \n",
    " y = f(w_1x_1 + w_2x_2 + b)\n",
    "$\n",
    "\n",
    "$ \n",
    " y = f\\left(\\sum_{i=1}^{N} w_ix_i + b\\right)\n",
    "$\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tensors\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on tensors, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    " \n",
    "\n",
    "<img src=\"./assets/tensor_examples.svg\" width=\"600px\">\n",
    "\n",
    "Just like Numpy arrays, Pytorch tensors can be added, multiplied, subtracted, etc.\n",
    "\n",
    "#### A simple neuron with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate data we need to compute the output of the neuron. We have `5` input features, just random for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task**: Calculate the output of the network with input features `features`, weights `weights`, and bias `bias`. Similar to Numpy, PyTorch has a [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) function, as well as a `.sum()` method on tensors, for taking sums. Use the function `activation` defined above as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the multiplication and sum in the same operation using a matrix multiplication. In general, you'll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs.\n",
    "\n",
    "We can perform the same operation using matrice multiplication. Useuse [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) which is somewhat more complicated and supports broadcasting. If we try to do it with `features` and `weights` as they are.\n",
    "\n",
    "**Note** When doing matrice multiplication consider reshaping matrix in correct shape to avoid error. \n",
    "\n",
    "There are a few options here: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
    "\n",
    "> **Task**:  Compute the output of the single unit network using matrix multiplicatiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi layer network\n",
    "\n",
    "We saw how to compute the output of single unit network. The power of neural networks come when multiple units into layers. \n",
    "The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix.\n",
    "\n",
    "<img src='./assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "The first layer shown on the bottom here are the inputs, understandably called the **input layer**. The middle layer is called the **hidden layer**, and the final layer (on the right) is the **output layer**. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))\n",
    "\n",
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** Calculate and print the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. The correct value should be `tensor([[0.3171]])` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Autograd\n",
    "\n",
    "Automatic differentiation package : **No need to worry about back propagation partial derivatives and chain rule** \n",
    "\n",
    "Tensors track their computational history and support gradient computation\n",
    "\n",
    "`requires_grad=True` : Tells PyTorch that we want to compute gradients for the specific tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "print(x.grad) # no gradient at the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward()` function is responsible for calculation of gradients and accumulate (not apply) them in respective tensors\n",
    "\n",
    "The tensor with `requires_grad=True`: has attribute to check the gradients values : `grad`\n",
    "\n",
    "Let's consider a function of x. \n",
    "$f(x) = x^2 + 2x + 1$\n",
    "\n",
    ">**Question** what is the gradient at the point $x=5$\n",
    "\n",
    "The following code will compute and accumulate the gradient w.r.t $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<AddBackward0>) True\n",
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "# we compute some function f(x) = x^2 + 2x + 1\n",
    "z = x ** 2 + 2*x + 1\n",
    "\n",
    "print(z, z.requires_grad)\n",
    "\n",
    "z.backward() # compute and propagate the gradient\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the previous cell several time and see how the value of the gradient changes\n",
    "\n",
    "**Note**: Because the gradient is accumulated everytime you call `backward()` it is important to zero the accumulated values before any calculations, i.e., `x.grad = None` or `zero_grad()` for optimizers.\n",
    "\n",
    "\n",
    "\n",
    "To stop PyTorch from tracking the history and forming the backward graph, the code can be wrapped inside with torch.no_grad(): It will make the code run faster whenever gradient tracking is not needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.) False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # we compute some function f(x) = x^2 + 2x + 1\n",
    "    z = x ** 2 + 2*x + 1\n",
    "\n",
    "    print(z, z.requires_grad)\n",
    "\n",
    "    # z.backward()  will trigger an eoor no gradient is tracked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward graph is created automatically and dynamically by autograd class during forward pass. Backward() simply calculates the gradients by passing its argument to the already made backward graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t x =  tensor([0., 5., 3.])\n",
      "Gradient w.r.t y =  tensor([ 1.,  2., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 10.0], requires_grad = True)\n",
    "y = torch.tensor([0.0, 5.0, 3], requires_grad = True)\n",
    "\n",
    "z = x * y\n",
    "\n",
    "z.backward(torch.tensor([1.0, 1.0, 1.0])) # compute the gradient usinng an external gradient\n",
    "print(\"Gradient w.r.t x = \", x.grad)\n",
    "print(\"Gradient w.r.t y = \", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor to numpy array and vice-versa\n",
    "\n",
    "PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54588847, 0.16549643, 0.5081551 ],\n",
       "       [0.52564826, 0.09550564, 0.42998682],\n",
       "       [0.77517588, 0.80700138, 0.21719122],\n",
       "       [0.33303698, 0.09066601, 0.5987956 ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5459, 0.1655, 0.5082],\n",
       "        [0.5256, 0.0955, 0.4300],\n",
       "        [0.7752, 0.8070, 0.2172],\n",
       "        [0.3330, 0.0907, 0.5988]], dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54588847, 0.16549643, 0.5081551 ],\n",
       "       [0.52564826, 0.09550564, 0.42998682],\n",
       "       [0.77517588, 0.80700138, 0.21719122],\n",
       "       [0.33303698, 0.09066601, 0.5987956 ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Model Design in Pytorch\n",
    "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we'll use the MNIST dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels, you can see a sample below.:\n",
    " \n",
    "<img src=\"./assets/mnist.png\" width=\"500px\"> \n",
    "\n",
    "Our goal is to build a neural network that can take one of these images and predict the digit in the image.\n",
    "\n",
    "we have three simple parts that we need to build:\n",
    "1. Data Loading process.\n",
    "2. Model building.\n",
    "3. the training loops.\n",
    "\n",
    "### 1. Data Loading\n",
    "\n",
    "Data Loading in pytorch is very easy and broken into 3 steps:\n",
    "1. Data Source.\n",
    "2. Data Transformations.\n",
    "3. Data Loader.\n",
    "\n",
    "\n",
    "\n",
    "#### Loading data\n",
    "\n",
    "Pytorch uses data loading utility which is called `DataLoader` that supports:\n",
    "automatic batching, transformation, single- and multi-process data loading and more.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch. utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "test_batch_size = 100\n",
    "\n",
    "data_transformations = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])\n",
    "\n",
    "mnist_train = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=data_transformations)\n",
    "mnist_test = datasets.MNIST('../data', train=False,\n",
    "                            transform=data_transformations)\n",
    "\n",
    "train_loader = DataLoader(mnist_train,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test,\n",
    "                         batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label= tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbdc8326410>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print('Label=', labels[0])\n",
    "plt.imshow(images[0].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to build a simple network for this dataset using weight matrices and matrix multiplications. Then, we'll see how to do it using PyTorch's `nn` module which provides a much more convenient and powerful method for defining network architectures.\n",
    "\n",
    "We will create fully connected layers (Each unit in one layer is connected to each unit in the next layer) with following architecture:\n",
    "\n",
    "- Input layer of size `784 = 28x28`: this correspond to the *flattening* of the input image, i.e., tranform the 2D images into 1D vectors. Having batch size of `32`, the input size will be `(32, 784)` \n",
    "- Two hidden layers with 256 and 100 units\n",
    "- Output layer with 10 units corresponding to number of classes\n",
    "\n",
    "> **Task**: Flatten the batch of images images. Then build a multi-layer network with 784 input units, 100 hidden units, 100 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we'll add one that gives us a probability distribution next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# Flatten the input images\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# Create parameters with random initialization\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 100)\n",
    "b2 = torch.randn(100)\n",
    "\n",
    "w3 = torch.randn(100, 10)\n",
    "b3 = torch.randn(10)\n",
    "\n",
    "# Write the code to compute the output of network from the image inputs. \n",
    "# 3 lines of code\n",
    "h1 = activation(torch.mm(inputs, w1))\n",
    "h2 = activation(torch.mm(h1, w2))\n",
    "out = activation(torch.mm(h2, w3))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 10 outputs for our network. We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to. Something that looks like this: \n",
    "\n",
    "<image src=\"./assets/image_distribution.png\" width=\"400px\">\n",
    "\n",
    "Here we see that the probability for each class is roughly the same. This is representing an untrained network, it hasn't seen any data yet so it just returns a uniform distribution with equal probabilities for each class.\n",
    "\n",
    "To calculate this probability distribution, we often use the softmax function. Mathematically this looks like:\n",
    "\n",
    "\n",
    "#### $\\sigma(x_i) = \\dfrac{e^{x_i}}{\\sum_{j=1}^{K}e^{x_j}}$\n",
    "\n",
    "\n",
    "What this does is squish each input between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Implement a function softmax that performs the softmax calculation and returns probability distributions for each example in the batch. Note that you'll need to pay attention to the shapes when doing this. If you have a tensor `a` with shape `(32, 10)` and a tensor `b` with shape `(32,)`, doing `a/b` will give you an error because PyTorch will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. The way to think about this is for each of the 32 examples, you only want to divide by one value, the sum in the denominator. So you need b to have a shape of `(32, 1)`. This way PyTorch will divide the 10 values in each row of a by the one value in each row of b. Pay attention to how you take the sum as well. You'll need to define the dim keyword in torch.sum. Setting dim=0 takes the sum across the rows while `dim=1` takes the sum across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "def softmax(x):\n",
    "    # return the softmax \n",
    "    x_exp = torch.exp(x)\n",
    "    return x_exp / torch.sum(x_exp, dim=1).view(-1, 1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Model building\n",
    "\n",
    "PyTorch provides a module `nn` that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units,  100 hidden units,, 10 output units and a softmax output.\n",
    "\n",
    "1. Defining components: <br/>\n",
    "This step is done in the constructor, where you will define the layers that will be used accordingly in the next step.\n",
    "2. Network flow: <br/>\n",
    "This step is done in the forward function. Where you will get the input batch as an argument then you will use the defined layers in the previous step to define the flow of the network then you will return the output batch.\n",
    "\n",
    "\n",
    "Pytorch is a dynamic framework, where you can use primitive python keywords with it.\n",
    "You can use if and while statements. Also, it can accepts and returns more than one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (hidden2): Linear(in_features=256, out_features=100, bias=True)\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = nn.Linear(28*28, 256)        \n",
    "        # Write 2 lines to define 2 more linear layers.\n",
    "        # 1 hidden layer with number of neurons numbers: 100\n",
    "        # 1 output layer that should output 10 neurons, one for each class.\n",
    "        self.hidden2 =  nn.Linear(256, 100)\n",
    "        self.output = nn.Linear(100, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the linear layers hidden1, hidden2, and output\n",
    "        # accepts only flattened input (1D batches)\n",
    "        # while the batch x is of size (batch, 28 * 28)\n",
    "        # define one line to flatten the x to be of size (batch_sz, 28 * 28)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Net().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training loops\n",
    "After that we should define the loops over tha batches and run the training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10 # number of times we will go through the data\n",
    "lr = 0.01 # learning rate\n",
    "momentum = 0.5 # the momentum: used only for stochastic \n",
    "log_interval = 10 # personal variable: display stats every log_interval\n",
    "\n",
    "criterion = nn.NLLLoss() # negative log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train() # initialize the attributes\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device) # move the data to the GPU / CPU\n",
    "        optimizer.zero_grad() # set the grads to zeros before training\n",
    "        output = model(data) # traing the model\n",
    "        loss = criterion(output, target) # \n",
    "        loss.backward() #  backward propagation\n",
    "        optimizer.step() # update the weights\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "    model.eval() # evaluate\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Do the same that was done in the previous function.\n",
    "            # But without backprobagating the loss and without running the optimizers\n",
    "            # As this function is only for test.\n",
    "            # write 3 lines to transform the data to the device, get the output and compute the loss \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\rTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_model.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How to check if the neural network model is overfitting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

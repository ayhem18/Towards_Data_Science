{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This is my attempt to solve the first assignment of the Introduction to Machine Learning course, fall 2022.\n",
    "## Note\n",
    "Please make sure the dataset is saved in the same working directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary: imports and Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main imports needed for the rest of the notebook\n",
    "import os\n",
    "import math\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "# setting seaborn to darkgrid for a more detailed display of the values\n",
    "STYLE = 'darkgrid'\n",
    "sns.set_style(STYLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "dataset_name = \"a1_dataset.csv\"\n",
    "file_path = os.path.join(wd, dataset_name) # setting the location of the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = pd.read_csv(file_path) # save original datafrae \n",
    "df = df_org.copy() # copy to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename the columns\n",
    "new_names = {\"target\": \"y\"}\n",
    "y = df['target']\n",
    "for i in range(1, len(df.columns) + 1):\n",
    "    new_names[f'var{str(i)}'] = f\"f{str(i)}\"\n",
    "df = df.rename(columns=new_names)\n",
    "print(df.columns)\n",
    "# I will drop the target column and add it to the dataframe when needed\n",
    "df.drop('y', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "In this part I preprocess the data for the training phase:\n",
    "* distinguish between numerical and categorial data\n",
    "* clean certain columns\n",
    "* encode categorical features\n",
    "* impute missing vaues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method returns a tuple of the column names : numerical then categorical\n",
    "def num_cat(df):\n",
    "    num = df.select_dtypes(np.number).columns.values\n",
    "    cat = df.select_dtypes(['object', 'category']).columns.values\n",
    "    return num, cat\n",
    "\n",
    "num_cols, cat_cols = num_cat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "Let's start cleaning by the fixing the 'var7' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set the last column to datetime for further manipulation\n",
    "try:    \n",
    "    df['f7'] = pd.to_datetime(df['f7']) \n",
    "except:\n",
    "    print(\"Certain dates are semantically invalid\")\n",
    "    \n",
    "from dateutil import parser\n",
    "\n",
    "# for futher manipulation we need to determine the invalid dates\n",
    "def validate_dates(row):\n",
    "    try:\n",
    "        row['valid_date'] = parser.parse(row['f7']) # if the data isinvalid an error will raise,\n",
    "    except ValueError:\n",
    "        row['valid_date'] = False # the except block will catch it and set the field to False\n",
    "    return row\n",
    "\n",
    "df = df.apply(validate_dates, axis=1)\n",
    "invalid_dates = df[df['valid_date'] == False]['f7'].values\n",
    "# drop the additional column\n",
    "df.drop('valid_date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invalid_dates) # this is the list of invalid dates in the dataframes\n",
    "# let's reduce these dates by 24 hours\n",
    "\n",
    "def fix_dates(row):\n",
    "    if row['f7'] in invalid_dates:\n",
    "        date, time = row['f7'].split()\n",
    "        # change the 29 to 28\n",
    "        date = date[:-2] + \"28\"\n",
    "        row['f7'] = date + \" \" + time\n",
    "    return row\n",
    "\n",
    "df = df.apply(fix_dates, axis=1)\n",
    "\n",
    "df['f7'] = pd.to_datetime(df['f7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the missing data has the sa\n",
    "print(df.dtypes)\n",
    "# now that the 7th column is converted to datetime, we can futher break it down and tackle each component of the date: year, month, day, time\n",
    "year = 'year'\n",
    "month = 'month'\n",
    "day = 'day'\n",
    "time = 'time'\n",
    "date_cols = [year, month, day, time]\n",
    "def decompose_date(row):\n",
    "    row[year] = row['f7'].year\n",
    "    row[month] = row['f7'].month\n",
    "    row[day] = row['f7'].day\n",
    "    row[time] = row['f7'].time\n",
    "    return row\n",
    "\n",
    "df = df.apply(decompose_date, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in date_cols[:-1]: # the time column has a significantly large number of unique values.\n",
    "    df_c = df.copy()\n",
    "    df_c['y'] = y\n",
    "    fig = sns.catplot(data=df_c, kind='count', x=c, col='y', col_order=[0, 1])\n",
    "    fig.set(xlabel=c, ylabel='count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are my observations:\n",
    "* there is only one year present in all dates: 2019. The year can be dropped then\n",
    "* The dataset is clearly not a time-series dataset where each second, minute or even hour is important, so the time part can be dropped as well\n",
    "\n",
    "There are two main options left:\n",
    "1. keep the date as month + day.\n",
    "2. reduce the date to the month value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f7_drop = ['f7', 'time', 'year']\n",
    "for t in f7_drop:\n",
    "    df.drop(t, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's investigate the first option: keep the date as day and month \n",
    "def set_date(row):\n",
    "    row['date'] = pd.Timestamp(year=2019, month=row[month], day=row[day])\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_date, axis=1) \n",
    "# sort the dataframe by date\n",
    "df = df.sort_values(by='date', ascending=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to consider the different interactions between the date and the rest of the columns\n",
    "num_cols, cat_cols = num_cat(df)\n",
    "\n",
    "for col in num_cols[:-2]:\n",
    "    col_by_date = pd.pivot_table(df, index='date', values=col, aggfunc=['count', 'mean', 'median'])\n",
    "    g = sns.relplot(kind='scatter', x=col_by_date.index.values, y=col_by_date[('count', col)].values)\n",
    "    g.fig.suptitle(f\"variation of {col} by date\")\n",
    "    g.set(xlabel='date', ylabel=f'mean of {col}')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it might be more efficient to consider the day of the year\n",
    "def set_day_of_year(row):\n",
    "    date = pd.Timestamp(year=2019, month=row[month], day=row[day])\n",
    "    row['day_of_year'] = (date - pd.Timestamp(year=2019, month=1, day=1)).days + 1\n",
    "    return row\n",
    "\n",
    "df = df.apply(set_day_of_year, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['date', 'day_of_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = y.copy()\n",
    "num_cols, cat_cols = num_cat(df)\n",
    "\n",
    "for col in num_cols:\n",
    "    g = sns.relplot(data=df, x='day_of_year', y=col)\n",
    "    g.fig.suptitle(f\"variation of {col} by day_of_year\")\n",
    "    g.set(xlabel='date', ylabel=f'{col}')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "df.drop('y', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the visualizations do not show any trend in the interacation between day_of_year and any other numerical feature or the target variable\n",
    "# let's confirm that by calculating the correlation\n",
    "df['y'] = y.copy()\n",
    "print(df.corr()['day_of_year'])\n",
    "df.drop('y', axis=1, inplace=True)\n",
    "# as we can see the correlation is below 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the provided evidence we can claim that the first option might not  be suitable for our data. Let's consider the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_cols = ['day', 'day_of_year', 'date']\n",
    "for d in day_cols: df.drop(d, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = y.copy()\n",
    "print(df.corr()['month'])\n",
    "df.drop('y' ,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated here the does not provide much information about the final classification. This column might not be useful for predicting the final target variable. However, we can investigate its effect when imputing the missing continous values: 'f4'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical variables\n",
    "It is imperative to convert the categorical variables to numerical representations before feeding them to machine learning models\n",
    "### Encoding f6\n",
    "Ordinal Encoder is a perfect encoding technique for f6 as the values \"yes\" and \"no\" are ordered. The order of the numerical representations does not matter as the machine learning algorithm can assign either positive or negative signs correcting the order proposed by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode f6 as no:0 and yes:1\n",
    "print(df['f6'].value_counts())\n",
    "# the column has only 2 values \"yes\" and \"no\" as suggested above.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder() # default parameters: as no customized order can be deduced from the data, the ordering is left up to the encoder\n",
    "X = oe.fit_transform(df[['f6']]) # create a new dataframe where the column f6 is encoded\n",
    "# create the final dataframe\n",
    "df = pd.concat([df.drop('f6', axis=1), pd.DataFrame(X, columns=['f6'])], axis=1)\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['f4'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding f3: the area column:\n",
    "In this section, we will experiement with two different encodings for the f3 column. As No order can be imposed on countries (the general context of the data is missing and countries appear twice with different classes and numerical features), among the possible encodings, I will consider: \n",
    "1. OneHotEncoding\n",
    "2. TargetEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before proceeding with the encoding, it is recommended to clean the data by normalizing the string representation and removing any possible unwanted characetrs\n",
    "def clean_country(row):\n",
    "    row['f3'] = row['f3'].strip().lower()\n",
    "    # remove any string between parentheses if they exist\n",
    "    row['f3'] = re.sub('\\(.*\\)', \"\", row['f3'])\n",
    "    # remove any string between brackets if they exist\n",
    "    row['f3'] = re.sub('\\[.*\\]', \"\", row['f3'])\n",
    "    return row\n",
    "\n",
    "df = df.apply(clean_country, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONE HOT ENCODING\n",
    "df_OHE = df.copy()\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "X = ohe.fit_transform(df_OHE[['f3']])\n",
    "df_OHE = pd.concat([df_OHE.drop('f3', axis=1), pd.DataFrame(X, columns=ohe.get_feature_names())], axis=1).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a function that would divide a dataframe the complete and to-impute parts, divide the complete parts into training and test datasets\n",
    "RANDOM_STATE = 11\n",
    "from sklearn.model_selection import train_test_split\n",
    "def divide_f4(df):\n",
    "    df_f4 = df[~df['f4'].isna()]\n",
    "    df_imp = df[df['f4'].isna()]\n",
    "    f4 = df_f4['f4'].copy()\n",
    "    df_f4.drop('f4', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    df, df_t, y, y_t = train_test_split(df_f4, f4, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    print(y.describe())\n",
    "    return df, df_t, y, y_t, df_imp\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding f3 for predicting f4\n",
    "\n",
    "* One Hot Encoding: Applying this encoding will lead to a dataset with a signficantly large number of feature and a relatively small number of samples\n",
    "*  Ordinal Encoding: there is no reason to believe that one area is by any computational mean can be ordered before or after another area.  \n",
    "\n",
    "The third option considered here is ***Target Encoding***. Each category will represent a numerical value that embeds some knowledge of the target in question. In this imputing we are considering the ***f4*** as a target.\n",
    "Each area is replaced by the ***mean*** of f4 values associated with it. If an area is seen for the first time then it is replaced with the general median of all f4 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TE = df.copy()\n",
    "\n",
    "df_TE, df_TE_t, y_TE, y_TE_t, df_TE_imp = divide_f4(df_TE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_TE['f4'] = y_TE\n",
    "f4_by_area = pd.pivot_table(df_TE, index='f3', values='f4', aggfunc=['count', 'mean'])\n",
    "df_TE.drop('f4', axis=1, inplace=True)\n",
    "print(f4_by_area)\n",
    "\n",
    "def f3_encode_f4(row):\n",
    "    area = row['f3']\n",
    "    # if the area encountered is included in the training data\n",
    "    if area  in f4_by_area.index.values:\n",
    "        row['f3'] = f4_by_area[('mean', 'f4')][area] # replace the country by the mean of the values of f4 associated with it\n",
    "    else: # if the area is encountered is not included in the training data, replace it with the general mean of f4\n",
    "        row['f3'] = y_TE.mean()\n",
    "    return row\n",
    "\n",
    "df_TE= df_TE.apply(f3_encode_f4, axis=1) # we have a training set where f3 is target encoded\n",
    "df_TE_t = df_TE_t.apply(f3_encode_f4, axis=1) # a test set where f3 is target encoded.\n",
    "df_TE_imp = df_TE_imp.apply(f3_encode_f4, axis=1)\n",
    "print(df_TE['f3'])\n",
    "print(df_TE_t['f3'])\n",
    "print(df_TE_imp['f3'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing f4\n",
    "As the different encoding methods were considered, it is now time to impute the missing values using a Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare cross validation\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "CV = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(df_4, df_4_t):\n",
    "    # scale the training data\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(df_4)\n",
    "    df = pd.DataFrame(Xs, columns=df_4.columns)\n",
    "\n",
    "    # scale the test data\n",
    "    Xs = scaler.transform(df_4_t)\n",
    "    df_t = pd.DataFrame(Xs, columns=df_4_t.columns)\n",
    "    return df, df_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor as knr\n",
    "\n",
    "SCORING = \"neg_mean_squared_error\"\n",
    "\n",
    "\n",
    "def best_ridge(df: pd.DataFrame, y:np.array ):\n",
    "    \"\"\"Given a training and dataset, it will return the Ridge model with the lowest mean squared error on cross validation\n",
    "\n",
    "    Args:\n",
    "        df_4 (pd.DataFrame): the training data\n",
    "        y_f4 (np.array): the training target values\n",
    "\n",
    "    Returns:\n",
    "    Ridge estimator with the alpha hyperparameter tuned\n",
    "    \"\"\"\n",
    "    global CV, SCORING\n",
    "    X_train = df.values\n",
    "    \n",
    "    ridge = Ridge()\n",
    "    parameters = {\"alpha\": [10 ** i for i in range(-3, 3)]}\n",
    "\n",
    "    ridge_search = GridSearchCV(ridge, parameters, cv=CV, scoring=SCORING, n_jobs=-1)\n",
    "\n",
    "    ridge_search.fit(X_train, y)\n",
    "\n",
    "    return ridge_search.best_estimator_, - ridge_search.best_score_\n",
    "\n",
    "def best_lasso(df: pd.DataFrame, y: np.array):\n",
    "    \"\"\"Given a training and dataset, it will return the Lasso model with the lowest mean squared error on cross validation\n",
    "\n",
    "    Args:\n",
    "        df_4 (pd.DataFrame): the training data\n",
    "        y_f4 (np.array): the training target values\n",
    "\n",
    "    Returns:\n",
    "    Lasso estimator with the alpha hyperparameter tuned.\n",
    "    \"\"\"\n",
    "    global CV, SCORING \n",
    "    X_train = df.values\n",
    "\n",
    "    lasso = Lasso()\n",
    "    \n",
    "    parameters = {\"alpha\": [10 ** i for i in range(-3, 3)]}\n",
    "\n",
    "    lasso_search = GridSearchCV(lasso, parameters, cv=CV, scoring=SCORING, n_jobs=-1)\n",
    "    lasso_search.fit(X_train, y)\n",
    "\n",
    "\n",
    "    return lasso_search.best_estimator_, - lasso_search.best_score_\n",
    "\n",
    "def best_knr(df, y):\n",
    "    \"\"\"Given a training and dataset, it will return the KNearestNeighbors Regressor with the lowest mean squared error on cross validation\n",
    "\n",
    "    Args:\n",
    "        df_4 (pd.DataFrame): the training data\n",
    "        y_f4 (np.array): the training target values\n",
    "\n",
    "    Returns:\n",
    "    KNearestNeighbors regressor with the \"K\" hyperparameter tuned.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = df.values\n",
    "\n",
    "    k_reg = knr() # a default model\n",
    "\n",
    "    # there are two main parameters to tune: number of neighbors and the type of distance\n",
    "\n",
    "    k_reg_params = {\"n_neighbors\": list(range(1, 15)), \"weights\":['uniform', 'distance']}\n",
    "\n",
    "    k_reg_search = GridSearchCV(k_reg, k_reg_params, cv=CV, scoring=SCORING)\n",
    "\n",
    "    k_reg_search.fit(X_train, y)\n",
    "\n",
    "    return k_reg_search.best_estimator_, -k_reg_search.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def best_poly_features(df, y):\n",
    "    \"\"\"given the training dataset and the corresponding target values, it returns the degree for which POLYNOMIAL REGRESSION \n",
    "    performs the best\n",
    "\n",
    "    Args:\n",
    "        df_4 (DataFrame): training data\n",
    "        yf4 (Series): training target values\n",
    "    \"\"\"\n",
    "    global CV\n",
    "    \n",
    "    X_t = df.values\n",
    "    y_t = y.values\n",
    "\n",
    "    polys = [PolynomialFeatures(degree=i) for i in range(2, 6)]\n",
    "    X_trains = [p.fit_transform(X_t) for p in polys]    \n",
    "    \n",
    "    # intiate a Linear Regression model\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    best_score = 10 ** 9\n",
    "    best_deg = 0\n",
    "    scoring = \"neg_mean_squared_error\"\n",
    "    for i in range(len(X_trains)): \n",
    "        score = -np.mean(cross_val_score(lr, X_trains[i], y_t, cv=CV, scoring=SCORING))\n",
    "        \n",
    "        print(f\"degree: {str(i + 2)}\" )\n",
    "        print(f\"score: {str(np.mean(score))}\")\n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_deg = i + 2\n",
    "    \n",
    "    return best_score, best_deg, polys[best_deg - 2]\n",
    "    # print(best_deg)\n",
    "    # print(best_score)\n",
    "\n",
    "    # X_train = X_trains[best_deg - 2] # set the training data\n",
    "    # X_test = polys[best_deg - 2].transform(df_4_test.values) # set the test data    \n",
    "\n",
    "\n",
    "def best_poly_reg(df, y):\n",
    "    _,_, poly = best_poly_features(df, y)\n",
    "    X_train = df.values\n",
    "    # apply the best polynomial features on the training data\n",
    "    X_train = poly.transform(X_train)\n",
    "    \n",
    "    # find the best ridge estimator with this training data\n",
    "    ridge_est, ridge_score = best_ridge(pd.DataFrame(X_train), y)\n",
    "    # find the best lasso estimator with this training data\n",
    "    lasso_est, lasso_score = best_lasso(pd.DataFrame(X_train), y)\n",
    "    \n",
    "    if ridge_score > lasso_score:\n",
    "        return lasso_est\n",
    "    return ridge_est\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# given a training and test dataset for prediting f4, this function will return the model that performs better the test dataset.\n",
    "def best_imputation_model(df_4, df_4_t, y_f4, y_f4_t, use_poly=True): \n",
    "    global CV\n",
    "    df, df_t = scale_data(df_4, df_4_t)\n",
    "    # we will find the best possible Ridge, Lasso, Polynomial Regularized Regression and the best KNN-R\n",
    "    # each of them will be tested on the test dataset and the one with best score will be returned\n",
    "    if use_poly:\n",
    "        poly = best_poly_reg(df, y_f4)\n",
    "    \n",
    "    ridge, _ = best_ridge(df, y_f4)\n",
    "    lasso, _ = best_lasso(df, y_f4)\n",
    "    knn, _ = best_knr(df, y_f4)\n",
    "\n",
    "    X_train = df.values\n",
    "    X_test = df_t.values\n",
    "    models = [ridge, lasso, knn]\n",
    "    if use_poly:\n",
    "        models.append(poly)\n",
    "    \n",
    "    best_score = 10 ** 9\n",
    "    best_model = None\n",
    "    \n",
    "    for m in models:\n",
    "        m.fit(X_train, y_f4) \n",
    "        y_pred = m.predict(X_test)\n",
    "        score = (mean_squared_error(y_pred, y_f4_t))\n",
    "        print(f\"model: {str(m)}\")\n",
    "        print(f\"mean squared error {str(score)}\")\n",
    "        if score < best_score:\n",
    "            best_model = m\n",
    "            best_score = score\n",
    "            \n",
    "    return best_model, best_score   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's divide the data one hot encoded the same way the target-encoded data is\n",
    "y_HE = df_OHE['f4'][y_TE.index.values]\n",
    "y_HE_t = df_OHE['f4'][y_TE_t.index.values]\n",
    "df_OHE.drop('f4', axis=1, inplace=True)\n",
    "df_HE = df_OHE.loc[df_TE.index.values, :]\n",
    "df_HE_t = df_OHE.loc[df_TE_t.index.values, :]\n",
    "df_TE_imp = df_OHE.loc[df_TE_imp.index.values, :]\n",
    "\n",
    "print((y_HE == y_TE).all()) \n",
    "print((y_HE_t == y_TE_t).all()) \n",
    "\n",
    "# the train, test, impute splitting is the same for both OHE and Target encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find the best imputation models for both types of encoding:\n",
    "impute_TE = best_imputation_model(df_TE, df_TE_t, y_TE, y_TE_t)\n",
    "impute_HE = best_imputation_model(df_HE, df_HE_t, y_HE, y_HE_t, use_poly=False)\n",
    "\n",
    "print(impute_TE[1])\n",
    "print(impute_HE[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

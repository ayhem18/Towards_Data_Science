{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This is my attempt for the first assignment of the Machine Learning course Fall 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Please make sure the dataset is saved in the ***same working directory*** as this jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: imports and loading data\n",
    "In this section, I load the data, import the necessary libraries needed for the rest of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "# setting seaborn to darkgrid for a more detailed display of the values\n",
    "STYLE = 'darkgrid'\n",
    "sns.set_style(STYLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "dataset_name = \"a1_dataset.csv\"\n",
    "file_path = os.path.join(wd, dataset_name) # setting the location of the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = pd.read_csv(file_path) # save original datafrae \n",
    "df = df_org.copy() # copy to work on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data \n",
    "In this section, I explore the dataset, prepare it: (data cleaning and preprocessing), analyse a number of aspects and interations in the data before building models for our prediction purposes \n",
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.info() # the only column with missing values is 'var4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename the columns\n",
    "new_names = {\"target\": \"y\"}\n",
    "y = df['target']\n",
    "for i in range(1, len(df.columns) + 1):\n",
    "    new_names[f'var{str(i)}'] = f\"f{str(i)}\"\n",
    "df = df.rename(columns=new_names)\n",
    "print(df.columns)\n",
    "# I will drop the target column and add it to the dataframe when needed\n",
    "df.drop('y', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical and categorial columns\n",
    "It is crucial to divide the data into categorical and numerical columns as each column should be treated differently. In this subsection, I perform this task in a systematic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method returns a tuple of the column names : numerical then categorical\n",
    "def num_cat(df):\n",
    "    num = df.select_dtypes(np.number).columns.values\n",
    "    cat = df.select_dtypes(['object', 'category']).columns.values\n",
    "    return num, cat\n",
    "\n",
    "\n",
    "num_cols, cat_cols = num_cat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data: visualization, grouping, descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method is short for first impression numerical, where I consider the values and its effect \n",
    "def first_imp_num(df, y, col_name):\n",
    "    df_c = df.dropna(axis=1).copy() # a copy with non Nan values\n",
    "    f1_fig = sns.relplot(data=df, x=df_c.index.values, y=col_name, col=y, col_order=[0, 1])\n",
    "    f1_fig.set(xlabel=\"index\", ylabel=col_name)\n",
    "    plt.show()\n",
    "\n",
    "for col in num_cols:\n",
    "    first_imp_num(df, y, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysing the numerical columns\n",
    "from empiricaldist import Cdf\n",
    "    \n",
    "def cdf_num(df, y, col_name, target=False):\n",
    "    df_c = df.dropna(axis=0).copy() # take a copy with no nan values in it.\n",
    "    df_c['y'] = y.copy()\n",
    "    if not target:\n",
    "        col_cdf = Cdf.from_seq(df[col_name])\n",
    "        col_cdf.plot()\n",
    "        plt.xlabel(f'{col_name}')\n",
    "        plt.ylabel(f'CDF for {col_name}')\n",
    "        plt.show()         \n",
    "    else :    \n",
    "        cdf_1 = Cdf.from_seq(df_c[df_c['y'] == 1][col_name])\n",
    "        cdf_0 = Cdf.from_seq(df_c[df_c['y'] == 0][col_name])\n",
    "        cdf_1.plot(label='y == 1')\n",
    "        cdf_0.plot(label='y == 0')\n",
    "        plt.xlabel(f'{col_name}')\n",
    "        plt.ylabel(f'CDF for {col_name}')\n",
    "        plt.legend()\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    cdf_num(df, y, col, target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "considering all numerical features (except for f4), negative examples tend to have high values for such features. The column f4 is not as informative as no clear trend or correlation (linear or non-linear) can be seen. This is possibly the result of the large number of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing categorical features\n",
    "The features f3, f6 and f7 are categorical features. The interacations should be understood in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing f7\n",
    "As the 'f7' includes invalid dates, more preprocessing steps should be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set the last column to datetime for further manipulation\n",
    "try:    \n",
    "    df['f7'] = pd.to_datetime(df['f7']) \n",
    "except:\n",
    "    print(\"Certain dates are semantically invalid\")\n",
    "    \n",
    "from dateutil import parser\n",
    "\n",
    "# for futher manipulation we need to determine the invalid dates\n",
    "def validate_dates(row):\n",
    "    try:\n",
    "        row['valid_date'] = parser.parse(row['f7']) # if the data isinvalid an error will raise,\n",
    "    except ValueError:\n",
    "        row['valid_date'] = False # the except block will catch it and set the field to False\n",
    "    return row\n",
    "\n",
    "df = df.apply(validate_dates, axis=1)\n",
    "invalid_dates = df[df['valid_date'] == False]['f7'].values\n",
    "# drop the additional column\n",
    "df.drop('valid_date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invalid_dates) # this is the list of invalid dates in the dataframes\n",
    "# let's reduce these dates by 24 hours\n",
    "\n",
    "def fix_dates(row):\n",
    "    if row['f7'] in invalid_dates:\n",
    "        date, time = row['f7'].split()\n",
    "        # change the 29 to 28\n",
    "        date = date[:-2] + \"28\"\n",
    "        row['f7'] = date + \" \" + time\n",
    "    return row\n",
    "\n",
    "df = df.apply(fix_dates, axis=1)\n",
    "\n",
    "df['f7'] = pd.to_datetime(df['f7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "# now that the 7th column is converted to datetime, we can futher break it down and tackle each component of the date: year, month, day, time\n",
    "year = 'year'\n",
    "month = 'month'\n",
    "day = 'day'\n",
    "time = 'time'\n",
    "date_cols = [year, month, day, time]\n",
    "def decompose_date(row):\n",
    "    row[year] = row['f7'].year\n",
    "    row[month] = row['f7'].month\n",
    "    row[day] = row['f7'].day\n",
    "    row[time] = row['f7'].time\n",
    "    return row\n",
    "\n",
    "df = df.apply(decompose_date, axis=1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in date_cols[1:-1]:\n",
    "    df_c = df.copy()\n",
    "    df_c['y'] = y\n",
    "    fig = sns.catplot(data=df_c, kind='count', x=c, col='y', col_order=[0, 1])\n",
    "    fig.set(xlabel=c, ylabel='count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only year present is 2019. The data concerns countries so time within a single day cannot be significant\n",
    "# year, day and  time columns are to be dropped.\n",
    "df = df.drop(year, axis=1)\n",
    "df = df.drop(time, axis=1)\n",
    "df = df.drop(day, axis=1)\n",
    "df = df.drop('f7', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the number of positively classed examples within each month\n",
    "df['y'] = y.copy()\n",
    "print(pd.pivot_table(df, index=month, values='y', aggfunc=['count', 'mean']))\n",
    "# we can see that the month's value does not affect the class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'f3' Column\n",
    "As the column 'f3' represents countries' (areas') names, it is recommended to normalize the string representation and remove any unnecessary characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_country(row):\n",
    "    row['f3'] = row['f3'].strip().lower()\n",
    "    # remove any string between parentheses if they exist\n",
    "    row['f3'] = re.sub('\\(.*\\)', \"\", row['f3'])\n",
    "    # remove any string between brackets if they exist\n",
    "    row['f3'] = re.sub('\\[.*\\]', \"\", row['f3'])\n",
    "    return row\n",
    "\n",
    "df = df.apply(clean_country, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date with the rest of the columns\n",
    "def set_date(row):\n",
    "    row['date'] = pd.Timestamp(year=2019, month=row[month], day=row[day])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode f6 as no:0 and yes:1\n",
    "print(df['f6'].value_counts())\n",
    "# the column's integrity is verified\n",
    "encode_dict = {\"no\":0, \"yes\":1}\n",
    "df['f6'] = df['f6'].apply(encode_dict.get)\n",
    "\n",
    "# for f in num_cols:\n",
    "#     print(pd.pivot_table(df, index='f3', columns=month, values=f, aggfunc=[np.mean]))\n",
    "# print(pd.pivot_table(df, index='f3', columns=month, values='f6', aggfunc=[np.mean]))\n",
    "\n",
    "f1_trend = pd.pivot_table(df, index='f3', columns=month, values='f1', aggfunc=[np.mean])\n",
    "f2_trend = pd.pivot_table(df, index='f3', columns=month, values='f2', aggfunc=[np.mean])\n",
    "f4_trend = pd.pivot_table(df, index='f3', columns=month, values='f4', aggfunc=[np.mean])\n",
    "f5_trend = pd.pivot_table(df, index='f3', columns=month, values='f5', aggfunc=[np.mean])\n",
    "f6_trend = pd.pivot_table(df, index='f3', columns=month, values='f6', aggfunc=[np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's account the ratio of times each country was classified positively\n",
    "country_y_ratio = pd.pivot_table(df, index='f3', values='y', aggfunc=['count', 'mean']).sort_values(by=[('mean', 'y'), ('count', 'y')], ascending=[False, False])\n",
    "print(country_y_ratio.iloc[:50,:]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f3 Encoding\n",
    "The dataframe displayed by the cell above represents the basis for an adequate target encoding of the 'f3' feature. The proposed encoding is:\n",
    "$\\begin{align} count \\cdot ratio \\end{align}$\n",
    "* count: the country's number of occurrences\n",
    "* ratio: the ratio of positive classes associated with these occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's investigate the effect of the months and days further.\n",
    "# f1_trend = pd.pivot_table(df, index='f3', columns=y, values='f1', aggfunc=[np.mean, np.median, 'count'])\n",
    "# f2_trend = pd.pivot_table(df, index='f3', columns=y, values='f2', aggfunc=[np.mean, np.median,'count'])\n",
    "# f4_trend = pd.pivot_table(df, index='f3', columns=y, values='f4', aggfunc=[np.mean, np.median, 'count'])\n",
    "# f5_trend = pd.pivot_table(df, index='f3', columns=y, values='f5', aggfunc=[np.mean, np.median, 'count'])\n",
    "\n",
    "# print(f1_trend)\n",
    "# print(f2_trend)\n",
    "# print(f4_trend)\n",
    "# print(f5_trend)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month, area(f3) and target\n",
    "In this section I will study the effect of the combination: month, area, and target class on the different numerical values (except 'f4').\n",
    "The procedure can be described as follows:\n",
    "* separate the positive and negative rows into two dataframes\n",
    "* for each dataframe group the rows by 'f3' and aggregate a given numerical feature on the month column: calculate the mean value at each month by country (area/f3)\n",
    "* visualize the evolution of the mean with respect to month column\n",
    "\n",
    "\n",
    "The mean in this case is a representative statistic as there is a less than 3 values for each combination of (country, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df_1 = df[df['y'] == 1]\n",
    "df_0 = df[df['y'] == 0]\n",
    "\n",
    "\n",
    "def feat_month_country(feat, aggs=None, num_display=2, num_samples=20):\n",
    "    if aggs is None:\n",
    "        aggs = [np.mean]\n",
    "    # visualize positive \n",
    "    f_months1 = pd.pivot_table(df_1, index='f3', columns=month, values=feat, aggfunc=[np.mean]) # calculate the mean of the feature accross different months for positive rows\n",
    "    f_months0 = pd.pivot_table(df_0, index='f3', columns=month, values=feat, aggfunc=[np.mean]) # calculate the mean of the feature accross different months for negative rows\n",
    "\n",
    "    for i in range(1, num_display + 1):\n",
    "        c0 = f_months1.index.values.tolist()\n",
    "        c0_sample = random.sample(c0, min(num_samples, len(c0))) \n",
    "\n",
    "        for c in c0_sample:\n",
    "            g = sns.lineplot(x=range(1, 8), y=f_months1.loc[c,:])\n",
    "        g.set(xlabel='months', ylabel=feat, label=c)\n",
    "        g.set_title(f\"evolution of {feat} with respect to months for positive areas: plot {str(i)}\")\n",
    "        plt.show()\n",
    "        \n",
    "    # visualize negative\n",
    "    for i in range(1, num_display + 1):\n",
    "        c0 = f_months0.index.values.tolist()\n",
    "        c0_sample = random.sample(c0, min(num_samples, len(c0))) \n",
    "\n",
    "        for c in c0_sample:\n",
    "            g = sns.lineplot(x=range(1, 8), y=f_months0.loc[c,:])\n",
    "        g.set(xlabel='months', ylabel=feat, label=c)\n",
    "        g.set_title(f\"evolution of {feat} with respect to months for negative areas: plot {str(i)}\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['f1', 'f2', 'f5']\n",
    "for c in cols:\n",
    "    feat_month_country(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualizations display chaotric interaction between the different features grouped by the country ('f3') column and the date column ('f7'). \n",
    "Thus, we can safely assume the month + country combination has little to no correlation with the numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.pivot_table(df, index='f3', columns=month, values='f6', aggfunc=['count']))\n",
    "# print(pd.pivot_table(df, index='f3', columns=month, values='f6', aggfunc=['mean']))\n",
    "print(pd.pivot_table(df, index=month, values='f6', aggfunc=['count', 'mean'])) # the month does not correlate with f6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the evolution of different values with respect to the months regardless of the country column\n",
    "# we can acheive that using box plots\n",
    "def feat_month(feat, aggs=None):\n",
    "    if aggs is None:\n",
    "        aggs = [np.mean]\n",
    "    f_month = pd.pivot_table(df, index=month, values=feat, aggfunc=aggs)\n",
    "    for stat in aggs:\n",
    "        fig = sns.relplot(kind=\"line\", ci=None, data=f_month, x='month', y=f_month[(stat, feat)])\n",
    "        fig.fig.suptitle(f\"{stat}'s variation of {feat} with respect to the month\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['f1', 'f2', 'f5']:\n",
    "    feat_month(f, ['max', 'min', 'mean', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "As we can see from the last two subsections, the date reduced to the month value (as the year is the same across the dataset) does not correlate by any mean with neither the country, target or even the (non-missing) numerical features.  \n",
    "The only possible use of the 'f7' column is impute the 'f4' missing values (if possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'f6' column\n",
    "It is time to consider the interaction between the categorical feature 'f6' and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(kind='count', ci=None, data=df, x='y', col='f6', col_order=[0, 1])\n",
    "g.set(xlabel='target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the plot above reveals the value of 'f6' has little to no effect the final classification\n",
    "# let's calcuate the corretation between these two values to solidify this observation\n",
    "print(df.loc[:, ['f6', 'y']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing the f4 column\n",
    "As the 'f4' column misses 600 values, it is of absolute necessity to fill these missing values. In our case, the imputing method would be building a regression model predicting the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop('y', axis=1, inplace=True)\n",
    "df_4 = df[~df['f4'].isna()]\n",
    "print(df_4.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data in relation to f4\n",
    "In this subsection, we are tackling a sub problem. However, it is a still a different problem (a regression to say the least), so it should approached accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_4['f4'])\n",
    "# we can see that the distribution of 'f4' is not too far from normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize the relation of 'f4' with the rest of the columns\n",
    "for col in num_cols[:-2]:\n",
    "    g = sns.relplot(kind='scatter', data=df_4, x=col, y='f4')\n",
    "    g.fig.suptitle(f\"variation of 'f4' with respect of {col}\")\n",
    "    g.set(xlabel=col, ylabel='f4')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualizations demonstrate that the relations between f4 and the other numerical (continous) features are definitely non-linear. Thus further experiementing is needed to reveal the hidden interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's investigate the effect of categorical features \n",
    "sns.boxplot(data=df_4, y='f4', x='f6')\n",
    "plt.show()\n",
    "sns.boxplot(data=df_4, y='f4', x=month)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, the first plot demonstrates that f6 provides almost no information about f4 as the variation of f4 for both categories is roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4_by_area = pd.pivot_table(df_4, index='f3', values='f4', aggfunc=['count', 'max', 'min', 'mean','median'])\n",
    "# f3 is the most informative predictor so far. \n",
    "f4_by_month = pd.pivot_table(df_4, index=month, values='f4', aggfunc=['count', 'median'])\n",
    "def encode_area(row):\n",
    "    area = row['f3']\n",
    "    # encode the area as the mean of f4 for that area\n",
    "    if area in f4_by_area.index.values:\n",
    "        row['f3'] = f4_by_area[('mean', 'f4')][area]\n",
    "    # if the area if seen for the first time: encode it as the median of f4 by corresponding month\n",
    "    else:\n",
    "        mo = row[month]\n",
    "        row['f3'] = f4_by_month[('median', 'f4')][mo]\n",
    "    return row\n",
    "print(f4_by_area)\n",
    "print(f4_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = df[df['f4'].isna()]\n",
    "print(df_imp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df_4.apply(encode_area, axis=1) # encode the f3 column for the complete data\n",
    "df_imp = df_imp.apply(encode_area, axis=1) # encode the f3 column for the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing the missing values: different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's divide the training data into training and test data: to evaluate the performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "yf4 = df_4['f4']\n",
    "df_4.drop('f4', axis=1, inplace=True)\n",
    "\n",
    "df_4, df_test, yf4, y_test = train_test_split(df_4, yf4, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataframe in question for experimenting\n",
    "from sklearn.preprocessing import StandardScaler # used to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_tmp = df_4.iloc[:, :4]\n",
    "Xs = scaler.fit_transform(df_tmp)\n",
    "df_t = pd.DataFrame(Xs, columns=df_tmp.columns)\n",
    "df_t['f6'] = df_4['f6'].values\n",
    "df_t[month] = df_4[month].values\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarize the test dataset as well\n",
    "df_tmp = df_test.iloc[:, :4]\n",
    "Xs = scaler.transform(df_tmp)\n",
    "df_t = pd.DataFrame(Xs, columns=df_tmp.columns)\n",
    "df_t['f6'] = df_test['f6'].values\n",
    "df_t[month] = df_test[month].values\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression\n",
    "The previous analysis conducted above displays the non-linear interaction between f4 and the rest of the feature. Thus a simple Linear Regression will clear underfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.metrics.get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# I will try polynomial features with degrees 2, 3, 4, 5\n",
    "# the choice will be determined using cross validation\n",
    "X_4_train = df_4.values\n",
    "y_4_train = yf4.values\n",
    "\n",
    "polys = [PolynomialFeatures(degree=i) for i in range(2, 6)]\n",
    "X_trains = [p.fit_transform(X_4_train) for p in polys]    \n",
    "# intiate a Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "best_score = 10 ** 9\n",
    "best_deg = 0\n",
    "scoring = \"neg_mean_squared_error\"\n",
    "for i in range(len(X_trains)): \n",
    "    score = -np.mean(cross_val_score(lr, X_trains[i], y_4_train, cv=cv, scoring=scoring))\n",
    "    \n",
    "    print(f\"degree: {str(i + 2)}\" )\n",
    "    print(f\"score: {str(np.mean(score))}\")\n",
    "    if best_score > score:\n",
    "        best_score = score\n",
    "        best_deg = i + 2\n",
    "   \n",
    "print(best_deg)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the size of the dataset is relatively small, our model is likely to overfit: Lasso and Ridge are more likely to be better options\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "lasso = Lasso()\n",
    "parameters = {\"alpha\": [10 ** i for i in range(-3, 3)]}\n",
    "\n",
    "lasso_search = GridSearchCV(lasso, parameters, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "lasso_search.fit(X_trains[best_deg - 2], y_4_train)\n",
    "\n",
    "lasso_est = lasso_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_test = polys[best_deg - 2].transform(df_test.values)\n",
    "\n",
    "f4_pred = lasso_est.predict(X_test)\n",
    "print(mean_squared_error(y_test, f4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_dicts = dict(zip(y_test.tolist(), f4_pred.tolist()))\n",
    "print(error_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "parameters = {\"alpha\": [10 ** i for i in range(-3, 3)]}\n",
    "\n",
    "ridge_search = GridSearchCV(ridge, parameters, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "ridge_search.fit(X_trains[best_deg - 2], y_4_train)\n",
    "\n",
    "ridge_est = ridge_search.best_estimator_ \n",
    "\n",
    "ridge_f4_pred = ridge_est.predict(X_test)\n",
    "print(mean_squared_error(y_test, ridge_f4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_dicts = dict(zip(y_test.tolist(), ridge_f4_pred.tolist()))\n",
    "print(error_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.36663468e-02 -4.06884124e-03  9.01891595e-01  3.93239861e-02\n",
      " -4.49997720e+00 -3.63241092e-01  2.27033379e+01]\n",
      "247.90776513014242\n"
     ]
    }
   ],
   "source": [
    "lasso_linear = Lasso()\n",
    "parameters = {\"alpha\": [10 ** i for i in range(-3, 3)]}\n",
    "\n",
    "lasso_lin_search =  GridSearchCV(lasso_linear, parameters, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "lasso_lin_search.fit(X_4_train, y_4_train)\n",
    "\n",
    "lasso_lin_est = lasso_lin_search.best_estimator_\n",
    "print(lasso_lin_est.coef_)\n",
    "\n",
    "lasso_lin_pred = lasso_lin_est.predict(df_test.values)\n",
    "\n",
    "print(mean_squared_error(y_test, lasso_lin_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This is my attempt to solve the first assigment of the Nature Language Processing Course offered by Innopolis University "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary: Imports and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to determine whether the notebook is running on Collab or not\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# please make sure the Assignment1_data.zip file is in the current working directory\n",
    "# unzip the file\n",
    "\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "DOWNLOAD_DIR = \"sample_data\" if IN_COLAB else os.getcwd() \n",
    "file_name = \"Assignment1_data.zip\"\n",
    "file_name_no_zip = \"data\"\n",
    "loc = os.path.join(DOWNLOAD_DIR, file_name)\n",
    "# opening the zip file in READ mode\n",
    "\n",
    "if not os.path.isdir(os.path.join(DOWNLOAD_DIR, file_name_no_zip)):\n",
    "    with ZipFile(loc, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "        print('Done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Manual Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize tweets by hand\n",
    "\n",
    "As a first task you need to tokenize 15 tweets by hand. This will allow you to understand the problem from a linguistic point of view. The guidelines for tweet tokenization are as follows:\n",
    "\n",
    "- Each smiley is a separate token\n",
    "- Each hashtag is an individual token. Each user reference is an individual token\n",
    "- If a word has spaces between them then it is converted to a single token\n",
    "- If a sentence ends with a word that legitimately has a full stop (abbreviations, for example), add a final full stop\n",
    "- All punctuations are individual tokens. This includes double-quotes and single quotes also\n",
    "- A URL is a single token\n",
    "\n",
    "Example of output\n",
    "\n",
    "    Input tweet\n",
    "    @xfranman Old age has made N A T O!\n",
    "\n",
    "    Tokenized tweet (separated by comma)\n",
    "    @xfranman , Old , age , has , made , NATO , !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*see\n",
      "@toiletooth Hours?\n",
      "@anitapuspasari waduh..\n",
      "'Pop may throw in the towel second half.. #SemST\n",
      "@ambienteer Yeah, pretty much how i feel about it.\n",
      "God is so good! All the praises go up to Him! #SemST\n",
      "Just purchased 1 MP-443 Grach http://140mafia.com #140mafia\n",
      "@florianseroussi Nothing really noticeable? Are you kidding?\n",
      "Just Posted: Redneck Dragon - Part XXVIII (http://cli.gs/gWy0yT)\n",
      "aw fuck! it turns out you can't defend the champagne glass hot tub.\n",
      "@xfranman Old age has not made you any wiser or more mature. For shame! #SemST\n",
      "RT @weirdnews The Worldâ€™s Smallest Whale Shark - http://weirdasianews.com?p=4455\n",
      "Greater is He who is in you than he who is in the world. - 1John 4:4 #god #bible #SemST\n",
      "\" Could journos please stop putting the word \"\"gate\"\" after everything they write... gate.\"\n",
      "Live forex trading - another profitable trade with Tflow students ... http://u.mavrev.com/cflb\n",
      "Religious people are people too...... Just bigoted,genitalia mutilating, angry gullible people #SemST\n"
     ]
    }
   ],
   "source": [
    "# read the file\n",
    "f1  = []\n",
    "with open('file1', 'r') as file1:\n",
    "    lines_1 = file1.readlines()\n",
    "    lines_1 = sorted(lines_1, key=lambda x: len(x))\n",
    "    # remove the new line character at the end of each tweet\n",
    "    f1 = [l[:-1] for l in lines_1[:16]]\n",
    "    \n",
    "for f in f1:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*see']\n",
      "['@toiletooth', 'Hours?']\n",
      "['@anitapuspasari', 'waduh..']\n",
      "[\"'Pop\", 'may', 'throw', 'in', 'the', 'towel', 'second', 'half..', '#SemST']\n",
      "['@ambienteer', 'Yeah,', 'pretty', 'much', 'how', 'i', 'feel', 'about', 'it.']\n",
      "['God', 'is', 'so', 'good!', 'All', 'the', 'praises', 'go', 'up', 'to', 'Him!', '#SemST']\n",
      "['Just', 'purchased', '1', 'MP-443', 'Grach', 'http://140mafia.com', '#140mafia']\n",
      "['@florianseroussi', 'Nothing', 'really', 'noticeable?', 'Are', 'you', 'kidding?']\n",
      "['Just', 'Posted:', 'Redneck', 'Dragon', '-', 'Part', 'XXVIII', '(http://cli.gs/gWy0yT)']\n",
      "['aw', 'fuck!', 'it', 'turns', 'out', 'you', \"can't\", 'defend', 'the', 'champagne', 'glass', 'hot', 'tub.']\n",
      "['@xfranman', 'Old', 'age', 'has', 'not', 'made', 'you', 'any', 'wiser', 'or', 'more', 'mature.', 'For', 'shame!', '#SemST']\n",
      "['RT', '@weirdnews', 'The', 'Worldâ€™s', 'Smallest', 'Whale', 'Shark', '-', 'http://weirdasianews.com?p=4455']\n",
      "['Greater', 'is', 'He', 'who', 'is', 'in', 'you', 'than', 'he', 'who', 'is', 'in', 'the', 'world.', '-', '1John', '4:4', '#god', '#bible', '#SemST']\n",
      "['\"', 'Could', 'journos', 'please', 'stop', 'putting', 'the', 'word', '\"\"gate\"\"', 'after', 'everything', 'they', 'write...', 'gate.\"']\n",
      "['Live', 'forex', 'trading', '-', 'another', 'profitable', 'trade', 'with', 'Tflow', 'students', '...', 'http://u.mavrev.com/cflb']\n",
      "['Religious', 'people', 'are', 'people', 'too......', 'Just', 'bigoted,genitalia', 'mutilating,', 'angry', 'gullible', 'people', '#SemST']\n"
     ]
    }
   ],
   "source": [
    "for f in f1:\n",
    "    print(f.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = ['*', 'see']\n",
    "t2 = ['@toiletooth', 'Hours', '?']\n",
    "t3 = ['@anitapuspasari', 'waduh', '.', '.']\n",
    "t4 = [\"'\", \"Pop\", 'may', 'throw', 'in', 'the', 'towel', 'second', 'half', '.', '.', '#', 'SemST']\n",
    "t5 = ['@ambienteer', 'Yeah',',', 'pretty', 'much', 'how', 'i', 'feel', 'about', 'it', '.']\n",
    "t6 = ['God', 'is', 'so', 'good', '!', 'All', 'the', 'praises', 'go', 'up', 'to', 'Him', '!', '#', 'SemST']\n",
    "t7 = ['Just', 'purchased', '1', 'MP-443', 'Grach', 'http://140mafia.com', '#', '140mafia']\n",
    "t8 = ['@florianseroussi', 'Nothing', 'really', 'noticeable','?', 'Are', 'you', 'kidding', '?']\n",
    "t9 = ['Just', 'Posted', ':', 'Redneck', 'Dragon', '-', 'Part', 'XXVIII', '(', 'http://cli.gs/gWy0yT', ')']\n",
    "t10 = ['aw', 'fuck','!', 'it', 'turns', 'out', 'you', '\"', \"can\", \"'\", \"t\", 'defend', 'the', 'champagne', 'glass', 'hot', 'tub','.']\n",
    "t11 = ['@xfranman', 'Old', 'age', 'has', 'not', 'made', 'you', 'any', 'wiser', 'or', 'more', 'mature', '.', 'For', 'shame', '!', '#','SemST']\n",
    "t12 = ['RT', '@weirdnews', 'The', \"World\", \"'\", \"s\", 'Smallest', 'Whale', 'Shark', '-', 'http://weirdasianews.com?p=4455']\n",
    "t12 = ['Greater', 'is', 'He', 'who', 'is', 'in', 'you', 'than', 'he', 'who', 'is', 'in', 'the', 'world', '.', '-', '1John', '4',':','4', '#', 'god', '#', 'bible', '#','SemST']\n",
    "t13 = ['\"', 'Could', 'journos', 'please', 'stop', 'putting', 'the', 'word', '\"', '\"', 'gate', '\"', '\"', 'after', 'everything', 'they', 'write', '.', '.', '.', 'gate','.','\"']\n",
    "t14 = ['Live', 'forex', 'trading', '-', 'another', 'profitable', 'trade', 'with', 'Tflow', 'students', '.', '.', '.', 'http://u.mavrev.com/cflb']\n",
    "t15 = ['Religious', 'people', 'are', 'people', 'too', '.' '.', '.', '.', '.', '.', 'Just', 'bigoted', ',', 'genitalia', 'mutilating',',', 'angry', 'gullible', 'people', '#', 'SemST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tweet\n",
      "*see\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "* , see\n",
      "\n",
      "Input Tweet\n",
      "@toiletooth Hours?\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "@toiletooth , Hours , ?\n",
      "\n",
      "Input Tweet\n",
      "@anitapuspasari waduh..\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "@anitapuspasari , waduh , . , .\n",
      "\n",
      "Input Tweet\n",
      "'Pop may throw in the towel second half.. #SemST\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "' , Pop , may , throw , in , the , towel , second , half , . , . , # , SemST\n",
      "\n",
      "Input Tweet\n",
      "@ambienteer Yeah, pretty much how i feel about it.\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "@ambienteer , Yeah , , , pretty , much , how , i , feel , about , it , .\n",
      "\n",
      "Input Tweet\n",
      "God is so good! All the praises go up to Him! #SemST\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "God , is , so , good , ! , All , the , praises , go , up , to , Him , ! , # , SemST\n",
      "\n",
      "Input Tweet\n",
      "Just purchased 1 MP-443 Grach http://140mafia.com #140mafia\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "Just , purchased , 1 , MP-443 , Grach , http://140mafia.com , # , 140mafia\n",
      "\n",
      "Input Tweet\n",
      "@florianseroussi Nothing really noticeable? Are you kidding?\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "@florianseroussi , Nothing , really , noticeable , ? , Are , you , kidding , ?\n",
      "\n",
      "Input Tweet\n",
      "Just Posted: Redneck Dragon - Part XXVIII (http://cli.gs/gWy0yT)\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "Just , Posted , : , Redneck , Dragon , - , Part , XXVIII , ( , http://cli.gs/gWy0yT , )\n",
      "\n",
      "Input Tweet\n",
      "aw fuck! it turns out you can't defend the champagne glass hot tub.\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "aw , fuck , ! , it , turns , out , you , \" , can , ' , t , defend , the , champagne , glass , hot , tub , .\n",
      "\n",
      "Input Tweet\n",
      "@xfranman Old age has not made you any wiser or more mature. For shame! #SemST\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "@xfranman , Old , age , has , not , made , you , any , wiser , or , more , mature , . , For , shame , ! , # , SemST\n",
      "\n",
      "Input Tweet\n",
      "RT @weirdnews The Worldâ€™s Smallest Whale Shark - http://weirdasianews.com?p=4455\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "Greater , is , He , who , is , in , you , than , he , who , is , in , the , world , . , - , 1John , 4 , : , 4 , # , god , # , bible , # , SemST\n",
      "\n",
      "Input Tweet\n",
      "Greater is He who is in you than he who is in the world. - 1John 4:4 #god #bible #SemST\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "\" , Could , journos , please , stop , putting , the , word , \" , \" , gate , \" , \" , after , everything , they , write , . , . , . , gate , . , \"\n",
      "\n",
      "Input Tweet\n",
      "\" Could journos please stop putting the word \"\"gate\"\" after everything they write... gate.\"\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "Live , forex , trading , - , another , profitable , trade , with , Tflow , students , . , . , . , http://u.mavrev.com/cflb\n",
      "\n",
      "Input Tweet\n",
      "Live forex trading - another profitable trade with Tflow students ... http://u.mavrev.com/cflb\n",
      "\n",
      "Tokenized tweet (separated by comma)\n",
      "Religious , people , are , people , too , .. , . , . , . , . , Just , bigoted , , , genitalia , mutilating , , , angry , gullible , people , # , SemST\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's represent the tokens above as a string for execution\n",
    "tokens_str = \" \".join([f't{i},'for i in range(1, 16)])\n",
    "tokens_str = '[' + tokens_str + ']'\n",
    "# print(tokens_str)\n",
    "tokens = eval(tokens_str)\n",
    "\n",
    "for tweet, t in zip(f1, tokens):\n",
    "    print(\"Input Tweet\")\n",
    "    print(tweet)\n",
    "    print()\n",
    "    print(\"Tokenized tweet (separated by comma)\")\n",
    "    print(\" , \".join(t))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions: How should we proceed with non-ut8 characters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement 4 tokenizers\n",
    "\n",
    "Your task is to implement the 4 different tokenizers that take a list of tweets on a topic and output tokenization for each:\n",
    "\n",
    "- White Space Tokenization\n",
    "- Sentencepiece\n",
    "- Tokenizing text using regular expressions\n",
    "- NLTK TweetTokenizer\n",
    "\n",
    "For tokenizing text using regular expressions use the rules in task 1. Combine task 1 rules into regular expression and create a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb0ce5cb6b092cde9f0ba713d915425207ed6ea08d3ede97530b87c251a3aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

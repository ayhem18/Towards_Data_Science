{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKboZnAdgrRM"
      },
      "source": [
        "# Tweets Tokenization\n",
        "\n",
        "The goal of the assignment is to write a tweet tokenizer. The input of the code will be a set of tweet text and the output will be the tokens in each tweet. The assignment is made up of four tasks.\n",
        "\n",
        "The [data](https://drive.google.com/file/d/15x_wPAflvYQ2Xh38iNQGrqUIWLj5l5Nw/view?usp=share_link) contains 5 files whereby each contains 44 tweets. Each tweet is separated by a newline. For manual tokenization only one file should be used.\n",
        "\n",
        "Grading:\n",
        "- 30 points - Tokenize tweets by hand\n",
        "- 30 points - Implement 4 tokenizers\n",
        "- 20 points - Stemming and Lemmatization\n",
        "- 20 points - Explain sentencepiece (for masters only)\n",
        "\n",
        "\n",
        "Remarks: \n",
        "- Use Python 3 or greater\n",
        "- Max is 80 points for bachelors, 100 points for masters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLDjjAvemUP_"
      },
      "source": [
        "## Tokenize tweets by hand\n",
        "\n",
        "As a first task you need to tokenize 15 tweets by hand. This will allow you to understand the problem from a linguistic point of view. The guidelines for tweet tokenization are as follows:\n",
        "\n",
        "- Each smiley is a separate token\n",
        "- Each hashtag is an individual token. Each user reference is an individual token\n",
        "- If a word has spaces between them then it is converted to a single token\n",
        "- If a sentence ends with a word that legitimately has a full stop (abbreviations, for example), add a final full stop\n",
        "- All punctuations are individual tokens. This includes double-quotes and single quotes also\n",
        "- A URL is a single token\n",
        "\n",
        "Example of output\n",
        "\n",
        "    Input tweet\n",
        "    @xfranman Old age has made N A T O!\n",
        "\n",
        "    Tokenized tweet (separated by comma)\n",
        "    @xfranman , Old , age , has , made , NATO , !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KKKwTidnzUw"
      },
      "source": [
        "\n",
        "    1. Input tweet\n",
        "    ...\n",
        "    1. Tokenized tweet\n",
        "    ...\n",
        "\n",
        "    2. Input tweet\n",
        "    ...\n",
        "    2. Tokenized tweet\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2J2AD2nmUhi"
      },
      "source": [
        "## Implement 4 tokenizers\n",
        "\n",
        "Your task is to implement the 4 different tokenizers that take a list of tweets on a topic and output tokenization for each:\n",
        "\n",
        "- White Space Tokenization\n",
        "- Sentencepiece\n",
        "- Tokenizing text using regular expressions\n",
        "- NLTK TweetTokenizer\n",
        "\n",
        "For tokenizing text using regular expressions use the rules in task 1. Combine task 1 rules into regular expression and create a tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uZId1tjrfyz"
      },
      "outputs": [],
      "source": [
        "def white_space_tokenizer(text: str) -> list[str]:\n",
        "    \n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ou1WE8Krf_W"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "def sentencepiece_wrapper(text: str) -> List[str]:\n",
        "    \n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC32dK_urf5P"
      },
      "outputs": [],
      "source": [
        "def re_tokenizer(text: str) -> List[str]:\n",
        "    \n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8UVniWVrgMT"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "def nltk_tweet_tokenizer(text: str) -> List[str]:\n",
        "    \n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIhPteb4s_Yn"
      },
      "source": [
        "Run your implementations on the data. Compare the results, decide which one is better. List the advantages of the best tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muAZeHkMtaCd"
      },
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlzpzjgEpY-R"
      },
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "Your task is to write two functions: stem and lemmatize. Input is a text, so you need to tokenize it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghAR1rSjsnz1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def stem(text: str) -> List[str]:\n",
        "\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZkR3TPYuk5T"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"...\")\n",
        "\n",
        "def lemmatize(text: str) -> List[str]:\n",
        "\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "islrHZ6UmUoh"
      },
      "source": [
        "## Explain sentencepiece (for masters only)\n",
        "\n",
        "For this task you will have to use sentencepiece text tokenizer. Your task will be to read how it works and write a minimum 10 sentences explanation of the tokenizer works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2RjMwlEshCu"
      },
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmNpWzfLmUyE"
      },
      "source": [
        "## Resources\n",
        "\n",
        "1. [Regular Expressions 1](https://realpython.com/regex-python/)\n",
        "2. [Regular Expressions 2](https://realpython.com/regex-python-part-2/)\n",
        "2. [Spacy Lemmatizer](https://spacy.io/api/lemmatizer)\n",
        "2. [NLTK Stem](https://www.nltk.org/howto/stem.html)\n",
        "3. [SentencePiece](https://github.com/google/sentencepiece)\n",
        "4. [sentencepiece tokenizer](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('ds_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "7bb0ce5cb6b092cde9f0ba713d915425207ed6ea08d3ede97530b87c251a3aee"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

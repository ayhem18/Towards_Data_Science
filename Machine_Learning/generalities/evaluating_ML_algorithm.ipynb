{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "## 1. Evaluation\n",
    "Let's consider the situation where a certain (correct) model was trained by a well-preprocessed data. Yet, it provide unacceptable errors with unseen data. Among the options available to the ML engineer:\n",
    "* Get more training data\n",
    "* consider only a subset of features\n",
    "* adding more features: the present features might not be informative enough\n",
    "* adding complex features: polynomial ones (in case of linear model)\n",
    "* considering larger / smaller ***regularization*** parameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 train test splitting\n",
    "A basic method to evaluate a model is to first divide the data set provided into two subsets:\n",
    "* training set: generally $70\\%$ of the entire dataset\n",
    "* testing set: generally $30\\%$ of the entire dataset\n",
    "We fit the model with the training set. Then, we evaluate its performance on the test set by computing the cost function.\n",
    "\n",
    "### 1.2 train - cross validation - test splitting\n",
    "Consider the scenario where multiple models are possible. For example, having multiple polynomial regressions. In this case, the degree of the polynomial $d$ can be seen as an additional parameter (hyperparameter). Choosing the parameter $d$ based on the test set does not guarantee the final model is general enough to perform well on unseen data. Therefore, a new division is introduced: \n",
    "* training set: with which each of the polynomial models is fit, generall $60\\%$ of the entire dataset\n",
    "* cross-validation set: to choose which degree is the most optimal, generally $20\\%$\n",
    "* test set: used to evaluate the final model (based on the cross-validation results) performance. $20\\%$ \n",
    "Consequently, we can derive the terms: train error, cross-validatation (CV) error and test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Bias or Variance\n",
    "Generally, a low-performance can always break down to either ***high bias*** or ***high variance***.  \n",
    "* high variance: when the model overfits the data: the CV error is much higher than the train error since the model fits the training dataset to a large extent\n",
    "* high bias: when the model underfits the data: Both CV and train errors are quite high since the model is not well trained and should be expected to make poor predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization\n",
    "### 2.1 Cost functions\n",
    "Assuming that the model is associated with the cost function $J(\\theta)$. and a regularized cost function $J_{reg}(\\theta) = J(\\theta) + \\sum_{i=1}^{m} \\theta_i ^ 2$. We introduce the following: \n",
    "* train cost function = $J_{train} (\\theta)$ \n",
    "* CV cost functio = $J_{cv}(\\theta)$\n",
    "* test cost function = $J_{test} (\\Theta) $\n",
    "where each of the above-mentioned cost functions has the same formula as the general cost function, only itereting through the corresponding data set.\n",
    "### 2.2 General selection procedure\n",
    "To choose the best ***model variant*** and ***regularization parameter***, the following procedure is useful: \n",
    "1. create a list of possible $\\lambda$'s \n",
    "2. create the models with the different variants (in the previous example, polynomial with different degrees)\n",
    "3. iterate through the possible values of $\\lambda$'s and for each value fit the possible model variants obtaining **$\\lambda$** model variants \n",
    "4. for each choice, compute the cross validation error\n",
    "5. select the combination $(\\lambda, model~variant)$ that minimizes the cross validation error\n",
    "6. test the final model by computing $J_{test} (\\Theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Learning Curves\n",
    "#### 2.3.1 High bias\n",
    "A model that underfits the training data does not consider all the relevant aspects of the problem. Consequently, additional training sample will not improve the performance as simple the model does not take full advantage of the additional data.\n",
    "Taking into consideration such remarks, one practical indicator of ***high bias*** is the following learning curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![High bias Learning curve](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/generalities/learning_curve_high_bias.png?raw=true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear model is quite likely to underfit a complex problem with a significant number of features. When plotting $J_{cv}(\\theta)$ and $J_{train} (\\theta)$ with $m$ as the parameter. We can see that after a certain training set size, $J_{cv}(\\theta)$ ceases to decrease and the $J_{train} (\\theta)$ ceases to increase. A similar behavior is a serious indicator of underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 High Variance\n",
    "The larger the dataset, the more complex the model should be to overfit the training data. Therefore, if we consider an initial model with high variance, it might be helpful to keep adding training samples. The following learning curves are generally obtained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![High bias Learning curve](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/generalities/learning_curve_high_variance.png?raw=true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of such results we can divide the possible solutions as follows:\n",
    "1. fixing high bias (underfit)\n",
    "* add more complex features\n",
    "* decrease $\\lambda$ the regularization parameter\n",
    "* add more features\n",
    "2. fixing high  variance (overfit)\n",
    "* adding training sample\n",
    "* increase $\\lambda$ the regularization parameter\n",
    "* consider a subset of features"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e6cc62e2280f752aa7e5178eced3fb8e0cbe7333ea77893546e6db4cefb4290"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "## 1 Motivation\n",
    "In real world Machine learning projects, problems tend to have significantly large dimensionality for several reasons:\n",
    "* the problem is complex and include several aspects\n",
    "* several teams are working together and certain features are redundant.\n",
    "Therefore, reducing the number of features has several advantages:\n",
    "* improve performance (depending on the situation)\n",
    "* visualize the dataset and have a better intuitive understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 P.C.A\n",
    "### 2.1 Probelm formulation\n",
    "The Principal Component Analysis algorithm is the most popular alogrithm for dimensionality reduction. Assuming \n",
    "$m$ vector $\\in \\mathbb{R} ^ {n}$, the algorithm finds $k$ vectors $\\in \\mathbb{R} ^ {k}$ forming a sub space such that the sum of the projections (projection error) on that space is minimal.\n",
    "### 2.2 Algorithm\n",
    "Before executing the algorithm, it is preferable to apply mean normalization as well as features scaling. The algorithm can be broken to the following steps\n",
    "1. compute the covariance matrix:\n",
    "$\n",
    "\\begin{align} \\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} \\cdot (x^{(i)}) ^ T = \\frac{1}{m} X ^ {T} \\cdot X\n",
    "\\end{align}\n",
    "$\n",
    "2. apply the Single Value Decomposition on $\\Sigma$ obtaining matrices $U$, $S$, $V$. \n",
    "3. Consider the matrix $U_{reduced}$ as the matrix composed out of the first $k$ columns of $U$\n",
    "4. for every vector $x^{(i)}$ we compute the vector $x_{approx} ^{(i)} = U_{reduced} ^ T \\cdot x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Number of principle components: $K$\n",
    "#### 2.3.1 Choosing K\n",
    "The Average squared projection error is defined as: $\\begin{align} \\frac{1}{m} \\sum_{i=1}^{m} ||x^{(i)} - x_{approx} ^{(i)}|| ^{2} \\end{align}$\n",
    "\n",
    "The total variance is defined as $\\frac{1}{m} \\sum_{i=1}^{m} ||x^{(i)}||^2$\n",
    "The number $K$ is chosen as the smallest number satisfying: \n",
    "$\\begin{align} \n",
    "\\frac{\\frac{1}{m} \\sum_{i=1}^{m} ||x^{(i)} - x_{approx} ^{(i)}|| ^{2}}{\\frac{1}{m} \\sum_{i=1}^{m} ||x^{(i)}||^2} = \n",
    "\\frac{\\sum_{i=1}^{m} ||x^{(i)} - x_{approx} ^{(i)}|| ^{2}}{\\sum_{i=1}^{m} ||x^{(i)}||^2} \\leq C \\end{align}$\n",
    "\n",
    "We choose $K$ such that $(1 - C) * 100$\\% of the variance is retained \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Implementation notes\n",
    "It is possible to prove:\n",
    "$\\begin{align} \n",
    "\\frac{\\sum_{i=1}^{m} ||x^{(i)} - x_{approx} ^{(i)}|| ^{2}}{\\sum_{i=1}^{m} ||x^{(i)}||^2} = 1 - \\frac{\\sum_{i=1}^{k} S_{ii}}{\\sum_{i=1}^{n} S_{ii}}\n",
    "\\end{align}$\n",
    "where $S$ is the diagonal matrix returned by the SVD and $S_{ii}$ is the $i$-th diagonal value.  \n",
    "Thus, the value of $K$ can be chosen with only one call to the *SVD* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 The new training dataset\n",
    "For an initial dataset represented as \n",
    "$\\begin{align}\n",
    "X =\\begin{bmatrix} \n",
    "x^{(1)} \\\\\n",
    "x^{(2)} \\\\\n",
    ".. \\\\\n",
    ".. \\\\\n",
    "x^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\end{align}$\n",
    ", it is possible to find the new dataset \n",
    "$\\begin{align}\n",
    "Z =\\begin{bmatrix} \n",
    "z^{(1)} \\\\\n",
    "z^{(2)} \\\\\n",
    ".. \\\\\n",
    ".. \\\\\n",
    "z^{(m)}\n",
    "\\end{bmatrix} = X \\cdot U_{reduced}\n",
    "\\end{align}$  \n",
    "We have\n",
    "$\\begin{align} z = x_{approx} = U_{reduced} ^ T \\cdot x \\end{align}$\n",
    "This can be extended to the entire dataset:\n",
    "$ \\begin{align} U_{reduced} ^ T \\cdot X^T = \\begin{bmatrix} z_1, z_2, ..., z_m \\end{bmatrix} = Z^{T}  \\end{align}$ \n",
    "Thus $\\begin{align} Z = X \\cdot U_{reduced} \\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 PCA and Supervised Learning\n",
    "For a supervised learning problem and a dataset $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})$. \n",
    "1. extract the inputs unlabeled \n",
    "2. approximate them $z^{(1)}, z^{(2)},..., z^{(m)}$\n",
    "3. obtain the new training set $(z^{(1)}, y^{(1)}), (z^{(2)}, y^{(2)}), ..., (z^{(m)}, y^{(m)})$\n",
    "4. The mapping parameters obtained : $U_{reduced}, k$ are then used to convert $x_{cv}$ to $z_{cv}$ and $x_{test}$ to $z_{test}$. No additional execution of PCS is required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

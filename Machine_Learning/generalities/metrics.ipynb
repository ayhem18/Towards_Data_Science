{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Average Precision\n",
    "Mean Average Precision is one of the most widely used metrics to evaluate an Object Detection model. Let's first recall some famous metrics:\n",
    "$\\begin{align}\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\end{align}$\n",
    "where $TP$ stands for True Positive: Predictive Positive and it is true.\n",
    "$\\begin{align} \n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "\\end{align}$\n",
    "corresponds to the proportions of actually Positive instance out of all instances predicted as positive by the model. As for object detection, things get a bit more interesting:\n",
    "$\\begin{align} \n",
    "AP = \\sum_{i = 1}^{n - 1} (recall[i] - recall[i + 1]) * precision[i]\n",
    "\\end{align}$\n",
    "where $recall[i]$ represents the recall's metric given the $i$-th threshold out of $n$ thresholds. Recalls are appended with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

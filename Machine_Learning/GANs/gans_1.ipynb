{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview1\n",
    "This notebook is created to save notes from the lectures of the GAN specialization offered on Coursera\n",
    " \n",
    "## Week1\n",
    "### Intuition\n",
    "Let's start with the big picture\n",
    "ML can be divided into 2 main branches: discriminative and Generative:\n",
    "\n",
    "1. Discriminative: simply what to know what a set of numbers represents out of a very specific set of possibilities: basically what is that ? (that is $X$ btw) or what makes $y$ (belongs to?) **$Y$**\n",
    "2. Generative: seeks to answer: how can I construct an instance of $Y$ ? (best explanation so far)\n",
    "\n",
    "More formally:\n",
    "\n",
    "1. Discriminative: $P(Y|X)$ given $X$\n",
    "2. Generative: a possible $X$ given $Y$, the possible is expressed by some noise $\\epsilon$\n",
    "\n",
    "well there are 2 types of GMs:\n",
    "\n",
    "1. Auto encoders: Encoder-Decoder architecture: The intuition is as follows: build a component that can determine what makes $x$ X, and a component that can build (with a touch of indeterminism) given a specific instance of the core of $X$: something like an architect: encoder and a construction team: :decoder.\n",
    "2. GANs: Generator and Discriminator. The intuition is as follows: if I have an extremely competent inspector that can distinguishes between fake and real $x$'s, but I can still fool him. Then, my $x$ are too realistic and roughly speaking I know how to construct a possible $x$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Well, The course do not introduce any interesting details as they focus more on the general flow:\n",
    "\n",
    "1. Discrimininator is a classifier: (not much of a surprise isn't?) Given a set of noise vectors + some real examples, the current generator will turn the noise vectors into fake images. The discriminator is trained on a binary classification task: Real or Fake\n",
    "\n",
    "2. Generator: Not very clear so far. Yet, the workflow is as follows: Given a noise vec, create an image, pass it to the discriminator and update $\\theta_g$ according to the discriminator's output. The generator's output is heavily influenced by the datasets it is trained on: The most frequent features are more likely to appear in the generator's output.\n",
    "\n",
    "3. The training is iterative:\n",
    "    * train the discriminator, given $\\theta_g$, update $\\theta_d$\n",
    "    * train the generator: given $\\theta_d$, update $\\theta_g$: THE LABELS PASSED TO THE DISCRIMINATOR SHOULD BE ALL `REAL`, THE LOSS IS PROPORTIONAL TO HOW WELL THE DISCRIMINATOR DIFFERENTIATES BETWEEN THE FAKE IMAGES AND THE REAL ONES...\n",
    "    * an important NOTE to keep in mind: training a generator with a superior discriminator is not a fruitful process. As an almost certain output from a discriminator cannot help the generator learn\n",
    "    * It is more likely to have a superior Discriminator than a superior generator. The training process should account for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "This week is a review of several Computer Vision basic components\n",
    "\n",
    "## Batch normalization\n",
    "* let's get the intuition here starting with 2 indenpendent variables $x_1$, $x_2$\n",
    "    1. if $x_1$ is too left skewed and the other one is belly-shaped, even linear regression will have to assign larger weights to the simply because one distribution is shifted in comparison to the other\n",
    "    2. The learning will be simpler if the distributions are as close as possible to one another\n",
    "\n",
    "* extending this issue to neural network, the phenomena of ***internal covariat shift***. In other words, even if the initial features are indeed standarized, it is not necessarily the case for $z^[i]$ further in the network.\n",
    "\n",
    "* Assuming we have the outputs of layer $l - 1$: $a^{[l - 1]}$: then the outputs of the next layer generally would be $$a^{[l]} = \\sigma (z^l = W^l \\cdot a^{[l-1]} + b^{l - 1})$$ The here is to use the batch to convert $$z^{[i]}_k = \\frac{z^{[i]}_k - E(z^{[i]}_k)}{\\sqrt Var(z^{[i]}_k)}$$\n",
    "\n",
    "## Upsampling\n",
    "* I am already familiar with padding, stride and the convolution operations in general. well, it is basically an algorithmic idea:\n",
    "* Nearest Neighbors interpolation \n",
    "* Linear interpolation\n",
    "* Bi-linear interpolation \n",
    "* Similar to the usual convolution, we have a transpose convolution layer. I will add more details about this later."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

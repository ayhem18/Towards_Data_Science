{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression in General\n",
    "## 1.1 Main idea\n",
    "Linear regression is, as the name suggests, a supervised learning algorithm: a regression. The main idea is to fit the provided data into one line. The algorithm mainly finds the line with the minimal numerical error.\n",
    "\n",
    "## 1.2 Notation\n",
    "**m** generally refers to the number of training examples, samples\n",
    "\n",
    "**x's** refer to an input variable, a feature\n",
    "\n",
    "**y's** refer to an output variable, a target. \n",
    "More specifically, $(x^(i) , y^(i))$ represent the ith feature and target in the given sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. One variable Linear Regression\n",
    "\n",
    "## 2.1 Main idea\n",
    "given a large set of features **x's**, we would like our algorithm to express **y =  a * x + b**. The coefficients are chosen such that\n",
    "the cost function is minimized\n",
    "\n",
    "## 2.2 cost function and mathematical background\n",
    "\n",
    "Please refer to the following link: https://github.com/ayhem18/Towards_Data_science/blob/master/Machine%20Learning/Linear%20Regression/math_parts/multi_var_LR_math.pdf\n",
    "### 2.2.1 Remarques \n",
    "In addition to the analytically calculated solution, we can use optimization algorithms such as Gradient Descent (This algorithm is quite usuful since the square error metric function is a *convex* function.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Multi variable regression\n",
    "## 3.1 Notation\n",
    "Additional notation should be introduced in this case:\n",
    "\n",
    "$n$ = number of features: number of $x$'s per sample\n",
    "\n",
    "$x^{(i)}$: the vector of features of the $i$-th training sample\n",
    "\n",
    "$x_j^{(i)}$: the value of the $j$-th vaylue in the $i$-th training sample.\n",
    "## 3.2 Main idea\n",
    "Given a set of data with $n$ features $x$'s and a target $y$, we would like to approximate the target $y$ to a linear combination of the features. In other words,  express the prediction $h$ (or the hypothesis) as \n",
    "\n",
    "$\\begin{aligned}\n",
    "    h(\\theta) = \\theta_0 + \\theta_1 \\cdot x_1 + .. \\theta_n \\cdot x_n\n",
    "\\end{aligned}$\n",
    "\n",
    "## 3.3 Cost function and mathematical background\n",
    "Please refer to the following link: https://github.com/ayhem18/Towards_Data_science/blob/master/Machine%20Learning/Linear%20Regression/math_parts/multi_var_LR_math.pdf\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

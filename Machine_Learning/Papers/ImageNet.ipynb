{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "[ImageNet Classification with Deep Convolutional Neural Networks](https://dl.acm.org/doi/10.1145/3065386) is one of the classics in the Computer Vision literature. This small notebook is summary in my own words of the paper.  \n",
    "\n",
    "## Architecture\n",
    "### Relu\n",
    "Neural Networks are capable of learning complex interactions because of their non linearity. The latter is achieved by a non-linear saturation function. Emperical evidence shows that RELU activation fuction leads to significantly faster training time in comparison to classical activation functions such as **sigmoid** and **tanh**\n",
    "### Local Response Normalization\n",
    "This is a technique that was introduced to improve the network's generalization. Let's break down. Denoting by $a^{i}_{x, y}$, the value of the activation of a neuron at the $i$-th kernel (filter) after applying ***RELU***. The local Response Normalization technique produces a slightly different output $b^{i}_{x, y}$: \n",
    "$$b^{i}_{x, y} = \\frac{a^{i}_{x, y}}{(k + \\alpha \\cdot \\sum_{j = max(0, i - \\frac{n}{2})}^{min(N-1, i + \\frac{n}{2})} (a^{j}_{x, y})^2)^{\\beta}}$$\n",
    "where $k, n, \\alpha, \\beta$ are hyperparameters chosen by cross validation technique.\n",
    "\n",
    "Let's break this seemingly complicated expression down:\n",
    "Given the activation value at a single neuron, we sum the activation values of each element in the spatial position $(x, y)$ within the range $\\pm \\frac{n}{2}$ (without going out of the boarders which explain the use of **max** and **min** operators). This sum is then multiplied by $\\alpha$ and added an integer $k$. The final result is raised to the power of $\\beta$\n",
    "\n",
    "### Overlapping Pooling\n",
    "The paper suggests using overlapping pooling as a means of reducing generalization error. More specifically have the size of the pooling (kernel / filter) larger than the stride.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Reduction\n",
    "### Data augmentation\n",
    "The authors of this paper introduced 2 main techniques:\n",
    "1. Random crop: The initial images of the dataset are of shape (256, 256, 3). The network takes an input of shape (224, 224, 3). So the main idea, is to take a random crop as well as their reflections as a training example for the network. This simple idea increased the size of the dataset by factor $(32 * 32 * 2) = 2048$\n",
    "Additionally, they propagate the error on the average predictions across the $5$ patches produced with the ***random crop*** technique.\n",
    "\n",
    "2. PCA. This can be done as follows:\n",
    "    1. Building the covariance matrix $U$ of the pixel values matrix: it should be a 3 * 3 matrix\n",
    "    2. Extracting the eigenvalues and eigenvectors of $p1, p2, p3$, $\\lambda_1, \\lambda_2, \\lambda_3$\n",
    "    3. for each pixel value: $I_{x,y} = [I_{x,y}^{R}, I_{x,y}^{G}, I_{x,y}^{B}]$, add the quantity:\n",
    "        $$ [p1, p2, p3] \\cdot [\\alpha_1 \\cdot \\lambda_1, \\alpha_2 \\cdot \\lambda_2, \\alpha_3 \\lambda_3] ^ T$$\n",
    "    where $\\alpha_i$ are values sampled from a guassiand random variable with mean $0$ and variance $0.1$\n",
    "\n",
    "\n",
    "### Dropout\n",
    "They used a dropout with probability $p = 0.5$ only in the first 2 fully connected layers.\n",
    "\n",
    "\n",
    "\n",
    "### Training details\n",
    "The most interesting details are the following:\n",
    "1. using weight decay: the update rule is as follows:\n",
    "$$ w_{i+1} = w_i -\\alpha \\cdot \\frac{d_{w_i}}{d W} + \\lambda \\cdot w \\cdot w ^ T$$\n",
    "It is important to notice that weight decay is different from $L2$ regularization as where the 2nd term is added only to the cost / loss function.\n",
    "2. adjusting the learning rate: starting with a lr of $0.01$, a dividing by 10 whenever the validation error stopped improving with the current rate (The exact meaning of \"stopped improving\" was not further explained in the paper.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

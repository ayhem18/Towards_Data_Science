{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "DVC_DIR = os.path.join(HOME, 'dogs-vs-cats') \n",
    "TRAIN_DIR = os.path.join(DVC_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DVC_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\DEV\\Towards_Data_Science\\Machine_Learning\\Papers\\VGG\\dogs-vs-cats\\train\n",
      "c:\\Users\\bouab\\DEV\\Towards_Data_Science\\Machine_Learning\\Papers\\VGG\\dogs-vs-cats\\test\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_DIR)\n",
    "print(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "import data_setup \n",
    "from data_setup import prepare_DVC_dataset\n",
    "# time to prepare the data for training\n",
    "# prepare_DVC_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get pyqt6 if needed\n",
    "# try:\n",
    "#     import PyQt6\n",
    "# except ModuleNotFoundError:\n",
    "#     ! pip3 install pyqt6\n",
    "\n",
    "# # upload the file for the labeler file\n",
    "# url = \"https://raw.githubusercontent.com/ayhem18/Towards_SE/main/More_Python/PyQT/Labeler/labeler.py\"\n",
    "# import requests \n",
    "# r = requests.get(url)\n",
    "# with open('labeler.py', 'wb') as f:\n",
    "#     f.write(r.content)\n",
    "\n",
    "# # time to label the data\n",
    "from pathlib import Path\n",
    "TEST_DIR_RENAMED = os.path.join(Path(TEST_DIR).parent, 'test_original')\n",
    "# os.rename(TEST_DIR, TEST_DIR_RENAMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import sys\n",
    "# from labeler import LabelerController, LabelerModel, LabelerWindow\n",
    "# from PyQt6.QtWidgets import QApplication\n",
    "\n",
    "# classes = ['dog', 'cat']\n",
    "# from_dir = TEST_DIR_RENAMED\n",
    "# to_dir = TEST_DIR\n",
    "# labelerApp = QApplication([])\n",
    "# view = LabelerWindow(classes=classes)\n",
    "# # let's initialize the controller\n",
    "# model = LabelerModel(classes=classes,\n",
    "#                         from_directory=from_dir,\n",
    "#                         to_directory=to_dir,\n",
    "#                         copy=False,\n",
    "#                         )\n",
    "# controller = LabelerController(view=view, model=model)\n",
    "# view.show()\n",
    "# sys.exit(labelerApp.exec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11419\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(TEST_DIR_RENAMED)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the data is ready \n",
    "# time to create the dataloader\n",
    "import pytorch_modular.data_loaders as loaders\n",
    "importlib.reload(loaders)\n",
    "import torchvision.transforms as T\n",
    "\n",
    "RESIZE = (256, 256)\n",
    "\n",
    "trans = T.Compose([T.Resize(size=(256, 256)), \n",
    "                   T.GaussianBlur(kernel_size=(5, 5), sigma=(0.2, 0.5)), \n",
    "                   T.ToTensor(), \n",
    "                   ])\n",
    "\n",
    "# now we have a small dataset and we can proceed\n",
    "\n",
    "train_loader, test_loader, indices_to_names = loaders.create_dataloaders(TRAIN_DIR,\n",
    "                                                            batch_size=20,\n",
    "                                                            train_transform=trans,\n",
    "                                                            test_dir=TEST_DIR, \n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's verify the data in loaded correctly\n",
    "# X, Y = next(iter(train_loader))\n",
    "# im, label = X[0], Y[0]\n",
    "# print(im.shape)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# img = im.permute((1, 2, 0)).cpu().numpy()\n",
    "# print( indices_to_names[label.item()])\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [02:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m classifier \u001B[39m=\u001B[39m DVG_classifier(input_shape\u001B[39m=\u001B[39m(\u001B[39m256\u001B[39m, \u001B[39m256\u001B[39m, \u001B[39m3\u001B[39m))\n\u001B[0;32m      7\u001B[0m optimizer \u001B[39m=\u001B[39m Adam(classifier\u001B[39m.\u001B[39mparameters(), lr\u001B[39m=\u001B[39m\u001B[39m10\u001B[39m \u001B[39m*\u001B[39m\u001B[39m*\u001B[39m \u001B[39m-\u001B[39m\u001B[39m3\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m train_model(classifier,\n\u001B[0;32m      9\u001B[0m             train_loader,\n\u001B[0;32m     10\u001B[0m             test_loader,    \n\u001B[0;32m     11\u001B[0m             optimizer\u001B[39m=\u001B[39;49moptimizer,\n\u001B[0;32m     12\u001B[0m             epochs\u001B[39m=\u001B[39;49m\u001B[39m30\u001B[39;49m,\n\u001B[0;32m     13\u001B[0m             print_progress\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m\n\u001B[0;32m     14\u001B[0m             )\n",
      "File \u001B[1;32mc:\\Users\\bouab\\DEV\\Towards_Data_Science\\Machine_Learning\\Papers\\VGG\\dvc_vgg.py:48\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_dataloader, test_dataloader, optimizer, epochs, writer, device, print_progress)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[39m# 3. Loop through training and testing steps for a number of epochs\u001B[39;00m\n\u001B[0;32m     47\u001B[0m \u001B[39mfor\u001B[39;00m epoch \u001B[39min\u001B[39;00m tqdm(\u001B[39mrange\u001B[39m(epochs)):\n\u001B[1;32m---> 48\u001B[0m     train_loss, train_acc \u001B[39m=\u001B[39m train_per_epoch(model\u001B[39m=\u001B[39;49mmodel,\n\u001B[0;32m     49\u001B[0m                                             dataloader\u001B[39m=\u001B[39;49mtrain_dataloader,\n\u001B[0;32m     50\u001B[0m                                             loss_fn\u001B[39m=\u001B[39;49mnn\u001B[39m.\u001B[39;49mBCEWithLogitsLoss,\n\u001B[0;32m     51\u001B[0m                                             optimizer\u001B[39m=\u001B[39;49moptimizer,\n\u001B[0;32m     52\u001B[0m                                             output_layer\u001B[39m=\u001B[39;49mnn\u001B[39m.\u001B[39;49mSigmoid,\n\u001B[0;32m     53\u001B[0m                                             device\u001B[39m=\u001B[39;49mdevice)\n\u001B[0;32m     55\u001B[0m     test_loss, test_acc \u001B[39m=\u001B[39m test_per_epoch(model\u001B[39m=\u001B[39mmodel,\n\u001B[0;32m     56\u001B[0m                                          dataloader\u001B[39m=\u001B[39mtest_dataloader,\n\u001B[0;32m     57\u001B[0m                                          loss_fn\u001B[39m=\u001B[39mnn\u001B[39m.\u001B[39mBCEWithLogitsLoss,\n\u001B[0;32m     58\u001B[0m                                          output_layer\u001B[39m=\u001B[39mnn\u001B[39m.\u001B[39mSigmoid,\n\u001B[0;32m     59\u001B[0m                                          device\u001B[39m=\u001B[39mdevice)\n\u001B[0;32m     61\u001B[0m     \u001B[39mif\u001B[39;00m print_progress:\n\u001B[0;32m     62\u001B[0m         \u001B[39m# 4. Print out what's happening\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\bouab\\DEV\\Towards_Data_Science\\Machine_Learning\\Papers\\VGG\\pytorch_modular\\engine_classification.py:46\u001B[0m, in \u001B[0;36mtrain_per_epoch\u001B[1;34m(model, dataloader, loss_fn, optimizer, output_layer, metrics, device)\u001B[0m\n\u001B[0;32m     43\u001B[0m train_loss \u001B[39m=\u001B[39m \u001B[39m0\u001B[39m\n\u001B[0;32m     44\u001B[0m train_metrics \u001B[39m=\u001B[39m [\u001B[39m0\u001B[39m \u001B[39mfor\u001B[39;00m _ \u001B[39min\u001B[39;00m metrics]\n\u001B[1;32m---> 46\u001B[0m \u001B[39mfor\u001B[39;00m batch, (X, y) \u001B[39min\u001B[39;00m \u001B[39menumerate\u001B[39;49m(dataloader):\n\u001B[0;32m     47\u001B[0m     X, y \u001B[39m=\u001B[39m X\u001B[39m.\u001B[39mto(device), y\u001B[39m.\u001B[39mto(device)\n\u001B[0;32m     48\u001B[0m     \u001B[39m# get the forward pass first\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\bouab\\DEV\\ds_env_new\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    439\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_iterator\n\u001B[0;32m    440\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m--> 441\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_get_iterator()\n",
      "File \u001B[1;32mc:\\Users\\bouab\\DEV\\ds_env_new\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcheck_worker_number_rationality()\n\u001B[1;32m--> 388\u001B[0m     \u001B[39mreturn\u001B[39;00m _MultiProcessingDataLoaderIter(\u001B[39mself\u001B[39;49m)\n",
      "File \u001B[1;32mc:\\Users\\bouab\\DEV\\ds_env_new\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[1;34m(self, loader)\u001B[0m\n\u001B[0;32m   1035\u001B[0m w\u001B[39m.\u001B[39mdaemon \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[39m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[39m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m \u001B[39m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[0;32m   1039\u001B[0m \u001B[39m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[0;32m   1040\u001B[0m \u001B[39m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m \u001B[39m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[1;32m-> 1042\u001B[0m w\u001B[39m.\u001B[39;49mstart()\n\u001B[0;32m   1043\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_index_queues\u001B[39m.\u001B[39mappend(index_queue)\n\u001B[0;32m   1044\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_workers\u001B[39m.\u001B[39mappend(w)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[39massert\u001B[39;00m \u001B[39mnot\u001B[39;00m _current_process\u001B[39m.\u001B[39m_config\u001B[39m.\u001B[39mget(\u001B[39m'\u001B[39m\u001B[39mdaemon\u001B[39m\u001B[39m'\u001B[39m), \\\n\u001B[0;32m    119\u001B[0m        \u001B[39m'\u001B[39m\u001B[39mdaemonic processes are not allowed to have children\u001B[39m\u001B[39m'\u001B[39m\n\u001B[0;32m    120\u001B[0m _cleanup()\n\u001B[1;32m--> 121\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_popen \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_Popen(\u001B[39mself\u001B[39;49m)\n\u001B[0;32m    122\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_sentinel \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_popen\u001B[39m.\u001B[39msentinel\n\u001B[0;32m    123\u001B[0m \u001B[39m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[39m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[39m@staticmethod\u001B[39m\n\u001B[0;32m    223\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_Popen\u001B[39m(process_obj):\n\u001B[1;32m--> 224\u001B[0m     \u001B[39mreturn\u001B[39;00m _default_context\u001B[39m.\u001B[39;49mget_context()\u001B[39m.\u001B[39;49mProcess\u001B[39m.\u001B[39;49m_Popen(process_obj)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:336\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[39m@staticmethod\u001B[39m\n\u001B[0;32m    334\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_Popen\u001B[39m(process_obj):\n\u001B[0;32m    335\u001B[0m     \u001B[39mfrom\u001B[39;00m \u001B[39m.\u001B[39;00m\u001B[39mpopen_spawn_win32\u001B[39;00m \u001B[39mimport\u001B[39;00m Popen\n\u001B[1;32m--> 336\u001B[0m     \u001B[39mreturn\u001B[39;00m Popen(process_obj)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, process_obj)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m     92\u001B[0m     reduction\u001B[39m.\u001B[39mdump(prep_data, to_child)\n\u001B[1;32m---> 93\u001B[0m     reduction\u001B[39m.\u001B[39;49mdump(process_obj, to_child)\n\u001B[0;32m     94\u001B[0m \u001B[39mfinally\u001B[39;00m:\n\u001B[0;32m     95\u001B[0m     set_spawning_popen(\u001B[39mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\reduction.py:60\u001B[0m, in \u001B[0;36mdump\u001B[1;34m(obj, file, protocol)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mdump\u001B[39m(obj, file, protocol\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m):\n\u001B[0;32m     59\u001B[0m \u001B[39m    \u001B[39m\u001B[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m     ForkingPickler(file, protocol)\u001B[39m.\u001B[39;49mdump(obj)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# seems like our dataloaders work !\n",
    "# time to train the model\n",
    "from dvc_vgg import DVG_classifier, train_model\n",
    "from torch.optim import Adam\n",
    "\n",
    "classifier = DVG_classifier(input_shape=(256, 256, 3))\n",
    "optimizer = Adam(classifier.parameters(), lr=10 ** -3)\n",
    "train_model(classifier,\n",
    "            train_loader,\n",
    "            test_loader,    \n",
    "            optimizer=optimizer,\n",
    "            epochs=30,\n",
    "            print_progress=True\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## 1. Intuition\n",
    "There are two main advantages for smaller values of $\\theta_1$, $\\theta_2$... $\\theta_n$: \n",
    "1. ***Simpler*** hypothesis\n",
    "2. Less prone to ***overfitting***\n",
    "\n",
    "### 1.1 The modified cost function\n",
    "The new cost function can be expressed as follows:\n",
    "\n",
    "$\\begin{aligned}\n",
    "J_{new}(\\theta) = J_{old}(\\theta) + \\frac{\\lambda}{2m} * \\sum_{j = 1}^{n} \\theta_j\n",
    "\\end{aligned}$ \n",
    "\n",
    "where $c$ is the constant term (assuming $J$ is a sum or a product).\n",
    "### 1.2 Explanation\n",
    "The sum introduced is from $1$ to $n$ where $n$ is the number of features par training example. According to the convention, the first parameter/weight $\\theta_0$ is not panalized. Therefore the sum starts from $1$. The term $\\lambda$ is referred to as the ***regularization*** parameter. Roughly speaking, with the regularization term, the optimazation algorithm tends to set a certain number of parameters $\\theta$ to values close to $0$ resulting in slightly less fit to the training data but general enough for accurate future predictions.\n",
    "### 1.3 Notes\n",
    "It is note worthy that for extremely large values of $\\lambda$, the model ends up ***underfitting*** the data. A large regularization parameter means a large penalty for all parameters. Therefore, the model  assigns infinitesimal values for most parameters which oversimplies the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularized Linear Regression\n",
    "### 2.1 Gradient descent\n",
    "The update part of the algorithm can be expressed as folows: \n",
    "\n",
    "$\\begin{aligned}\n",
    "\\theta_0 := \\theta_0 - \\alpha * \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^i) - y^i)x_0^i\\\\\n",
    "\\theta_j := \\theta_j*(1 - \\alpha * \\frac{\\lambda}{m}) - \\alpha * \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^i) - y^i)x_j^i \n",
    "\\end{aligned}$ \n",
    "\n",
    "### 2.1 Analytical Solution: Normal Equation\n",
    "The introduction of the regularization term slightly modifies the analytical solution\n",
    "$\\begin{aligned}\\theta = (X ^ T * X + \\lambda * \\begin{bmatrix} \n",
    "0 && .. && .. && 0 \\\\\n",
    "0 && 1 && .. && 0 \\\\\n",
    "0 && 0 && 1.. && 0 \\\\\n",
    "0 && 0 && .. && 1\n",
    "\\end{bmatrix})^{-1} X ^ T  y\\end{aligned}$\n",
    "It is possible to prove that the matrix is always invertible for all values $\\lambda > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regularized Logistic Regression\n",
    "### 3.1 The Cost Function\n",
    "The regularized cost function can be expressed as follows:\n",
    "$\\begin{aligned}\n",
    "    J(\\theta) = -\\frac{1}{m} \\cdot \\sum _{i=1}^{m} [y ^ {(i)} \\cdot \\log(h_{\\theta} (x)) + (1 - y) \\cdot \\log(1 - h_{\\theta} (x))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j ^ 2 \n",
    "\\end{aligned}$ \n",
    "\n",
    "### 3.2 The Gradient descent\n",
    "Similarly to linear regression, the update part of the gradient descent can be expressed as follows\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\theta_0 := \\theta_0 - \\alpha * \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^i) - y^i)x_0^i\\\\\n",
    "\\theta_j := \\theta_j*(1 - \\alpha * \\frac{\\lambda}{m}) - \\alpha * \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^i) - y^i)x_j^i \n",
    "\\end{aligned}$ "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e6cc62e2280f752aa7e5178eced3fb8e0cbe7333ea77893546e6db4cefb4290"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

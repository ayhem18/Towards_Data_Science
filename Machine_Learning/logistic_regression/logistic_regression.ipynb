{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Logistic Regression\n",
    "## 1. Binary Classification\n",
    "This is a type of problems where target values are either 1 or 0. For such problems, Linear Regression is not the perfect solution for several reasons:\n",
    "\n",
    "1. Due to the binary nature of target values, a treshhold $t$ should be chosen: if $h_{\\theta} (x) \\ge t$ then it is intrepreted as $1$ and $0$ otherwise. Such treshhold is significantly affected by outliers. Therefore $t$ might get too large contradicting what most of the data indicates.\n",
    "\n",
    "2. even if the feature data is between 0 and 1, the resulting prdecition might not fell in the range between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Logistic Regression\n",
    "Logistic regression computes weights. However, the hypothesis $h_{\\theta} (x) $ is no longer $ \\theta ^ T\\cdot x$ but $g(\\theta ^ T \\cdot x)$ where $g$ is a mathematical function mapping $\\mathbb{R}$ to the interval $(0,1)$\n",
    "\n",
    "### 2.1 Sigmoid function\n",
    "let's consider the function $f(x) = \\frac{L}{1 + e^{- k \\cdot (x - x_0)}}$. The parameters are defined as follows:\n",
    "\n",
    "1. $L$: The maximum of the function\n",
    "2. $x_0$: the midpoint of the function\n",
    "3. $k$: The growth rate\n",
    "\n",
    "For $L = 1$ , $x_0 = 0$, $k = 1$, we define the *sigmoid* function: \n",
    "$\\begin{align}\n",
    "    f(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Hypothersis interpretation\n",
    "We have\n",
    "$\\begin{aligned}\n",
    "h_{\\theta} (x) = \\sigma(\\theta ^ T \\cdot x) =P(y = 1 |x, \\theta)\n",
    "\\end{aligned}$ \n",
    "In other words, the result is interpreted as the probability that $y = 1$ given the vector of features $x$, parameterized by $\\theta$. \n",
    "\n",
    "Since $y$ is either $1$ or $0$, the following holds: \n",
    "$\\begin{aligned}\n",
    "    P(y = 1 |x, \\theta) = 1 - P(y = 0 |x, \\theta)\n",
    "\\end{aligned}$ \n",
    "\n",
    "Conventionally, classification takes place according as follows:\n",
    "\n",
    "$h_{\\theta} (x) \\ge 0.5 \\rightarrow y = 1$ and \n",
    "$h_{\\theta} (x) \\le 0.5 \\rightarrow y = 0$. \n",
    "\n",
    "By analysing the sigmoid function we can see such statements are equivalent to:\n",
    "\n",
    "$\\theta \\cdot x \\ge 0 \\rightarrow y = 1$ and \n",
    "$\\theta \\cdot x \\le 0 \\rightarrow y = 0$.\n",
    "\n",
    "The model can be expanded to represent more complex shapes by introducing new features $x_k = x_{i_1} ^ {a_1} \\cdot x_{i_2} ^ {a_2}... \\cdot x_{i_n} ^ {a_n} $ for some natural numbers $a_1, a_2.. a_n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 Cost function and mathematical background\n",
    "#### 2.4.1 Linear Regression as a starting point\n",
    "The *cost function* or (metric) used to evaluate a linear regression model can be expressed as \n",
    "\n",
    "$\\begin{align}\n",
    "    J = \\frac{1}{2m} \\sum _{i=1}^{m}(y_i - h_{\\theta} (x^i))^2 \n",
    "\\end{align}$ \n",
    "\n",
    "let's consider $J(y, x, \\theta) = (y - h_{\\theta} (x))^2$. for $h_{\\theta} (x) = \\frac{1}{1 + e ^ {-\\theta \\cdot x}}$\n",
    "The function $J$ is ***non-convex***. The numerical approach is likely to fail for such cases. Therefore, a slightly different\n",
    "$ J(y, x, \\theta) $ should be introduced.\n",
    "\n",
    "#### 2.4.2 The final cost function\n",
    "The final cost function can be expressed as follows:\n",
    "\n",
    "$\\begin{align}\n",
    "    J(\\theta) = -\\frac{1}{m} \\cdot \\sum _{i=1}^{m} [y ^ {(i)} \\cdot \\log(h_{\\theta} (x)) + (1 - y) \\cdot \\log(1 - h_{\\theta} (x))]\n",
    "\\end{align}$ \n",
    "The mathematical derivation can be found through the following [link](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine%20Learning/classification_and_logistic_regression/math_parts/Logistic_Regression_math.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiclass classification\n",
    "### 3.1 Extending Logistic regression\n",
    "The mechanism provided by Logistic regression can be manipulated using the ***One Vs all*** method.\n",
    "#### 3.1.1 Ons Vs All Method\n",
    "Let's consider $n$ labels, denoted by numbers from $1$ to $n$. We consider $n$ versions where at each we consider one class as $1$ while the rest of the classes as a single class represented as $0$. Through $n$ iterations, we produce $n$ hypothesis \n",
    "\n",
    "$\\begin{aligned}\n",
    "h^i(\\theta)  = P(y = i|x, \\theta) , ~ i = 1, 2 ....n \\\\\n",
    "prediction = \\mathop{max}_{i} h^i(\\theta)\n",
    "\\end{aligned} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 The Overfitting problem\n",
    "Among the model's performance metrics, is how well it fits the training data and how well it is expected to fit new unseen data. Therefore, a model might range from ***underfit*** to ***overfit***:\n",
    "1. ***underfit*** or ***high bias*** is when the model does not fit the training data well enough. *bias* term denotes that the model might incorporate assumptions made by the model's designder.\n",
    "2. ***overfit*** or ***high variance*** is when the model fits quite well with the data (perfectly possibly). Yet, it fails to be general enough to produce accurate predictions for unseen data. the term *variance* expresses the large variability required to fit the training data to a large extent.\n",
    "\n",
    "This problem might be addressed by two approaches.\n",
    "1. Reduce the number of features either manually or by a model selection algorithm\n",
    "2. Apply ***regularization***: keeping all features while decreasing their magnitudes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "## 1. Motivation\n",
    "Let's consider one of the main appliations of Deep Learning: computer vision. Previously all images considered were of the size 64 by 64. The input features would then be of $3 \\cdot 64 \\cdot 64 = 12288$ dimensional. New data and high resolution images are generally $10^3 \\cdot 10 ^ 3$ which means that input vector is $10 ^6$ dimentional. Assuming a neural network with $1000$ units in the first hidden layer, the first mapping matrix is already: $10 ^ 9$ parameters which is already quite large. DL researchers conceived the convulotion operations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutions\n",
    "### 2.1 The convulation operator\n",
    "The convolution operator is a binary matrix operator heavily used in convolutional networks. For mathematical explanation, consider the following [link](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/CNN/math_parts/Convolution_operator.pdf)\n",
    "### 2.2 Edge detection\n",
    "To detect vertical edges a number of matrices generally referred to as ***filters*** were conceived. Among the popular ones:\n",
    "1. $\\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1\\end{bmatrix}$ \n",
    "\n",
    "2. $\\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1\\end{bmatrix}$: sobel fitler\n",
    "\n",
    "3. $\\begin{bmatrix} 3 & 0 & -3 \\\\ 10 & 0 & -10 \\\\ 3 & 0 & -3\\end{bmatrix}$: schwarr fitler\n",
    "\n",
    "4. set the filter's values as learnable parameters instead of hard coding them.\n",
    "\n",
    "Such filters are used for vertical edge detection. The transposed matrices can be used to detect horizontal edges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Padding\n",
    "Assuming a picture of the shape $(s, s)$ and a fitler $(f, f)$ then the convolution operator produces a $(s - f + 1, s - f + 1)$. Such approach has certain shortcomings:\n",
    "1. The picture's size shrinks with every convolution operation: with a large NN, the image might get signficantly reduced by the latest layers\n",
    "2. The values at the corners are generally included few times. Thus the information they bring are easily lost\n",
    "\n",
    "Padding is a possible solution. Before applying convolution, a random number $p$ is selected and $2p$ rows and columns are added, hence obtaining a $(s + 2p, s + 2p)$ matrix. This way, it is possible to keep the matrix's size non-decreasing as well as make better use of the information in the corner cellls. \\\n",
    "There are two main approaches to padding:\n",
    "1. Valid: np padding\n",
    "2. Same: padding enough to keep the input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Striding\n",
    "In addition to padding, Deep Learning researchers introduced striding, or the notion of a ***stride***: the number of squares between two consecutive inner squares.In other words, we start by applying the weighted multiplication between the filter square and the square at the top left corner then we move to the next square $s$ squares away. Assuming we have a $(n, n)$ square and a $(f,f)$ filter then with padding $p$ and stride $s$. The resulting square would be of shape: $\\lfloor\\frac{n + 2p - f}{s}\\rfloor + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Convolution on 3d objects.\n",
    "Assuming we have a $(n_1, n_2,  n_c)$ array as well as a $(f_1,f_2, n_c)$ fiter. Then the result is a $(\\lfloor\\frac{n_1 + 2p - f_1}{s}\\rfloor + 1, \\lfloor\\frac{n_2 + 2p - f_2}{s}\\rfloor + 1)$. The value at each square in the resulting array is the sum of all pairwise convolutions of the $n_c$ individual filter's layers. Which is why it is a mathematical condition to have $n_c$ as a common value. \\\n",
    "The notion of 3d convolution enables the use of multiple filters at once, detecting different features at once. The image below is more expressive illustration provided by the amazing course of *CNN* by ***Andrew NG***:\n",
    "[image](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/CNN/Convolution_3D.png?raw=true)\n",
    "\n",
    "The square in the top left corner is the sum of weighted sum between the filter and the top left $(3, 3)$ squares in the red, green and blue layers. The same mechanism is applied to the rest of the squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional Neural Networks\n",
    "### 3.1 Convolutional Layer\n",
    "Starting with a $(n_{hi}, n_{wi}, n_{ci})$ array, we apply the the convolution operation to each of the $f$ fitlers, obtaining $f$ new arrays with shapes: $n_{ho}, n_{wo}$. Each of them is summed up with a bias unit $b_i$ and then applied an activation function element-wise. The final output of the layer would be a $n_{ho}, n_{wo}, f$\n",
    "\n",
    "where the $f$ previous arrays are stacked on one another.\n",
    "### 3.2 Parameters\n",
    "Among the crucial features of CNN is that the number of parameters in a single layer is architecture dependent and not input dependent. More specifically, the number of parameters in a layer is estimated as: $(n_{ho}\\cdot n_{wo} + 1) \\cdot f$. The input's size does not affect by any mean the number of parameters\n",
    "### 3.3 Notation\n",
    "For the $l$-th layer, the following holds:\n",
    "* $f^{[l]}$ = fitler size\n",
    "* $p^{[l]}$ = padding\n",
    "* $s^{[l]}$ = stride\n",
    "* $n_c^{[l - 1]}$ = number of filters: number of channels in the input\n",
    "* input: $(n_h^{[l - 1]} * n_w^{[l - 1]} * n_c^{[l - 1]})$\n",
    "* output: $(n_h^{[l]} * n_w^{[l]} * n_c^{[l]})$ where \n",
    "    * $n_h^{[l]} = \\lfloor\\frac{n_h^{[l - 1]} + 2p^{[l - 1]} - f^{[l - 1]}}{s^{[l - 1]}}\\rfloor + 1$\n",
    "    * $n_w^{[l]} = \\lfloor\\frac{n_w^{[l - 1]} + 2p^{[l - 1]} - f^{[l - 1]}}{s^{[l - 1]}}\\rfloor + 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Example: \n",
    "For a first simple CNN example, consider the following [illustration](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/CNN/CNN_example.png?raw=true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "* A convolution extracts certain features from an input image. The values within the 3d array referred to as ***filter*** determines the features extracted. \n",
    "* The 2d object resulting is referred to as the ***feature map***\n",
    "* Zero padding is quite useful to keep as much information about the edges as possible while approximately maintaining the input's size throughout the Network which enabless the creation of significantly deep networks.\n",
    "* Pooling layers gradually reduce the height and width of the input as they divide the input into regions and summarizing these regions in a single value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Pooling Layers\n",
    "In addition to Convolutional layers, there is also pooling layers. There are two main types: \n",
    "* max Pooling layers\n",
    "* average Pooling layers\n",
    "\n",
    "Pooling when applied to a $(n, n)$ matrix, filter of shape $(f * f)$ and stride $s$, the output is a matrix of size $(\\lfloor\\frac{n_- f_1}{s}\\rfloor + 1, \\lfloor\\frac{n_2 - f_2}{s}\\rfloor + 1)$ where each square is either the ***max*** or the ***average*** of the elements in the corresponding matrix.\n",
    "#### Example\n",
    "Consider the following [example](https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png).\n",
    "#### Pooling in 3D\n",
    "Extending the operation to higher dimensions is simply done by applying the exact operation channel (layer) wise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This series of notebooks was created to track and save notes from the recommended book  ***A First Course in Machine Learning*** as it lays out a theoretical foundation for ML in general. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Modeling\n",
    "## Least Squares approachs\n",
    "The main idea behind modeling is to be able to predict a certain target value denoted by $t$ given another set of values **$x$**. In other words, modeling servers to discover, find, (learn as we are in ML context) a functional dependency betwen $t$ and $x$ ($x$ can be anything: scalar to tensors...) if it exists. The gate to the modeling word is Linear Modeling: specifically Least Square Method.   \n",
    "The assumptions are simple:  \n",
    "1. $t = f(x, w)$ where $w$ is set of parameters (the more complex the phenomena is, the more paramters, we are likely to have)\n",
    "2. the relationship between $t$ and $x$ is linear. This might not be satisfied to the tiniest details, nevertheless, we try to find the values that describe the linear relationship best   \n",
    "### the best solution\n",
    "well I used the term **best**. Let's clear some ambiguity here. The best solution is the one that minimizes the **loss**. Here are you again throwing terms out of nowhere, where a loss is a mathematical function used to describe how close the model's preditions are to the real values. Therefore, is a modeling / (design ?) choice. In this section, we consider the following loss function:\n",
    "$\n",
    "\\begin{align}\n",
    "L_n = (t_n - f(x, w)) ^ 2\n",
    "\\end{align}\n",
    "$\n",
    "where $t_n$ is the $n$-th data point / example / row in the dataset in question. Thus, each data point is associated with a loss value. The total loss of the model: a numerical value that represents the model's ability to learn from the data is: \n",
    "$\\begin{align}\n",
    "L = \\frac{1}{N} \\cdot \\sum_{i = 1}^{N} L_i\n",
    "\\end{align}$ \n",
    "In other words, the average of all losses across the entire dataset.   \n",
    "This loss function was mainly chosen for having an analytical solution. Nevertheless, the modern computational power reduced the importance of mathematical convenience. In other words, the criteria for choosing a loss function is no longer its mathematical convenience (having an analytical solution), but its convenience to the data in question and thus the nature of the phenomenon being modeled."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution\n",
    "Well, let's not go too deep into the math here. The result along with its proof can be found through the following [link](https://github.com/ayhem18/Towards_Data_Science/blob/main/Mathematics/Linear%20Algebra/orthogonality%202.pdf)\n",
    "### Issues with the Linear model\n",
    "The linear model as its name suggests is limited to linear functions. Impactful problems are definitily too complex to be modeled as a simple line in $\\mathbb{R}^n$. Additionally, the linear model outputs a very precise value, the letter might not be too sensible in comparion to range-based outputs.\n",
    "### NonLinear response from a linear model\n",
    "The model can be extended/generalized as follows:\n",
    "$\n",
    "\\begin{align}\n",
    "t = f(w, g_i(x_i))\n",
    "\\end{align}\n",
    "$\n",
    "where $g_i$ is a function applied on the $i$-th feature. This function can be the identity function, polynomial or even non-linear complex function such as exponentials and logarithms. This helps overcome to an extent the limitations of linear models.\n",
    "### Generalization and over-fitting\n",
    "The value of the loss function on the training data might be a misleading indicator of the model's quality, more specifically its predictions' quality. decreasing the training loss function improves the model's performance to a certain point where the quality of the predictions deteriorate quickly. Determining the right complexity enabling the model to perform well on both seen and unseen data is quite a challenging task: managing the bias-variance tradeoff.\n",
    "### Validation and Cross-validation\n",
    "Among the model selection techniques we can mentioned: \n",
    "1. validation dataset: partitioning the data into two datasets: one for training, and another for testing the quality of the prediction. choose the model performing better on the 2nd dataset\n",
    "2. Kfold: is creating $K$ pairs of dataset by first defining $K$ ranges in the training dataset. Through $K$ iterations, one range (partition) is used as validation dataset and the rest is for training. The final evaluation is a statistic calculated out of the $k$ validation measures.\n",
    "\n",
    "Assuming $N$ datapoints, then some of the popular choices is to use $K << N$ for computational reasons.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intuition behind regularization\n",
    "One way to mathematically express a linear model's complexity is the sum squared of its parameters:\n",
    "$\n",
    "\\begin{align}\n",
    "w^T \\cdot w\n",
    "\\end{align}\n",
    "$\n",
    "Consequently, to both penalize very simple models as well as overly complex ones we modify the original loss function by introducing an extra term:\n",
    "$\n",
    "\\begin{align} \n",
    "L = \\frac{1}{N} \\cdot \\sum_{i = 1}^{N}  (t_n - w^T \\cdot x) ^ 2+ \\lambda \\cdot w^T \\cdot w\n",
    "\\end{align}\n",
    "$\n",
    "where $\\lambda$ represents the regularization term and controls the strength, or the cost of having over-complexity. Very large values of $\\lambda$ can lead to the model underfitting the training data, as its complexity upper bounded in a way that does not match the problem's."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Modeling: A maximum likelihood approach\n",
    "The linear model generates predictions that might with certain data points deviate significantly from the true values. It is not reasonable to defend such deviation. From a modeling perspective, a model that incorporates this uncertainty is more robust and more reliable. Thus, predictions would move from exact values to ranges. The mathematical tool to express uncertainty is probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### thinking generatively\n",
    "Let's change perspective here: the question to be asked now is \"Is it possible to build a model that generates a dataset quite similar to the one in question ?\". Modeling has an inheritent issue: the system / model built is definitely less complex than the one actually generating the data. Such an issue creates a discrepancy between the model and reality. This discrepancy is too significant to be ignored. One mechanism to tackle such discrepancy is to have a method that generates a random value representing the error. The latter will be added to the final prediction: suggesting a possible range of values associated with probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb0ce5cb6b092cde9f0ba713d915425207ed6ea08d3ede97530b87c251a3aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

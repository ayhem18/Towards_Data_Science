{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "## 1. Notation\n",
    "In the rest of these notes, the following notation will be used:\n",
    "* $n_u$: number of users (currently saved in the dataset)\n",
    "* $n_i$: number of items to be rated by the users: popular examples of item would be movies, products...\n",
    "* $r(i, j)$ a value representing whether the $j$-th user rated the $i$-th item\n",
    "* $y^{(i, j)}$ the rating given by user $j$ to the item $i$, only defined when $r(i, j) = 1$\n",
    "* $x^{(i)}$ features vector for item $i$\n",
    "* $\\theta^{(i)}$ parameter vector for user $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. First approach: Content-Based Recommendations\n",
    "Assuming each item has a set of $n$ features. For instance for movie items, the features would be the percentage of each gender, or category and for the a magazine product, the features might be affordability, quality, delivery, price. Given a data set of users and their ratings, the algorithm would find vectors $\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n)}$ such as the predicted rating of user $j$ for item $i$ would be $(\\theta^{(j)}) ^ T \\cdot x^{(i)}$. Linear models are one possible approach, where the cost function for one user (to learn parameter $\\theta^{(j)}$) can be written as:\n",
    "$\\begin{aligned} \n",
    "\\frac{1}{2} \\sum_{i:r(i,j)=1}^{} ((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n}(\\theta^{(j)}_k)^2\n",
    "\\end{aligned}$\n",
    "where $(i:r(i,j)=1)$ represents the set of indices $i$ that satisfy $r(i, j)=1$, informally the indices of movies rated by the user $j$ \\\n",
    "\n",
    "The general cost function for all users can be written as:\n",
    "$\\begin{aligned} \n",
    "\\frac{1}{2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1}^{} ((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(\\theta^{(j)}_k)^2\n",
    "\\end{aligned}$\n",
    "Such cost function is similar to the neural networks' cost function. The optimization problem can be solved using optimization algorithms such as gradient descent. The gradients are as follows:\n",
    "$\\begin{aligned}\n",
    "\\theta^{(j)}_k := \\theta^{(j)}_k - \\alpha \\cdot \\sum_{i:r(i,j)=1}^{} ((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)}) x^{(i)}_k ~ for ~ k = 0 \\\\\n",
    "\\theta^{(j)}_k := \\theta^{(j)}_k - \\alpha \\cdot (\\sum_{i:r(i,j)=1}^{}((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)}) x^{(i)}_k + \\lambda \\cdot (\\theta^{(j)}_k) ^ 2) ~ for ~ k \\neq 0\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collaborative filtering: a more efficient approach\n",
    "### 3.1 The iterative approach\n",
    "In the previous section, we assumed the presence of features vectors. Such an assumption is not always realistic as it might be extremely expensive or even impossible. Let's assume the users' preferences are provided: how important certain features are. Additionally, their ratings are provided as well. Using a linear model it is possible to predict (estimate) the features combination of the items. More formally, given $\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n)}$, it is possible to minimze the cost function for one item's features $x^{(i)}$:\n",
    "$\\begin{aligned}\n",
    "\\frac{1}{2} \\sum_{j:r(i,j)=1}^{} ((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n}(x^{(i)}_k)^2\n",
    "\\end{aligned}$\n",
    "where ${j:r(i,j)=1}$ represents the set of indices $j$ satisfying $r(i,j)=1$. Informally, it represents the set of users' indices who rated the item $i$. \n",
    "\n",
    "More generally, given $\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n_u)}$, to learn $x^{(1)}, x^{(2)}, ..., x^{(n_i)}$, the general function is optimized:\n",
    "$\\begin{aligned}\n",
    "\\frac{1}{2} \\sum_{i=1}^{ni}\\sum_{j:r(i,j)=1}^{} ((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_i}\\sum_{k=1}^{n}(x^{(i)}_k)^2\n",
    "\\end{aligned}$\n",
    "Therefore, we conclude that:\n",
    "1. given $x^{(1)}, x^{(2)}, ..., x^{(n_i)}$, we can estimate $\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n_u)}$\n",
    "2. given $\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n_u)}$, we can estimate $x^{(1)}, x^{(2)}, ..., x^{(n_i)}$\n",
    "\n",
    "Thus, one possible solution is to randomly initialize $\\Theta$, estimate $X$, provide a better estimate for $\\Theta$ and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Collaborative Filtering\n",
    "Although the iterative approach is of acceptable efficiency, it requires both additional time and computational power. Thus, a different approach would be to consider both $X$ and $\\Theta$ as simulateneously variables/parameters of this optimization problem. We recall the cost function with respect to $\\Theta$:\n",
    "$\\begin{aligned}\n",
    "\\frac{1}{2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1}^{} ((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(\\theta^{(j)}_k)^2\n",
    "\\end{aligned}$ \n",
    "as well as the cost function with respect to $X$:\n",
    "$\\begin{aligned}\n",
    "\\frac{1}{2} \\sum_{i=1}^{ni}\\sum_{j:r(i,j)=1}^{} ((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_i}\\sum_{k=1}^{n}(x^{(i)}_k)^2\n",
    "\\end{aligned}$\n",
    "Optimizing $X$ and $\\Theta$ simultaneoulsy:\n",
    "$\\begin{aligned}\n",
    "J(\\Theta, X) = \\frac{1}{2} \\sum_{(i,j):r(i,j)=1}^{}((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(\\theta^{(j)}_k)^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_i}\\sum_{k=1}^{n}(x^{(i)}_k)^2\n",
    "\\end{aligned}$\n",
    "It is crucial to note that:\n",
    "$\\begin{aligned} \n",
    "\\sum_{i=1}^{ni}\\sum_{j:r(i,j)=1}^{} = \\sum_{(i,j):r(i,j)=1}^{} = \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1}^{}\n",
    "\\end{aligned}$\n",
    "Informally, \n",
    "\n",
    "the set of users who rated item $i$ for all items $i$ = the set of pairs (item, user), $(i, j)$ where user $j$ rated item $i$ = the set of items rated by user $j$ for all users $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Final algorithm\n",
    "1. Initialize $x^{(1)}, x^{(2)}, ..., x^{(n_i)}$,  $\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n_u)}$ to small random values to break the symmetry\n",
    "2. Minimize $J(\\Theta, X)$ using an optimazation algorithm:\n",
    "* $\\theta^{(j)}_k := \\theta^{(j)}_k - \\alpha \\cdot \\sum_{(i,j):r(i,j)=1}^{}((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)}) x^{(i)}_k + \\lambda \\cdot (\\theta^{(j)}_k) ^ 2$\n",
    "* $x^{(i)}_k := x^{(i)}_k - \\alpha \\cdot \\sum_{(i,j):r(i,j)=1}^{}((\\theta^{(j)}) ^ T \\cdot x^{(i)} - y^{(i, j)}) \\theta^{(j)}_k + \\lambda \\cdot (x^{(i)}_k) ^ 2$ \n",
    "3. for a user with paramters $\\theta$ an item with features $x$, predict a rating of $\\theta^{T}x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Implementation notes\n",
    "#### 3.4.1 matrices representations and vectorization\n",
    "We denote \n",
    "$X = \\begin{bmatrix}(X^{(1)})^T  \\\\\n",
    "(X^{(2)})^ T \\\\\n",
    "... \\\\\n",
    "... \\\\\n",
    "(X^{(n_i)})^ T\\end{bmatrix}$ and \n",
    "$\\Theta = \\begin{bmatrix}(\\theta^{(1)})^T  \\\\\n",
    "(\\theta^{(2)})^ T \\\\\n",
    "... \\\\\n",
    "... \\\\\n",
    "(\\theta^{(n_u)})^ T\\end{bmatrix}$\n",
    "The predictive ratings's matrix:\n",
    "$\\begin{bmatrix}\n",
    "(\\theta^{(1)}) ^ T x^{(1)} && (\\theta^{(2)}) ^ T \\cdot x^{(1)} && .. && (\\theta^{(n_u)}) ^ T x^{(1)}\\\\\n",
    "(\\theta^{(1)}) ^ T x^{(2)} && (\\theta^{(2)}) ^ T \\cdot x^{(2)} && .. && (\\theta^{(n_u)}) ^ T x^{(2)} \\\\\n",
    ".. && .. &&.. && .. \\\\\n",
    "(\\theta^{(1)}) ^ T x^{(n_i)} && (\\theta^{(2)}) ^ T \\cdot x^{(n_i)} && .. && (\\theta^{(n_u)}) ^ T x^{(n_i)}\n",
    "\\end{bmatrix}$\n",
    "can be vectorized as follows: $X \\cdot \\Theta ^ T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Mean normalization\n",
    "Considering user $l$ who have not yet rated any item, optimizing the cost function $J(\\Theta, X)$ would be reduced to optimizing \n",
    "$\\begin{aligned}\n",
    "\\sum_{k=1}^{n}(\\theta^{(l)}_k)^2\n",
    "\\end{aligned}$\n",
    "whose solution is $\\theta^{(l)}_k = 0, ~ k = 1,2,...,n$. Thus every new user will be assigned $\\theta_{new} = 0$ and all their predective rating will be estimated as $0$.\n",
    "\n",
    "Mean normalization can overcome such issue. The initial rating matrix $R$. replace every rating $r(i,j)$ by $r(i,j) - \\mu_i$ where $\\mu_i$ is the average rating of item$i$. Additionally, the predictive rating is no longer $\\theta^{T}x$, but $~\\theta^{T}x + \\mu_i$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

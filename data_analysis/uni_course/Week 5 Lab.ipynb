{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e4295-3cc5-484a-9327-aa6d3d6e4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_moons, fetch_20newsgroups\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af2e1d-e2cd-447d-bac2-0b31d1b58861",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this section, we will implement the Logistic Regression algorithm from scratch using **Gradient Descent** to minimize the binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c4d21-ebf9-432b-af73-4cbb464ec138",
   "metadata": {},
   "source": [
    "## Logistic Regression Formula:\n",
    "\n",
    "- Logistic Regression predicts a probability $( P(y=1|X) )$ using the sigmoid function:\n",
    "$$\n",
    "  [\n",
    "  h_{\\theta}(X) = \\frac{1}{1 + e^{-\\theta^T X}}\n",
    "  ]\n",
    "$$\n",
    "- The **loss function** for Logistic Regression (binary cross-entropy loss) is:\n",
    "$$\n",
    "  [\n",
    "  J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]\n",
    "  ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bdd564-9a94-4299-9f52-a7ce4ccd2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Logistic Regression cost function\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    cost = (-1 / m) * (y.T @ np.log(h) + (1 - y).T @ np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "# Gradient Descent for Logistic Regression\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    costs = []\n",
    "    for _ in range(iterations):\n",
    "        theta -= (alpha / m) * (X.T @ (sigmoid(X @ theta) - y))\n",
    "        costs.append(compute_cost(X, y, theta))\n",
    "    return theta, costs\n",
    "\n",
    "# Generate some binary classification data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 2)\n",
    "X = np.c_[np.ones(X.shape[0]), X]  # Add intercept\n",
    "y = (X[:, 1] + X[:, 2] > 0).astype(int).reshape(-1, 1)  # Linearly separable labels\n",
    "\n",
    "# Initialize theta\n",
    "theta_init = np.zeros((X.shape[1], 1))\n",
    "\n",
    "# Train Logistic Regression with Gradient Descent\n",
    "theta, costs = gradient_descent(X, y, theta_init, alpha=0.01, iterations=10000)\n",
    "print(\"Learned parameters (theta):\", theta.ravel())\n",
    "\n",
    "# Predict function\n",
    "def predict(X, theta):\n",
    "    return sigmoid(X @ theta) >= 0.5\n",
    "\n",
    "# Accuracy of predictions\n",
    "preds = predict(X, theta)\n",
    "accuracy = np.mean(preds == y) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a637d70-4cbb-4920-992b-4c47641de01a",
   "metadata": {},
   "source": [
    "## Logistic Regression with Scikit-learn\n",
    "\n",
    "Use scikit-learn’s `LogisticRegression` to replicate the results obtained from the manual implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269151c-458f-4d6c-9ceb-ca0361941265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scikit-learn's Logistic Regression\n",
    "model = LogisticRegression(fit_intercept=True, solver='lbfgs')\n",
    "model.fit(X[:, 1:], y.ravel())  # Omit the intercept column since fit_intercept is True\n",
    "\n",
    "# Predict and compare results\n",
    "y_pred = model.predict(X[:, 1:])\n",
    "accuracy_sklearn = accuracy_score(y, y_pred) * 100\n",
    "print(f\"Accuracy (scikit-learn): {accuracy_sklearn:.2f}%\")\n",
    "\n",
    "# Compare learned parameters\n",
    "print(\"Learned parameters (scikit-learn):\", model.intercept_, model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e4161-b0f2-4a1d-b772-fdf96de2262e",
   "metadata": {},
   "source": [
    "## Polynomial Logistic Regression and Regularization\n",
    "\n",
    "Implement **Polynomial Logistic Regression** and apply **Regularization** (Ridge/Lasso) to prevent overfitting.\n",
    "- Use scikit-learn’s `PolynomialFeatures` to create polynomial terms (e.g., quadratic terms for each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b692b-1f1b-43f7-a71e-6e4b57c4218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear dataset (moons)\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"Moons Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c602eb0-9008-42cf-95d1-cce63db6ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Logistic Regression (without polynomial features)\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Polynomial Logistic Regression\n",
    "poly = PolynomialFeatures(degree=...) # Todo: try to fine-tune\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "model_poly = LogisticRegression()\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predictions and Accuracy\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "y_pred_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr) * 100\n",
    "accuracy_poly = accuracy_score(y_test, y_pred_poly) * 100\n",
    "\n",
    "print(f\"Accuracy with Logistic Regression: {accuracy_lr:.2f}%\")\n",
    "print(f\"Accuracy with Polynomial Logistic Regression: {accuracy_poly:.2f}%\")\n",
    "\n",
    "# Plot decision boundaries\n",
    "\n",
    "def plot_decision_boundary(model, X, y, poly=None, ax=None, title=\"Decision Boundary\"):\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    # If PolynomialFeatures is used, transform the mesh grid\n",
    "    if poly is not None:\n",
    "        X_grid = poly.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Predict using the model\n",
    "    Z = model.predict(X_grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot contour and training examples\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', cmap='viridis')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Plot the decision boundary for standard Logistic Regression\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plot_decision_boundary(model_lr, X_test, y_test, ax=ax1, title=f\"Logistic Regression\\nAccuracy: {accuracy_lr:.2f}%\")\n",
    "\n",
    "# Plot the decision boundary for Polynomial Logistic Regression\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plot_decision_boundary(model_poly, X_test, y_test, poly=poly, ax=ax2, title=f\"Polynomial Logistic Regression\\nAccuracy: {accuracy_poly:.2f}%\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd999c-ac8a-4844-a737-1e2089670c4f",
   "metadata": {},
   "source": [
    "### Regularization:\n",
    "\n",
    "- Introduce **Ridge (L2)** and **Lasso (L1)** regularization to avoid overfitting when using high-degree polynomial features.\n",
    "- In scikit-learn, this can be controlled with the `C` parameter (inverse of regularization strength)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25649855-e964-41ac-aeec-0547d1ab14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge (L2) Regularization\n",
    "model_ridge = LogisticRegression(penalty='l2', C=0.1)  # C is the inverse of lambda\n",
    "model_ridge.fit(X_train_poly, y_train.ravel())\n",
    "print(f\"Accuracy with Ridge Regularization: {accuracy_score(y_test, model_ridge.predict(X_test_poly)) * 100:.2f}%\")\n",
    "\n",
    "# Lasso (L1) Regularization\n",
    "model_lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)\n",
    "model_lasso.fit(X_train_poly, y_train.ravel())\n",
    "print(f\"Accuracy with Lasso Regularization: {accuracy_score(y_test, model_lasso.predict(X_test_poly)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee528a-7820-47c5-acc3-0b6cb0297c13",
   "metadata": {},
   "source": [
    "# Real Dataset\n",
    "\n",
    "Apply Logistic Regression to the **20 Newsgroups** dataset, which is commonly used for text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f4879-0551-43cb-b1e0-cc002e89e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Todo: encode the data from text to vectors\n",
    "...\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae4ab5-79b5-46f0-85e0-5ee6774e3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Logistic Regression on your features\n",
    "model_news = LogisticRegression()\n",
    "model_news.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3294a5-4959-44af-8865-401f083a548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_news = model_news.predict(X_test)\n",
    "accuracy_news = accuracy_score(y_test, y_pred_news)\n",
    "print(f\"Accuracy on 20 Newsgroups: {accuracy_news * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "class_names = newsgroups.target_names\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_news, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred_news)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099d467-77a0-4106-b13e-328bb56f6f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

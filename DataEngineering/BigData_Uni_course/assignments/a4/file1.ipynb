{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('GBvideos.csv', nrows=38000)\n",
    "d.to_csv('GBvideos_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"my spark app\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType, TimestampType, LongType\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"video_id\", StringType(), ),\n",
    "    StructField(\"trending_date\", StringType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"channel_title\", StringType(), False),\n",
    "    StructField(\"category_id\", IntegerType(), False),\n",
    "    StructField(\"publish_time\", TimestampType(), False),\n",
    "    StructField(\"tags\", StringType(), False),\n",
    "    StructField(\"views\", LongType(), False),\n",
    "    StructField(\"likes\", IntegerType(), False),\n",
    "    StructField(\"dislikes\", IntegerType(), False),\n",
    "    StructField(\"comment_count\", IntegerType(), False),\n",
    "    StructField(\"thumbnail_link\", StringType(), False),\n",
    "    StructField(\"comments_disabled\", BooleanType(), False),\n",
    "    StructField(\"ratings_disabled\", BooleanType(), False),\n",
    "    StructField(\"video_error_or_removed\", BooleanType(), False),\n",
    "    StructField(\"description\", StringType(), False),    \n",
    "])\n",
    "\n",
    "# You can also use spark.read.csv function\n",
    "df = spark.read.format(\"csv\").load(\"GBvideos.csv\", header = True, schema = schema)\n",
    "df.createOrReplaceTempView(\"videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|   video_id|dis_likes_difference|\n",
      "+-----------+--------------------+\n",
      "|VYOjWnS4cMY|           138849717|\n",
      "|xpVfcZ0ZcFM|           115703926|\n",
      "|kLpH1nSLJSs|           101196276|\n",
      "|ffxKSjUwKdU|            93591008|\n",
      "|7C2z4GqqS5E|            69445285|\n",
      "|_I_D_8Z4sJE|            57760713|\n",
      "|kX0vO4vlJuU|            52999142|\n",
      "|tCXGJQYZ9JA|            52839411|\n",
      "|8O_MwlZ2dEg|            52579739|\n",
      "|9jI-z9QN6g8|            50715687|\n",
      "+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 1\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "        SELECT video_id,  SUM(likes) - SUM(dislikes) as dis_likes_difference from videos\n",
    "        GROUP BY video_id\n",
    "        ORDER BY dis_likes_difference DESC;          \n",
    "          \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|      channel_title|         avg_likes|\n",
      "+-------------------+------------------+\n",
      "|ChildishGambinoVEVO| 4122446.027777778|\n",
      "|          Bad Bunny| 2957022.972222222|\n",
      "|   ArianaGrandeVevo| 2786633.285714286|\n",
      "|      LuisFonsiVEVO|         2686169.0|\n",
      "|          DrakeVEVO|         2389258.0|\n",
      "|            ibighit| 2359758.066666667|\n",
      "|  YouTube Spotlight| 1935720.037037037|\n",
      "|      Flow La Movie|1621740.7272727273|\n",
      "|         BeckyGVEVO| 1439571.619047619|\n",
      "|         Ed Sheeran|     1423959.96875|\n",
      "+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 2\n",
    "\n",
    "spark.sql(\"\"\" \n",
    "SELECT channel_title, AVG(likes) as avg_likes\n",
    "FROM videos\n",
    "GROUP BY channel_title\n",
    "ORDER BY avg_likes DESC;\n",
    "          \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|       channel_title|total_views_count|\n",
      "+--------------------+-----------------+\n",
      "|          NickyJamTV|       8516190092|\n",
      "|               Ozuna|       8305198063|\n",
      "|           Bad Bunny|       6891280759|\n",
      "|           DrakeVEVO|       6581834413|\n",
      "| ChildishGambinoVEVO|       6101309613|\n",
      "|       Flow La Movie|       5151438858|\n",
      "|    ArianaGrandeVevo|       4107436350|\n",
      "|Marvel Entertainment|       3993421220|\n",
      "|    jypentertainment|       3624070589|\n",
      "|          Ed Sheeran|       3589056783|\n",
      "+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT channel_title, SUM(views) as total_views_count\n",
    "FROM videos\n",
    "group by channel_title\n",
    "having SUM(views) > POWER(10, 6)\n",
    "ORDER BY  total_views_count DESC;          \n",
    "          \"\"\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------+\n",
      "|           channel_title|   avg(tag_count)|\n",
      "+------------------------+-----------------+\n",
      "|          특이한동물채널|             78.0|\n",
      "|                CCTV春晚|             67.0|\n",
      "|                theCHIVE|             63.0|\n",
      "|                 Sanadsk|             61.0|\n",
      "|             jessiepaege|             59.0|\n",
      "|영국남자 Korean Engli...|             58.0|\n",
      "|               Jake Paul|56.27272727272727|\n",
      "|         Rosanna Pansino|             56.0|\n",
      "|                    KARD|             56.0|\n",
      "|           Daniel Howell|             53.0|\n",
      "+------------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, size\n",
    "df.select('channel_title', size(split(df.tags, r'\\|', -1)).alias('tag_count')).groupBy(\"channel_title\").mean().orderBy('avg(tag_count)', ascending=False).show(10)# .orderBy('tag_average_mean').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|individual_tag|count|\n",
      "+--------------+-----+\n",
      "|       \"funny\"| 2629|\n",
      "|      \"comedy\"| 2322|\n",
      "|       \"music\"| 1972|\n",
      "|         \"Pop\"| 1556|\n",
      "|        \"2018\"| 1383|\n",
      "|       \"video\"| 1189|\n",
      "| \"music video\"| 1159|\n",
      "|     \"Records\"| 1137|\n",
      "|   \"interview\"| 1124|\n",
      "|    \"official\"| 1106|\n",
      "+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "# a temporary table where the 'tags' column is converted to the array data type\n",
    "df_temp = df.select((split(df.tags, r'\\|', -1)).alias('tags_list'))\n",
    "# df_temp.select(flatten(\"individual_tag\")).show()\n",
    "df_temp.select(explode('tags_list').alias('individual_tag')).groupBy('individual_tag').count().where(\"individual_tag != '[none]'\").orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|       channel_title|max_comments_count|\n",
      "+--------------------+------------------+\n",
      "|    Logan Paul Vlogs|           1626501|\n",
      "|             ibighit|           1228655|\n",
      "|   YouTube Spotlight|            845233|\n",
      "| ChildishGambinoVEVO|            553371|\n",
      "|Marvel Entertainment|            368739|\n",
      "|           DrakeVEVO|            301756|\n",
      "|         Lucas Lucco|            275795|\n",
      "|    jypentertainment|            274087|\n",
      "|    ArianaGrandeVevo|            259613|\n",
      "|          HowToBasic|            225618|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "\n",
    "SELECT channel_title, MAX(comment_count) as max_comments_count\n",
    "FROM videos\n",
    "GROUP BY channel_title\n",
    "ORDER BY max_comments_count DESC\n",
    "\n",
    "          \"\"\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"my spark app\") \\\n",
    "        .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = sc.textFile(\"GB_category_id.json\")\n",
    "json_reduced = json_object.reduce(lambda x, y: \"\\n\".join([x,y]))\n",
    "import re\n",
    "a = re.findall(r'\"id\".*:.*\"(\\w+)\".*', json_reduced)\n",
    "b = re.findall(r'\"title\".*:.*\"(\\w+)\"', json_reduced)\n",
    "categories = dict(zip(map(int, a),b))\n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"FRvideos.csv\", header=True)\n",
    "rdd = df.rdd \n",
    "rdd0 = rdd.map(lambda x: (x['title'], x['views'], x['channel_title'],))\n",
    "rdd1 = rdd0.groupBy(lambda x: x[2])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "QUESTION 1\n",
      "####################################################################################################\n",
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: date (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- views: long (nullable = true)\n",
      " |-- likes: long (nullable = true)\n",
      " |-- dislikes: long (nullable = true)\n",
      " |-- comment_count: long (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: boolean (nullable = true)\n",
      " |-- ratings_disabled: boolean (nullable = true)\n",
      " |-- video_error_or_removed: boolean (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"my spark app\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(\"\\n\\nQUESTION 1\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "  StructField('video_id', StringType(), True),\n",
    "  StructField('trending_date', DateType(), True),\n",
    "  StructField('title', StringType(), True),\n",
    "  StructField('channel_title', StringType(), True),\n",
    "  StructField('category_id', IntegerType(), True),\n",
    "  StructField('publish_time', TimestampType() , True),\n",
    "  StructField('tags', StringType(), True),\n",
    "  StructField('views', LongType(), True),\n",
    "  StructField('likes', LongType(), True),\n",
    "  StructField('dislikes', LongType(), True),\n",
    "  StructField('comment_count', LongType(), True),\n",
    "  StructField('thumbnail_link', StringType(), True),\n",
    "  StructField('comments_disabled', BooleanType(), True),\n",
    "  StructField('ratings_disabled', BooleanType(), True),\n",
    "  StructField('video_error_or_removed', BooleanType(), True),\n",
    "  StructField('description', StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"FRvideos.csv\", header=True, schema=schema)\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn('tags', split(col('tags'), r'\\|'))\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nQUESTION 2\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.select('video_id', 'publish_time', (df.likes - df.dislikes).alias('delta')). \\\n",
    "                            where(df.publish_time < '2010-01-01').orderBy('delta', ascending=False). \\\n",
    "                            show(10)\n",
    "\n",
    "\n",
    "print(\"\\n\\nQUESTION 2\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "\n",
    "df.groupby('channel_title').agg(avg('likes').alias('average_likes')).orderBy('average_likes', ascending=False).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nQUESTION 3\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "df.select('channel_title', 'views').groupby('channel_title').sum().filter('sum(views) > 1000000').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       correlation|\n",
      "+------------------+\n",
      "|0.8125691299632112|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(corr('likes', 'views').alias('correlation')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|       channel_title|   avg(tag_count)|\n",
      "+--------------------+-----------------+\n",
      "|        Yoann Leroux|             78.0|\n",
      "|     Official Hyolyn|             76.5|\n",
      "|       Ficko Primera|             75.0|\n",
      "|        CocoCam Jump|72.33333333333333|\n",
      "|         RallyPasion|             72.0|\n",
      "|               TraKz|             71.5|\n",
      "|Joshua - Réalisat...|             68.0|\n",
      "|       Zvezde Granda|             67.0|\n",
      "|            Voltor63|             67.0|\n",
      "|   Checkpoint Rallye|             65.0|\n",
      "+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('channel_title', size('tags').alias('tag_count')). \\\n",
    "groupBy(\"channel_title\").mean().\\\n",
    "orderBy('avg(tag_count)', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|individual_tag|frequency|\n",
      "+--------------+---------+\n",
      "|    \"\"humour\"\"|     1513|\n",
      "|      \"\"2018\"\"|      963|\n",
      "|  \"\"football\"\"|      791|\n",
      "|    \"\"france\"\"|      599|\n",
      "|      \"\"2017\"\"|      529|\n",
      "|       \"\"rap\"\"|      481|\n",
      "|     \"\"video\"\"|      463|\n",
      "| \"\"freestyle\"\"|      456|\n",
      "|   \"\"musique\"\"|      444|\n",
      "|      \"\"live\"\"|      436|\n",
      "+--------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(explode('tags').alias('individual_tag')). \\\n",
    "groupBy('individual_tag').count().where(\"individual_tag != '[none]'\"). \\\n",
    "orderBy('count', ascending=False).withColumnRenamed('count', 'frequency').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|       channel_title|max(comment_count)|\n",
      "+--------------------+------------------+\n",
      "|             ibighit|           1040912|\n",
      "|   YouTube Spotlight|            827755|\n",
      "|    Logan Paul Vlogs|            611327|\n",
      "|           AuronPlay|            315801|\n",
      "|        David Dobrik|            264405|\n",
      "| ChildishGambinoVEVO|            232723|\n",
      "|Marvel Entertainment|            177598|\n",
      "|    ArianaGrandeVevo|            176926|\n",
      "|            Lil pump|            161259|\n",
      "|         AsapSCIENCE|            141646|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"channel_title\").max(\"comment_count\").orderBy(desc(\"max(comment_count)\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def match(description):\n",
    "    if description is not None:\n",
    "        return [list(re.findall(r\"https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", description)),\n",
    "                list(re.findall(\"\\B\\@\\w+\", description))]\n",
    "    else :\n",
    "        return [[], []]\n",
    "\n",
    "regex_udf = udf(match, ArrayType(ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"title\", \"description\", regex_udf(col(\"description\"))).filter(col(\"title\").isNotNull() & col(\"description\").isNotNull()).withColumn(\"links\", col(\"match(description)\").getItem(0)).withColumn(\"mentions\", col(\"match(description)\").getItem(1)).orderBy(\"title\").drop(\"description\").drop(\"match(description)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"title\", \"description\", regex_udf(col(\"description\"))).where('title is not null and description is not null'). \\\n",
    "withColumn(\"links\", col(\"match(description)\").getItem(0)). \\\n",
    "withColumn(\"mentions\", col(\"match(description)\").getItem(1)). \\\n",
    "orderBy(\"title\").select('title', 'mentions', 'links').show() # drop(\"description\").drop(\"match(description)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the scan into different brain sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# let's see how it goes\n",
    "npy_file_path = os.path.join(current_dir, 'ihb.npy')\n",
    "\n",
    "all_data = np.load(npy_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 10, 246)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      160\n",
       "460    160\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "num_nans = [np.isnan(all_data[i]).sum() for i in range(all_data.shape[0])]\n",
    "nan_values = pd.Series(num_nans)\n",
    "nan_values.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = [all_data[i] for i in range(all_data.shape[0]) if np.isnan(all_data[i]).sum() == 0]\n",
    "g2 = [all_data[i] for i in range(all_data.shape[0]) if np.isnan(all_data[i]).sum() == 460]\n",
    "\n",
    "g2_nan_distribution = np.zeros(shape=(len(g2), len(g2)))\n",
    "\n",
    "for i1, element1 in enumerate(g2):\n",
    "\tfor i2, element2 in enumerate(g2):\n",
    "\t\tg2_nan_distribution[i1][i2] = all(np.sum(np.isnan(element1), axis=1) == np.sum(np.isnan(element2), axis=1))\n",
    "\n",
    "g2 = [scan[:, :-46] for scan in g2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first step: group scans of the same scan but with different smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know for a fact that we have 160 samples belonging to 20 subjects each having\n",
    "\n",
    "# let's calculate the auto correlation of each time sequence in each of scan\n",
    "from typing import Union, List\n",
    "from scipy.signal import correlate2d, correlate\n",
    "\n",
    "def autocorrelation_stats(scan: np.ndarray, aggregate:bool=True) -> Union[List, float]:\t\n",
    "\tassert len(scan) == 10\n",
    "\tauto_correlations =  [float(correlate(scan[i:], scan[:-i])) for i in range(1, 6)]\n",
    "\tif aggregate:\n",
    "\t\treturn np.mean(auto_correlations)\n",
    "\treturn auto_correlations\n",
    "\n",
    "def build_ac_pairs(scans: List[np.ndarray]) -> set:\n",
    "\tauto_corrs = np.zeros(shape=(len(scans), len(scans)))\n",
    "\n",
    "\tfor i1, element1 in enumerate(scans):\n",
    "\t\tfor i2, element2 in enumerate(scans):\n",
    "\t\t\tauto_corrs[i1][i2] = correlate2d(element1, element2, \"valid\").item()\t\n",
    "\t\n",
    "\t# for each row, row[0] represents the closest index to scan[i] in terms of auto correlation\n",
    "\t# row[1] represents the same index\n",
    "\tpaired_scans_by_ac = np.argsort(auto_corrs, axis=-1)[:, -2:]\n",
    "\n",
    "\tpairs = set()\n",
    "\n",
    "\tfor i in range(len(scans)):\n",
    "\t\tassert paired_scans_by_ac[i, 1] == i, \"check the code\"\n",
    "\t\tclosest_scan_index = paired_scans_by_ac[i, 0]\n",
    "\t\tif paired_scans_by_ac[closest_scan_index, 0] == i and (closest_scan_index, i) not in pairs:\n",
    "\t\t\tpairs.add((i, closest_scan_index)) \n",
    "\n",
    "\treturn pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_pairs, g2_pairs = build_ac_pairs(g1), build_ac_pairs(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g1_pairs), len(g2_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_segment_rep(scans: List[np.ndarray], pairs_indices: set) -> List[np.ndarray]:\n",
    "\tavg_segments = []\n",
    "\tfor i1, i2 in pairs_indices:\n",
    "\t\ts1, s2  = scans[i1], scans[i2]\n",
    "\t\tif s1.shape != s2.shape:\n",
    "\t\t\traise ValueError(\"Make sure the code is correct. found pairs with different shapes\")\n",
    "\t\tavg_segments.append((s1 + s2) / 2)\n",
    "\treturn avg_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_g1, avg_g2 = unified_segment_rep(g1, g1_pairs), unified_segment_rep(g2, g2_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auto_corr_concatenation(seg1: np.ndarray, seg2: np.ndarray):\n",
    "\tassert seg1.shape == seg2.shape, \"both segments same shape\"\n",
    "\tc_seg1 = np.concatenate([seg1, seg2], axis=0)\n",
    "\tc_seg2 = np.concatenate([seg2, seg1], axis=0)\n",
    "\n",
    "\tassert c_seg1.shape[0] == 2 * seg1.shape[0] and c_seg1.shape[1] == seg1.shape[1], \"concatenation correct\"\n",
    "\tassert c_seg2.shape[0] == 2 * seg2.shape[0] and c_seg2.shape[1] == seg2.shape[1], \"concatenation correct\"\n",
    "\n",
    "\tc1 = np.mean([correlate2d(c_seg1[i:i + len(seg1), :], seg1, \"valid\").item() for i in range(len(seg1))])\n",
    "\tc2 = np.mean([correlate2d(c_seg2[i:i + len(seg1), :], seg2, \"valid\").item() for i in range(len(seg1))])\n",
    "\treturn max(c1, c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step: grouping the different segments of the same scan sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_time_stamps(corrs: np.ndarray, k_closest:int):\n",
    "\tclosest_groups = np.argsort(corrs, axis=-1)[:, -k_closest:-1]\t\n",
    "\n",
    "\tpossible_pairs = set()\n",
    "\n",
    "\tfor i in range(len(corrs)):\n",
    "\t\tpossible_neighbors = closest_groups[i]\n",
    "\t\tfor pn in possible_neighbors:\n",
    "\t\t\tif i in closest_groups[pn]:\n",
    "\t\t\t\tpossible_pairs.add((i, pn))\n",
    "\n",
    "\treturn possible_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and variance of each brain feature\n",
    "g1_feats_mean = np.concatenate([np.mean(x, axis=0, keepdims=True) for x in avg_g1]).T\n",
    "corr_g1 = np.corrcoef(g1_feats_mean, rowvar=False)\n",
    "pairs_g1 = find_consecutive_time_stamps(corr_g1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 62)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_g1), len(set(chain.from_iterable(pairs_g1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 4, 6, 10, 13, 20, 21, 31, 34, 41, 45, 48, 55, 60, 65, 68, 71, 76}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "paired_indices = set(chain.from_iterable(pairs_g1))\n",
    "non_paired_indices = set(list(range(80))).difference(paired_indices)\n",
    "non_paired_indices_list = sorted(list(non_paired_indices))\n",
    "non_paired_indices_set = non_paired_indices\n",
    "non_paired_indices_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the same idea on non correlated points\n",
    "non_paired_feats = g1_feats_mean[:, non_paired_indices_list]\n",
    "non_paired_samples_corr = np.corrcoef(non_paired_feats, rowvar=False)\n",
    "non_paired_samples_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paired_second_batch = find_consecutive_time_stamps(non_paired_samples_corr, 2)\n",
    "paired_second_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_g1.update(paired_second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20, 21, 31, 34, 41, 45, 48, 55, 60, 65, 68, 71, 76}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(range(80))).difference(set(chain.from_iterable(pairs_g1)))\n",
    "\n",
    "pairs_g1.update([(20, 21), (31, 34), (41, 45), (48, 55), (60, 65), (68, 71), 76])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 14, 36],\n",
       "       [66, 19, 43],\n",
       "       [41, 11, 21],\n",
       "       [70, 58, 13],\n",
       "       [67, 10, 44],\n",
       "       [ 1, 45, 43],\n",
       "       [ 7,  6, 66],\n",
       "       [70, 48, 61],\n",
       "       [30, 53, 33],\n",
       "       [ 6, 11, 10],\n",
       "       [ 8, 43, 20],\n",
       "       [31, 78, 32],\n",
       "       [77, 36, 51],\n",
       "       [53, 58, 68],\n",
       "       [50, 66, 23],\n",
       "       [ 0, 60, 25],\n",
       "       [73, 32, 76],\n",
       "       [17, 66, 12]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_by_mean = np.argsort(corr_g1, axis=-1)[:, -4:-1]\n",
    "closest_by_mean = closest_by_mean[non_paired_indices_list, :]\n",
    "closest_by_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 41),\n",
       " (6, 21),\n",
       " (10, 13),\n",
       " (13, 10),\n",
       " (20, 45),\n",
       " (21, 6),\n",
       " (31, 48),\n",
       " (41, 6),\n",
       " (41, 10),\n",
       " (45, 20),\n",
       " (48, 31),\n",
       " (60, 68),\n",
       " (68, 60),\n",
       " (71, 76)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_best_pairing = []\n",
    "for row_index, row in enumerate(closest_by_mean):\t\n",
    "\tfor j in closest_by_mean[row_index]:\n",
    "\t\tif j in non_paired_indices_set:\n",
    "\t\t\t_best_pairing.append((non_paired_indices_list[row_index], j))\n",
    "\n",
    "_best_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 41), (6, 21), (10, 13), (13, 10), (20, 45), (21, 6), (31, 48), (41, 6), (45, 20), (48, 31), (60, 68), (68, 60)]\n"
     ]
    }
   ],
   "source": [
    "_repeated_pairing = []\n",
    "# add the new pairs to the existing pairs\n",
    "for (i1, i2) in _best_pairing:\n",
    "    if (i2, i1) in _best_pairing:\n",
    "        _repeated_pairing.append((i1, i2))\n",
    "        \n",
    "print(_repeated_pairing)\n",
    "_repeated_pairing.remove((6, 41))\n",
    "_repeated_pairing.remove((41, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 72)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_g1.update(_repeated_pairing)\n",
    "len(pairs_g1), len(set(chain.from_iterable(pairs_g1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(range(80))).difference(set(chain.from_iterable(pairs_g1)))\n",
    "random_pairs = [(2, 4), (34, 41), (55, 65), (71, 76)]\n",
    "pairs_g1.update(random_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2_feats_mean = np.concatenate([np.mean(x, axis=0, keepdims=True) for x in avg_g2]).T\n",
    "# corr_feat_var = np.corrcoef(feats_var)\n",
    "corr_g2 = np.corrcoef(g2_feats_mean, rowvar=False)\n",
    "# pairs_with_var = find_consecutive_time_stamps(corr_feat_var, 2)\n",
    "pairs_g2 = find_consecutive_time_stamps(corr_g2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 4, 18, 28, 37, 39, 41, 43, 46, 49, 54, 57, 60, 69, 73, 76, 78}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_indices = set(chain.from_iterable(pairs_g2))\n",
    "non_paired_indices = set(list(range(80))).difference(paired_indices)\n",
    "non_paired_indices_list = sorted(list(non_paired_indices))\n",
    "non_paired_indices_set = non_paired_indices\n",
    "non_paired_indices_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 44, 42],\n",
       "       [15, 14, 36],\n",
       "       [66, 19, 43],\n",
       "       [73, 75, 27],\n",
       "       [76, 33, 59],\n",
       "       [79, 78, 67],\n",
       "       [59,  0, 61],\n",
       "       [ 6, 11, 10],\n",
       "       [20,  4, 19],\n",
       "       [22, 47,  8],\n",
       "       [43, 15, 38],\n",
       "       [46, 12, 56],\n",
       "       [14, 23, 64],\n",
       "       [53, 58, 68],\n",
       "       [ 7, 66, 52],\n",
       "       [32, 33, 79],\n",
       "       [17, 66, 12],\n",
       "       [37, 67, 44]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_by_mean = np.argsort(corr_g1, axis=-1)[:, -4:-1]\n",
    "closest_by_mean = closest_by_mean[non_paired_indices_list, :]\n",
    "closest_by_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 43), (18, 73), (28, 76), (37, 78), (43, 4), (49, 43), (54, 46), (78, 37)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_best_pairing = []\n",
    "for row_index, row in enumerate(closest_by_mean):\t\n",
    "\tfor j in closest_by_mean[row_index]:\n",
    "\t\tif j in non_paired_indices_set:\n",
    "\t\t\t_best_pairing.append((non_paired_indices_list[row_index], j))\n",
    "\n",
    "_best_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 43), (37, 78), (43, 4), (78, 37)]\n"
     ]
    }
   ],
   "source": [
    "_repeated_pairing = []\n",
    "# add the new pairs to the existing pairs\n",
    "for (i1, i2) in _best_pairing:\n",
    "    if (i2, i1) in _best_pairing:\n",
    "        _repeated_pairing.append((i1, i2))\n",
    "        \n",
    "print(_repeated_pairing)\n",
    "pairs_g2.update(_repeated_pairing)\n",
    "# _repeated_pairing.remove((6, 41))\n",
    "# _repeated_pairing.remove((41, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 18, 28, 39, 41, 46, 49, 54, 57, 60, 69, 73, 76}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(range(80))).difference(set(chain.from_iterable(pairs_g2)))\n",
    "# random_pairs = [(2, 4), (34, 41), (55, 65), (71, 76)]\n",
    "# pairs_g2.update(random_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as la\n",
    "from scipy.fft import fft2\n",
    "\n",
    "g1_fft = np.concatenate([np.mean(np.abs(fft2(x)), axis=0, keepdims=True) for x in avg_g1], axis=0)\n",
    "g2_fft = np.concatenate([np.mean(np.abs(fft2(x)), axis=0, keepdims=True) for x in avg_g2], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_weights(x: np.ndarray, keepdims: bool=True):\n",
    "    return np.mean(np.abs(fft2(x)), axis=0, keepdims=keepdims)\n",
    "\n",
    "def get_top_freqs(x: np.ndarray, top_k:int) -> List[int]:\n",
    "\tfft_w = get_fft_weights(x, keepdims=False)\n",
    "\tif top_k is None:\n",
    "\t\treturn np.argsort(fft_w).tolist()\n",
    "\t\n",
    "\treturn np.argsort(fft_w, )[-top_k:].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got multiclass instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dcg_score\n\u001b[0;32m      4\u001b[0m y1, y2 \u001b[38;5;241m=\u001b[39m get_top_freqs(avg_g1[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m15\u001b[39m), get_top_freqs(avg_g1[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdcg_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bouab\\DEV\\Towards_Data_Science\\tds_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bouab\\DEV\\Towards_Data_Science\\tds_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1634\u001b[0m, in \u001b[0;36mdcg_score\u001b[1;34m(y_true, y_score, k, log_base, sample_weight, ignore_ties)\u001b[0m\n\u001b[0;32m   1632\u001b[0m y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1633\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m-> 1634\u001b[0m \u001b[43m_check_dcg_target_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39maverage(\n\u001b[0;32m   1636\u001b[0m     _dcg_sample_scores(\n\u001b[0;32m   1637\u001b[0m         y_true, y_score, k\u001b[38;5;241m=\u001b[39mk, log_base\u001b[38;5;241m=\u001b[39mlog_base, ignore_ties\u001b[38;5;241m=\u001b[39mignore_ties\n\u001b[0;32m   1638\u001b[0m     ),\n\u001b[0;32m   1639\u001b[0m     weights\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1640\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bouab\\DEV\\Towards_Data_Science\\tds_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1518\u001b[0m, in \u001b[0;36m_check_dcg_target_type\u001b[1;34m(y_true)\u001b[0m\n\u001b[0;32m   1512\u001b[0m supported_fmt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous-multioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1515\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass-multioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1516\u001b[0m )\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_fmt:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m formats are supported. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1520\u001b[0m             supported_fmt, y_type\n\u001b[0;32m   1521\u001b[0m         )\n\u001b[0;32m   1522\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got multiclass instead"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from sklearn.metrics import dcg_score\n",
    "\n",
    "y1, y2 = get_top_freqs(avg_g1[0], 15), get_top_freqs(avg_g1[1], 15)\n",
    "\n",
    "dcg_score(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_segment_by_fft(seg_index: int, segments: List[np.ndarray], num_freqs:Optional[int]):\n",
    "\tmax_ac_corr = -float('inf')\n",
    "\tbest_index = None\n",
    "\n",
    "\ttop_freqs = get_top_freqs(segments[seg_index], top_k=num_freqs)\n",
    "\n",
    "\tfor i in range(len(segments)):\n",
    "\t\tif i == seg_index: \n",
    "\t\t\tcontinue\n",
    "\t\t# compound segment\n",
    "\t\tother_seg = segments[i]\n",
    "\t\tseg_weights = get_fft_weights(segments[seg_index])\n",
    "\t\tother_seg_weights = get_fft_weights(other_seg)\n",
    "\t\tcorr = dcg_score(seg_weights, other_seg_weights)\n",
    "\n",
    "\t\tif corr > max_ac_corr:\n",
    "\t\t\tmax_ac_corr = corr \n",
    "\t\t\tbest_index = i\n",
    "\n",
    "\treturn best_index \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_avg_g1 = set()\n",
    "for i in range(len(avg_g1)):\n",
    "\tj= find_best_segment_by_fft(i, segments=avg_g1, num_freqs=None)\t\n",
    "\tpairs_avg_g1.add((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(avg_g1)):\n",
    "\tj = find_best_segment_by_fft(i, segments=avg_g1, num_freqs=20)\t\n",
    "\tif (i, j) in pairs_avg_g1 and (j, i) in pairs_avg_g1:\n",
    "\t\tcount += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1_2d = np.concatenate([np.mean(x, axis=0, keepdims=True) for x in avg_g1], axis=0)\n",
    "# g2_2d = np.concatenate([np.mean(x, axis=0, keepdims=True) for x in avg_g2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE \n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "x_reduced = tsne.fit_transform(g1_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_reduced[:, 0], x_reduced[:, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the scan into different brain sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# let's see how it goes\n",
    "npy_file_path = os.path.join(current_dir, 'ihb.npy')\n",
    "\n",
    "all_data = np.load(npy_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 10, 246)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      160\n",
       "460    160\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "num_nans = [np.isnan(all_data[i]).sum() for i in range(all_data.shape[0])]\n",
    "nan_values = pd.Series(num_nans)\n",
    "nan_values.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = [all_data[i] for i in range(all_data.shape[0]) if np.isnan(all_data[i]).sum() == 0]\n",
    "g2 = [all_data[i] for i in range(all_data.shape[0]) if np.isnan(all_data[i]).sum() == 460]\n",
    "\n",
    "g2_nan_distribution = np.zeros(shape=(len(g2), len(g2)))\n",
    "\n",
    "for i1, element1 in enumerate(g2):\n",
    "\tfor i2, element2 in enumerate(g2):\n",
    "\t\tg2_nan_distribution[i1][i2] = all(np.sum(np.isnan(element1), axis=1) == np.sum(np.isnan(element2), axis=1))\n",
    "\n",
    "g2 = [scan[:, :-46] for scan in g2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first step: group scans of the same scan but with different smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know for a fact that we have 160 samples belonging to 20 subjects each having\n",
    "\n",
    "# let's calculate the auto correlation of each time sequence in each of scan\n",
    "from typing import Union, List\n",
    "from scipy.signal import correlate2d, correlate\n",
    "\n",
    "def autocorrelation_stats(scan: np.ndarray, aggregate:bool=True) -> Union[List, float]:\t\n",
    "\tassert len(scan) == 10\n",
    "\tauto_correlations =  [float(correlate(scan[i:], scan[:-i])) for i in range(1, 6)]\n",
    "\tif aggregate:\n",
    "\t\treturn np.mean(auto_correlations)\n",
    "\treturn auto_correlations\n",
    "\n",
    "def build_ac_pairs(scans: List[np.ndarray]) -> set:\n",
    "\tauto_corrs = np.zeros(shape=(len(scans), len(scans)))\n",
    "\n",
    "\tfor i1, element1 in enumerate(scans):\n",
    "\t\tfor i2, element2 in enumerate(scans):\n",
    "\t\t\tauto_corrs[i1][i2] = correlate2d(element1, element2, \"valid\").item()\t\n",
    "\t\n",
    "\t# for each row, row[0] represents the closest index to scan[i] in terms of auto correlation\n",
    "\t# row[1] represents the same index\n",
    "\tpaired_scans_by_ac = np.argsort(auto_corrs, axis=-1)[:, -2:]\n",
    "\n",
    "\tpairs = set()\n",
    "\n",
    "\tfor i in range(len(scans)):\n",
    "\t\tassert paired_scans_by_ac[i, 1] == i, \"check the code\"\n",
    "\t\tclosest_scan_index = paired_scans_by_ac[i, 0]\n",
    "\t\tif paired_scans_by_ac[closest_scan_index, 0] == i and (closest_scan_index, i) not in pairs:\n",
    "\t\t\tpairs.add((i, closest_scan_index)) \n",
    "\n",
    "\treturn pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_pairs, g2_pairs = build_ac_pairs(g1), build_ac_pairs(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g1_pairs), len(g2_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step: grouping the different segments of the same scan sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_segment_rep(scans: List[np.ndarray], pairs_indices: set) -> List[np.ndarray]:\n",
    "\tavg_segments = []\n",
    "\tfor i1, i2 in pairs_indices:\n",
    "\t\ts1, s2  = scans[i1], scans[i2]\n",
    "\t\tif s1.shape != s2.shape:\n",
    "\t\t\traise ValueError(\"Make sure the code is correct. found pairs with different shapes\")\n",
    "\t\tavg_segments.append((s1 + s2) / 2)\n",
    "\treturn avg_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_g1, avg_g2 = unified_segment_rep(g1, g1_pairs), unified_segment_rep(g2, g2_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume that the different scan segments correspond to consecutive time stamps (a far-stretched assumption, but why not ?)\n",
    "# the whole idea here is that if we have two consecutive  segments s1 and s2 from an original sequence \"S\". Then we can see if the auto correlation between [s1, s2] and s1, s2 and [s2, s1], s1 and s2 \n",
    "# for each sequence to \"s1\" to find the best sequence \"s2\", we need \"n\" operations with a total of n^2: pretty much nothing when (n = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_next_segment(seg_index: int, segments: List[np.ndarray]):\n",
    "\tmax_ac_corr = -float('inf')\n",
    "\tbest_index = None\n",
    "\tbest_order = None\n",
    "\n",
    "\tfor i in range(len(segments)):\n",
    "\t\tif i == seg_index: \n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# # compound segment\n",
    "\t\tother_seg = segments[i]\n",
    "\t\t# # build the bigger sequence\n",
    "\t\tcompound_seg1 = np.concatenate([segments[seg_index], other_seg], axis=0)\n",
    "\t\tcompound_seg2 = np.concatenate([other_seg, segments[seg_index]], axis=0)\n",
    "\n",
    "\t\tseg_index_compound = np.concatenate([segments[seg_index], segments[seg_index]], axis=0)\n",
    "\t\tother_seg_compound = np.concatenate([other_seg, other_seg], axis=0)\n",
    "\n",
    "\t\tc1 = correlate2d(compound_seg1, seg_index_compound, \"valid\").item()\n",
    "\t\tc2 = correlate2d(compound_seg2, other_seg_compound, \"valid\").item()\n",
    "\n",
    "\t\t# c1 = correlate2d(compound_seg1, compound_seg1, \"valid\")\n",
    "\t\t# c2 = correlate2d(compound_seg2, compound_seg2, \"valid\")\n",
    "\t\tcorr = max(c1, c2)\n",
    "\n",
    "\t\tif corr > max_ac_corr:\n",
    "\t\t\tmax_ac_corr = corr \n",
    "\t\t\tbest_index = i\n",
    "\t\t\tbest_order = [seg_index, i] if c1 > c2 else [i, seg_index]\n",
    "\n",
    "\treturn best_index, best_order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1, s2, s3 = np.concatenate([avg_g1[0], avg_g1[1]], axis=0),np.concatenate([avg_g1[1], avg_g1[2]], axis=0),np.concatenate([avg_g1[0], avg_g1[2]], axis=0)\n",
    "# correlate2d(s1, s1, \"valid\").item(),correlate2d(s2, s2, \"valid\").item(),correlate2d(s3, s3, \"valid\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find the pairs somehow\n",
    "pairs_avg_g1 = set()\n",
    "for i in range(len(avg_g1)):\n",
    "\tj, _= find_best_next_segment(i, segments=avg_g1)\t\n",
    "\tpairs_avg_g1.add((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(avg_g1)):\n",
    "\tj, _ = find_best_next_segment(i, segments=avg_g1)\t\n",
    "\tif (i, j) in pairs_avg_g1 and (j, i) in pairs_avg_g1:\n",
    "\t\tcount += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.6043775 , 1.61858961, 1.78668952, 2.00848914, 3.94706962]),\n",
       " array([0.96732131, 0.97947739, 0.97260987, 0.97247485, 0.93905281,\n",
       "        0.97468263, 0.96306932, 0.95401796, 0.97552129, 0.97530294]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.abs(avg_g1[0]).mean(axis=0))[-5:], avg_g1[0].std(axis=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the scan into different brain sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import os\n",
    "import numpy as np\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# let's see how it goes\n",
    "npy_file_path = os.path.join(current_dir, 'ihb.npy')\n",
    "\n",
    "all_data = np.load(npy_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 10, 246)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      160\n",
       "460    160\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "num_nans = [np.isnan(all_data[i]).sum() for i in range(all_data.shape[0])]\n",
    "nan_values = pd.Series(num_nans)\n",
    "nan_values.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = [all_data[i] for i in range(all_data.shape[0]) if np.isnan(all_data[i]).sum() == 0]\n",
    "g2 = [all_data[i] for i in range(all_data.shape[0]) if np.isnan(all_data[i]).sum() == 460]\n",
    "\n",
    "g2_nan_distribution = np.zeros(shape=(len(g2), len(g2)))\n",
    "\n",
    "for i1, element1 in enumerate(g2):\n",
    "\tfor i2, element2 in enumerate(g2):\n",
    "\t\tg2_nan_distribution[i1][i2] = all(np.sum(np.isnan(element1), axis=1) == np.sum(np.isnan(element2), axis=1))\n",
    "\n",
    "g2 = [scan[:, :-46] for scan in g2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first step: group scans of the same scan but with different smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know for a fact that we have 160 samples belonging to 20 subjects each having\n",
    "\n",
    "# let's calculate the auto correlation of each time sequence in each of scan\n",
    "from typing import Union, List\n",
    "from scipy.signal import correlate2d, correlate\n",
    "\n",
    "def autocorrelation_stats(scan: np.ndarray, aggregate:bool=True) -> Union[List, float]:\t\n",
    "\tassert len(scan) == 10\n",
    "\tauto_correlations =  [float(correlate(scan[i:], scan[:-i])) for i in range(1, 6)]\n",
    "\tif aggregate:\n",
    "\t\treturn np.mean(auto_correlations)\n",
    "\treturn auto_correlations\n",
    "\n",
    "def build_ac_pairs(scans: List[np.ndarray]) -> set:\n",
    "\tauto_corrs = np.zeros(shape=(len(scans), len(scans)))\n",
    "\n",
    "\tfor i1, element1 in enumerate(scans):\n",
    "\t\tfor i2, element2 in enumerate(scans):\n",
    "\t\t\tauto_corrs[i1][i2] = correlate2d(element1, element2, \"valid\").item()\t\n",
    "\t\n",
    "\t# for each row, row[0] represents the closest index to scan[i] in terms of auto correlation\n",
    "\t# row[1] represents the same index\n",
    "\tpaired_scans_by_ac = np.argsort(auto_corrs, axis=-1)[:, -2:]\n",
    "\n",
    "\tpairs = set()\n",
    "\n",
    "\tfor i in range(len(scans)):\n",
    "\t\tassert paired_scans_by_ac[i, 1] == i, \"check the code\"\n",
    "\t\tclosest_scan_index = paired_scans_by_ac[i, 0]\n",
    "\t\tif paired_scans_by_ac[closest_scan_index, 0] == i and (closest_scan_index, i) not in pairs:\n",
    "\t\t\tpairs.add((i, closest_scan_index)) \n",
    "\n",
    "\treturn pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_pairs, g2_pairs = build_ac_pairs(g1), build_ac_pairs(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g1_pairs), len(g2_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_segment_rep(scans: List[np.ndarray], pairs_indices: set) -> List[np.ndarray]:\n",
    "\tavg_segments = []\n",
    "\tfor i1, i2 in pairs_indices:\n",
    "\t\ts1, s2  = scans[i1], scans[i2]\n",
    "\t\tif s1.shape != s2.shape:\n",
    "\t\t\traise ValueError(\"Make sure the code is correct. found pairs with different shapes\")\n",
    "\t\tavg_segments.append((s1 + s2) / 2)\n",
    "\treturn avg_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_g1, avg_g2 = unified_segment_rep(g1, g1_pairs), unified_segment_rep(g2, g2_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auto_corr_concatenation(seg1: np.ndarray, seg2: np.ndarray):\n",
    "\tassert seg1.shape == seg2.shape, \"both segments same shape\"\n",
    "\tc_seg1 = np.concatenate([seg1, seg2], axis=0)\n",
    "\tc_seg2 = np.concatenate([seg2, seg1], axis=0)\n",
    "\n",
    "\tassert c_seg1.shape[0] == 2 * seg1.shape[0] and c_seg1.shape[1] == seg1.shape[1], \"concatenation correct\"\n",
    "\tassert c_seg2.shape[0] == 2 * seg2.shape[0] and c_seg2.shape[1] == seg2.shape[1], \"concatenation correct\"\n",
    "\n",
    "\tc1 = np.mean([correlate2d(c_seg1[i:i + len(seg1), :], seg1, \"valid\").item() for i in range(len(seg1))])\n",
    "\tc2 = np.mean([correlate2d(c_seg2[i:i + len(seg1), :], seg2, \"valid\").item() for i in range(len(seg1))])\n",
    "\treturn max(c1, c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce 80 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see fi "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

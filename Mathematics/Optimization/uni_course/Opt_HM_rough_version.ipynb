{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a386bbbb",
   "metadata": {},
   "source": [
    "# Part 1. Solving the unconstrained optimization problem\n",
    "\n",
    "Consider again the empirical risk minimization:\n",
    "\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w\\|^2_2,\n",
    "\\end{equation}\n",
    "where $\\ell: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ is a loss function, $g : \\mathbb{R}^d \\times \\mathbb{R}^x \\to \\mathbb{R}$ is a model, $w$ are parameters of the model, $\\{x_i, y_i\\}_{i=1}^n$ is data of objects $x_i \\in \\mathbb{R}^x$ and labels $y_i \\in \\mathbb{R}$.\n",
    "\n",
    "We use the linear model $g(w, x) = w^T x$ and the logistic/sigmoid loss function: $\\ell(z,y) = \\ln (1 + \\exp(-yz))$ (__Important: $y$ must take values $-1$ or $1$__). \n",
    "\n",
    "As we already know, the resulting problem is called a logistic regression problem.\n",
    "\n",
    "This problem can be rewritten as follows:\n",
    "\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} f(w) := \\frac{1}{s} \\sum\\limits_{j=1}^s f_j(w) := \\frac{1}{s} \\sum\\limits_{j=1}^s \\left[\\frac{1}{b} \\sum\\limits_{i=1}^b l (g(w, x_{(j-1)b + i}), y_{(j-1)b + i}) + \\frac{\\lambda}{2} \\| w\\|^2_2\\right],\n",
    "\\end{equation}\n",
    "where $b$ is the batch size, $s$ is the number of batches, and $b s = n$ is the total sample size.\n",
    "\n",
    "The gradient of $f_j$:\n",
    "$$\n",
    "\\nabla f_j(w) = \\frac{1}{b} \\sum_{i=1}^b \\frac{-y_{(j-1)b + i} x_{(j-1)b + i}}{1 + \\exp(y_{(j-1)b + i} w^Tx_{(j-1)b + i})}.\n",
    "$$\n",
    "The Lipschitz constant of the gradient $\\nabla f_j$ can be estimated as $L_j = \\frac{1}{4b} \\sum\\limits_{i=1}^b \\| x_{(j-1)b + i} \\|^2_2$.\n",
    "\n",
    "Let us do some preparation work. The _mushrooms_ dataset is attached. Use the following code to generate a matrix $X$ and vector $y$, which will store the sample $\\{x_i, y_i\\}_{i=1}^n$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf961d3",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d1f7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "#the file must be in the same directory as notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41f4a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ef9c8",
   "metadata": {},
   "source": [
    "Let us change the vector $y$ so that $y_i$ takes values $-1$ and $1$. You can also do additional preprocessing of the data (with techniques from machine learning), but this is not additionally assessed in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9c6af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * y - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabb63a",
   "metadata": {},
   "source": [
    "Let us divide the data into two parts: train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8e9fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51caa066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6499, 112)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d427e",
   "metadata": {},
   "source": [
    "For $X_{train}$, $y_{train}$, estimate the constant $L$. \n",
    "Set $\\lambda$ such that $\\lambda \\approx L / 1000$.  \n",
    "Realize in the code the calculation of the value and gradient of $f$ ($X$, $y$, $\\lambda$ should be given as a parameter so that it is possible to change them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7860719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's seed the sources of randomness\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from typing import Tuple, List, Union, Sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec184b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function to verify the input (shapes and other conditions...)\n",
    "def p1_verify_input(X: np.ndarray, \n",
    "                    y: np.ndarray, \n",
    "                    w: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # first let's make sure the input is as expected: X is expected to have samples as rows\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"the training is expected to have at least 2 samples\")\n",
    "    \n",
    "    num_samples, dim = X.shape \n",
    "    # make sure 'y' matches the number of samples\n",
    "    y = np.expand_dims(y, axis=1) if y.ndim == 1 else y\n",
    "\n",
    "    if y.shape != (num_samples, 1):\n",
    "        raise ValueError((f\"The number of labels is expected to match the number of samples\"\n",
    "                          f\"\\nFound: {y.shape} Expected: {(num_samples, 1)}\"))\n",
    "\n",
    "    w = np.squeeze(w)    \n",
    "    # check 'w' as well:\n",
    "    w = np.expand_dims(w, axis=-1) if w.ndim == 1 else w\n",
    "\n",
    "    # make sure the dimensions match\n",
    "    if w.shape != (dim, 1):\n",
    "        raise ValueError((f\"The weight matrix 'w' is expected as a column vector with length {dim}\\n\"\n",
    "                          f\"Expected: {(dim, 1)}. Found: {w.shape}\"))\n",
    "    return X, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b903cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all these functions were used in the previous assignment and have been heavily tested.\n",
    "def p1_value_function(X: np.ndarray, \n",
    "                   y: np.ndarray,\n",
    "                   w: np.ndarray, \n",
    "                   lam: float) -> float:\n",
    "    \"\"\" This function calculates different values of the function gives it parameters:\n",
    "    Args:\n",
    "        X (np.ndarray): The training data\n",
    "        y (np.ndarray): The labels\n",
    "        w (np.ndarray): the weights\n",
    "        lam (float): lambda: the regularization hyper-parameter\n",
    "\n",
    "    Returns:\n",
    "        float: The value for the function with parameter 'w'\n",
    "    \"\"\"\n",
    "    # first step is to verify the input\n",
    "    X, y, w = p1_verify_input(X, y, w)\n",
    "    # now as our input has been verified we proceed by defining some useful intermediate variables\n",
    "    A = X * y\n",
    "    return np.mean(np.log(1 + np.exp((- A @ w).astype(np.float64)))) + (lam / 2) * np.linalg.norm(w) ** 2 \n",
    "\n",
    "def p1_gradient_function(X: np.ndarray, \n",
    "                      y: np.ndarray, \n",
    "                      w: np.ndarray, \n",
    "                      lam: float) -> np.ndarray:\n",
    "    # verify the input\n",
    "    X, y, w = p1_verify_input(X, y, w)\n",
    "\n",
    "    A = X * y\n",
    "    preds = A @ w \n",
    "\n",
    "    # then calculate the mean over axis=0 to get a row vector    \n",
    "    dl1 = np.mean(A / (1 + np.exp(preds)), axis=0)\n",
    "    assert dl1.shape == (X.shape[1], )\n",
    "    dl = - np.expand_dims(dl1, axis=-1) + lam * w\n",
    "    assert dl.shape == (X.shape[1], 1), \"The gradient is expected to be a column vector.\"\n",
    "    return dl\n",
    "\n",
    "def p1_hessian_function(X: np.ndarray, \n",
    "                   y: np.ndarray, \n",
    "                   w: np.ndarray, \n",
    "                   lam: float) -> np.ndarray:\n",
    "    # verify the input\n",
    "    X, y, w = p1_verify_input(X, y, w)\n",
    "    num_samples, dim = X.shape\n",
    "\n",
    "    A = X * y\n",
    "    preds = (A @ w).astype(np.float64) \n",
    "    # preds = preds.astype(np.float64)\n",
    "\n",
    "    # create the hessian matrix with zero values\n",
    "    hessian_matrix = np.zeros((dim, dim), dtype=np.float64)\n",
    "\n",
    "    exp_coeffs = (1 / (1 + np.exp(preds))) - (1 / ((1 + np.exp(preds)) ** 2))\n",
    "\n",
    "    assert exp_coeffs.shape == (num_samples, 1)\n",
    "\n",
    "    #iterate through samples\n",
    "    for i in range(num_samples):\n",
    "        # first step calculate the x_i * x_i ^ T\n",
    "        # extract x_i: row vector in code\n",
    "        x_i = X[i, :]\n",
    "        \n",
    "        assert x_i.shape == (dim,) , \"The row sample is expected to be 1 dimensional\"\n",
    "        # expand \n",
    "        x_i = np.expand_dims(x_i, axis=-1)\n",
    "\n",
    "        matrix_xi = x_i @ x_i.T\n",
    "        # add an assert to catch any errors with shapes\n",
    "        assert matrix_xi.shape == (dim, dim), \"Make sure the matrix x_i * x_i ^ T is computed correctly\"\n",
    "\n",
    "        hessian_xi = exp_coeffs[i][0] * matrix_xi\n",
    "        # time to add the coefficient associated with the matrix_xi\n",
    "        # hessian_xi = (exp1[i][0] / exp2[i][0]) * matrix_xi\n",
    "        hessian_matrix += hessian_xi\n",
    "\n",
    "    # make sure to divide by the number of samples \n",
    "    hessian_matrix = hessian_matrix / num_samples + lam * np.eye(dim)\n",
    "    # make sure the shape of the hessian matrix \n",
    "    hessian_matrix.shape == (dim , dim), \"Make sure the hessian matrix is of the correct shape\"\n",
    "    return hessian_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a726244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1_L_estimation(X: np.ndarray,\n",
    "                    y: np.ndarray,\n",
    "                    w: np.ndarray) -> float:\n",
    "    # return the estimation by the problem description\n",
    "    X, y, w = p1_verify_input(X, y, w) \n",
    "    num_samples, _ = X.shape\n",
    "    # first calculate L (without lambda)\n",
    "    L = np.linalg.norm(X) ** 2 / (4 * num_samples)\n",
    "    # set lambda\n",
    "    lam = L / 1000\n",
    "    return L + lam, lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373d531",
   "metadata": {},
   "source": [
    "## Problem 0. (5 points)__  Implement the ability to uniformly divide the training part of the dataset into batches of size $b$ ($b$ is a parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "835a71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(X: np.ndarray,\n",
    "                       batch_size: int, \n",
    "                       y: np.ndarray=None,\n",
    "                       even_split: bool = True\n",
    "                       ) -> Union[Tuple[Sequence[np.ndarray], Sequence[np.ndarray]], Sequence[np.ndarray]]:\n",
    "    \"\"\"A function to split the data into batches\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): _description_\n",
    "        batch_size (int): \n",
    "        y (np.ndarray, optional): labels. Defaults to None.\n",
    "        even_split (bool, optional): Whether to have all batches split evenly. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Union[Tuple[Sequence[np.ndarray], Sequence[np.ndarray]], Sequence[np.ndarray]]: the data batches, [Optional] the label batches \n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "\n",
    "    if even_split and len(X) % batch_size != 0:\n",
    "        raise ValueError((f\"Please pass a batch size that can split the data evenly or set 'even_split' to False.\\n\" \n",
    "                         f\"The number of samples: {len(data)}. The batch size: {batch_size}\"))\n",
    "    \n",
    "    if y is not None: \n",
    "        # make sure the number of samples is the same as that of the number of labels\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(f\"The number of samples should be the same as the number of labels.\")\n",
    "\n",
    "        y = np.expand_dims(y, axis=-1) if y.ndim == 1 else y        \n",
    "\n",
    "    if y is not None:\n",
    "        return [X[i: i + batch_size] for i in range(0, len(X), batch_size)], [y[i: i + batch_size] for i in range(0, len(y), batch_size)]\n",
    "\n",
    "    return [X[i: i + batch_size] for i in range(0, len(X), batch_size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377d566",
   "metadata": {},
   "source": [
    "## Problem 1. (50 points)__ This part of the assignment is related to __non-distributed stochastic__ methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e136f",
   "metadata": {},
   "source": [
    "### a). (5 points)__ Implement the SGD method:\n",
    "$$\n",
    "w^{k+1} = w^k - \\gamma_k \\nabla f_{j_k} (w^k),\n",
    "$$\n",
    "where the number $j_k$ is generated independently and uniformly from $\\{1, \\ldots, s \\}$.\n",
    "\n",
    "Just in case, we give here a variant of the function description for the gradient descent from the first assignment. You can use this format if you wish. Note that ``x_sol`` occurs in the code - this problem should be solved or criteria tied to ``x_sol`` should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65574e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_difference(x1: np.ndarray, x2: np.ndarray) -> float:\n",
    "    \"\"\"Computes the euclidean norm of the difference vector \n",
    "\n",
    "    Args:\n",
    "        x1 (np.ndarray): first vector\n",
    "        x2 (np.ndarray): second vector\n",
    "\n",
    "    Returns:\n",
    "        float: returns the norm of the difference\n",
    "    \"\"\"\n",
    "    return norm(x1 - x2, ord=2)\n",
    "\n",
    "def f_difference(f1: float, f2: float) -> float:\n",
    "    \"\"\"returns the absolute difference between 2 values \"\"\"\n",
    "    # the expression f_x_k - f_sol is equivalent since the problem is minimization,\n",
    "    # but the 'abs' function was used to make the method general and not only specific to the given problem\n",
    "    return abs(f1 - f2)      \n",
    "\n",
    "# let's define the modes in terms of strings\n",
    "x_diff = 'x_k+1 - x_k'\n",
    "f_diff = 'f(x_k+1) - f(x_k)'\n",
    "normalized_criterion = 'df_xk / df_x0'\n",
    "x_opt_diff = 'x* - x_k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6be5209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoachastic_gradient_descent(\n",
    "                    data: np.ndarray, \n",
    "                    labels: np.ndarray,\n",
    "                    data_batches: Sequence[np.ndarray],\n",
    "                    label_batches: Sequence[np.ndarray],\n",
    "                    function: callable,\n",
    "                    grad_function: callable, \n",
    "                    x_0: np.ndarray,                      \n",
    "                    x_sol: np.ndarray=None,\n",
    "                    K: int = 10 ** 3,\n",
    "                    eps: float = 10 ** -5, \n",
    "                    mode: str = normalized_criterion,\n",
    "                    gamma_k: callable = None,                      \n",
    "                    return_history: bool = False\n",
    "                            ) -> Union[List[np.ndarray], np.ndarray]:\n",
    "    \n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if isinstance(mode , str) and mode not in [f_diff, x_diff, normalized_criterion, x_opt_diff]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion, x_opt_diff]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    if mode == x_opt_diff and x_sol is None: \n",
    "        raise ValueError(f\"using mode = {x_opt_diff} requires passing the solution to the problem\")\n",
    "    \n",
    "    x_current, x_previous = x_0, x_0\n",
    "    x_history = [x_current]\n",
    "    criterion_history = []\n",
    "\n",
    "    for k in tqdm(range(K)):\n",
    "        # generate the data index\n",
    "        data_index = random.randint(0, len(data_batches) - 1)\n",
    "        # save the k-th point\n",
    "        x_previous = x_current\n",
    "\n",
    "        gamma = gamma_k(k)\n",
    "        \n",
    "        # create the arguments to pass to the gradient (only using the randomly selected batch)\n",
    "        grad_args = {\"X\": data_batches[data_index], \"y\": label_batches[data_index], 'w': x_current}\n",
    "        \n",
    "        # create the arguments to pass to the function: the entire data\n",
    "        func_args = {\"X\": data, 'y': labels, 'w': x_current}\n",
    "        \n",
    "        # compute the (k+1)-th point    \n",
    "        x_current = x_current - gamma * grad_function(**grad_args)  \n",
    "\n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(func_args), function(func_args))\n",
    "        \n",
    "        elif mode == x_diff:\n",
    "            diff = x_difference(x_current, x_previous)\n",
    "        \n",
    "        elif mode == normalized_criterion:\n",
    "            diff = norm(grad_function(**func_args), ord=2) / norm(grad_function(**{\"X\": data, 'y': labels, 'w': x_0}), ord=2)\n",
    "        \n",
    "        elif mode == x_opt_diff: \n",
    "            diff = norm(x_current - x_sol, ord=2)\n",
    "        else: \n",
    "            # the last case is where the criterion is passed as an argument\n",
    "            diff = mode(x_current)\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "        \n",
    "        if diff <= eps: \n",
    "            break\n",
    "\n",
    "    assert len(x_history) == k + 2, f\"expected {k + 2} points. Found: {len(x_history)}\"\n",
    "\n",
    "    # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return (x_history, criterion_history) if return_history else x_history[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d708445",
   "metadata": {},
   "source": [
    "### b). (7 points)__ \n",
    "\n",
    "Solve the optimization problem on the training sample using the implemented method. \n",
    "\n",
    "Take $b = 10$, and the step is $\\gamma_k \\equiv \\frac{1}{\\tilde L}$. \n",
    "\n",
    "From the point of view of the theory, what should be taken as $\\tilde L$? \n",
    "\n",
    "Draw the convergence plot: the value of the convergence criterion ($\\frac{\\| \\nabla f(w^k)\\|}{\\| \\nabla f(w^0)\\|}$) from the iteration number. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d7d51",
   "metadata": {},
   "source": [
    "We know the following:\n",
    "\n",
    "1. The linear convergence of SGD is theoretically guaranteed under the assumption that $f$ and each of $f_i$ are $\\mu$ strongly convex.\n",
    "2. If $f$ is $\\mu_1$ strongly convex and $\\mu_2 < \\mu_1$ then $f$ is also $\\mu_2$ strongly convex \n",
    "3. We are setting $\\mu = \\frac{L}{1000}$, thus larger $L$ means, larger $\\mu$. We will choose $\\tilde{L}$ that minimizes $\\mu, \\mu_1, \\mu_2 ... \\mu_n$:  \n",
    "\n",
    "Using these 2 facts, we should choose $\\tilde{L}$ as \n",
    "\n",
    "$$ \n",
    "\\tilde{L}  = \\min(L, L_1, L_2, ... L_s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "595a463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# create the set up: x_0 and seed\n",
    "def set_up(seed: int = 69) -> np.ndarray:\n",
    "        # changing the seed mainly changes the starting point\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    w_0 = np.random.randn(X_train.shape[1], 1)\n",
    "    return w_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d715107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's split the data into batches to define the inner functions\n",
    "X_BATCHES, Y_BATCHES = split_into_batches(X=X_train, y=y_train, batch_size=10, even_split=False)\n",
    "\n",
    "def set_L_Reg(data, labels, data_batches, label_batches) -> Tuple[float, float]:\n",
    "    L_tiled = float('inf')\n",
    "    Reg_tiled = float('inf')\n",
    "\n",
    "    for x, y in zip(data_batches, label_batches):\n",
    "        # compute L_i\n",
    "        l, r = p1_L_estimation(X=x, y=y, w=np.random.rand(x.shape[1], 1))\n",
    "        L_tiled, Reg_tiled = min([(l, r), (L_tiled, Reg_tiled)], key=lambda x: x[0])\n",
    "\n",
    "    L, REG = p1_L_estimation(X=data, y=labels, w=np.random.rand(X_train.shape[1], 1))\n",
    "\n",
    "    L_tiled, Reg_tiled = min([(L, REG), (L_tiled, Reg_tiled)], key=lambda x: x[0])\n",
    "\n",
    "    return L_tiled, Reg_tiled\n",
    "\n",
    "L_TILED, REG_TILED = set_L_Reg(X_train, y_train, X_BATCHES, Y_BATCHES)\n",
    "\n",
    "from functools import partial\n",
    "# a callable object with the main loss, gradient and hessian\n",
    "p1_stochastic_value = partial(p1_value_function, lam=REG_TILED)\n",
    "p1_stochastic_grad = partial(p1_gradient_function, lam=REG_TILED)\n",
    "p1_stochastic_h = partial(p1_hessian_function, lam=REG_TILED)\n",
    "\n",
    "# the estimation of 'L'\n",
    "assert abs(L_TILED / REG_TILED - 1000) <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "113877da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation( data: np.ndarray, \n",
    "                labels: np.ndarray,\n",
    "                data_batches: Sequence[np.ndarray],\n",
    "                label_batches: Sequence[np.ndarray],\n",
    "                optimization_algorithm: callable,\n",
    "                function: callable, \n",
    "                grad_function: callable, \n",
    "                gamma_k: callable=None,\n",
    "                momentum_k: callable=None,  \n",
    "                K = 10**3, \n",
    "                eps = 10**-8, \n",
    "                mode = normalized_criterion, \n",
    "                seed: int = 69, \n",
    "                x_sol: np.ndarray=None\n",
    "               ):\n",
    "\n",
    "    x_0 = set_up(seed=seed)\n",
    "    \n",
    "    # get the points from the gradient descent\n",
    "    if momentum_k is not None:\n",
    "        x_points, criterions = optimization_algorithm(\n",
    "                                    data=data,\n",
    "                                    labels=labels, \n",
    "                                    data_batches=data_batches, \n",
    "                                    label_batches=label_batches,\n",
    "                                    function=function, \n",
    "                                    grad_function=grad_function,  \n",
    "                                    x_0=x_0,\n",
    "                                    gamma_k=gamma_k,\n",
    "                                    momentum_k=momentum_k,\n",
    "                                    K = K,\n",
    "                                    eps=eps,\n",
    "                                    mode=mode, \n",
    "                                    return_history=True, \n",
    "                                    x_sol=x_sol\n",
    "                                    )\n",
    "        return x_points, criterions\n",
    "\n",
    "    x_points, criterions = optimization_algorithm(\n",
    "                                data=data,\n",
    "                                labels=labels,\n",
    "                                data_batches=data_batches,\n",
    "                                label_batches=label_batches,\n",
    "                                function=function, \n",
    "                                grad_function=grad_function,  \n",
    "                                x_0=x_0,\n",
    "                                gamma_k=gamma_k,\n",
    "                                K = K,\n",
    "                                eps=eps,\n",
    "                                mode=mode, \n",
    "                                return_history=True,\n",
    "                                x_sol=x_sol \n",
    "                                )\n",
    "\n",
    "    return x_points, criterions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70956ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "def plot_iterations(criterions: List[float],\n",
    "                    start_index: int = 0, \n",
    "                    end_index: int = -1,\n",
    "                    plot_label: str = None,\n",
    "                    x_label: str = None,\n",
    "                    y_label: str = None,\n",
    "                    show:bool = True,\n",
    "                    ):\n",
    "    \n",
    "    end_index = (end_index + len(criterions)) % len(criterions)\n",
    "\n",
    "    if plot_label is None:\n",
    "        plt.plot(list(range(start_index, end_index)), criterions[start_index:end_index])\n",
    "    else:\n",
    "        plt.plot(list(range(start_index, end_index)), criterions[start_index:end_index], label=str(plot_label))\n",
    "    \n",
    "\n",
    "    plt.xlabel('iteration' if x_label is None else x_label)\n",
    "    plt.ylabel('criterion (log_{10} scale)' if x_label is None else y_label)\n",
    "    \n",
    "    if show:\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d215b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant_gamma_k = lambda _ : 1 / L_TILED\n",
    "\n",
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# # run the simulation \n",
    "# x_points, criterions = simulation(data=X_train, \n",
    "#                                   labels=y_train,\n",
    "#                                   data_batches=X_BATCHES,\n",
    "#                                   label_batches=Y_BATCHES, \n",
    "#                                 optimization_algorithm=stoachastic_gradient_descent, \n",
    "#                                 function=p1_stochastic_value,\n",
    "#                                 grad_function=p1_stochastic_grad,\n",
    "#                                 gamma_k=constant_gamma_k, \n",
    "#                                 mode=normalized_criterion)\n",
    "\n",
    "# criterions = [np.log10(c) for c in criterions]\n",
    "\n",
    "# plot_iterations(criterions=criterions, \n",
    "#                 x_label='iteration', \n",
    "#                 y_label=f'{normalized_criterion}', \n",
    "#                 show=False)\n",
    "\n",
    "# # plt.legend()\n",
    "# plt.title(\"Stochastic gradient Descent\")\n",
    "# plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402789bc",
   "metadata": {},
   "source": [
    "CONCLUSION: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7b6d6",
   "metadata": {},
   "source": [
    "### c). (7 points)__ \n",
    "Vary the batch size: $b = 1, 10, 100, 1000$, \n",
    "\n",
    "take the step size equal to $\\gamma_k \\equiv \\frac{1}{\\tilde L}$ (note that $\\tilde L$ need to be recalculated for each $b$). \n",
    "\n",
    "Draw the convergence plot: the value of the convergence criterion from the iteration number for each $b$. \n",
    "\n",
    "Does this plot reflect a fair comparison? Why? \n",
    "\n",
    "Figure out how to compare the results to each other more honestly (running time is a good option, but you can't use it here) \n",
    "\n",
    "and draw a new comparison plot. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "faac2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's vary the batch size:\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# for b in [1, 10, 100, 1000]:\n",
    "#     # split the data\n",
    "#     data_batches, label_batches = split_into_batches(X=X_train, batch_size=b, y=y_train, even_split=False)\n",
    "#     # get the value 'L'\n",
    "#     l_tilde, reg_tilde = set_L_Reg(data=X_train, labels=y_train, data_batches=data_batches, label_batches=label_batches)\n",
    "    \n",
    "#     constant_gamma_k = lambda _ : 1 / l_tilde\n",
    "\n",
    "#     # redefine the funtions\n",
    "    \n",
    "#     sto_value = partial(p1_value_function, lam=reg_tilde)\n",
    "#     sto_grad = partial(p1_gradient_function, lam=reg_tilde)\n",
    "\n",
    "#     # run the simulation \n",
    "#     x_points, criterions = simulation(data=X_train, \n",
    "#                                     labels=y_train, \n",
    "#                                     data_batches=data_batches,\n",
    "#                                     label_batches=label_batches, \n",
    "#                                     optimization_algorithm=stoachastic_gradient_descent, \n",
    "#                                     function=sto_value,\n",
    "#                                     grad_function=sto_grad,\n",
    "#                                     gamma_k=constant_gamma_k, \n",
    "#                                     mode=normalized_criterion)\n",
    "\n",
    "#     criterions = [np.log10(c) for c in criterions]\n",
    "#     plot_iterations(criterions=criterions, \n",
    "#                     x_label='iteration', \n",
    "#                     y_label=f'{normalized_criterion}',\n",
    "#                     plot_label=f'batch_size: {b}', \n",
    "#                     show=False)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.title(\"Stochastic gradient Descent with different batch sizes\")\n",
    "# plt.show()      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ee3e5",
   "metadata": {},
   "source": [
    "### d). (6 points)__ Let us fix $b = 10$ and try to change the strategy of choosing the step:\n",
    "\n",
    "1) $\\gamma_k \\equiv \\frac{1}{\\tilde L}$ as we did before,\n",
    "\n",
    "2) $\\gamma_k \\equiv \\text{const}$, selecting $\\text{const}$ to get the \"best\" convergence (explain what you mean by \"best\"),\n",
    "\n",
    "3) $\\gamma_k = \\frac{1}{\\sqrt{k + 1}}$, \n",
    "\n",
    "4) $\\gamma_k = \\frac{1}{k + 1}$.\n",
    "\n",
    "Draw the convergence plot: the value of the convergence criterion from the iteration number. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a096334",
   "metadata": {},
   "source": [
    "#### Choosing the 'best' step size.\n",
    "\n",
    "According to the convergence analysis of SGD, conducted in the 10th-lecture, SGD converges linearly (to the neighborhood of the solution) for any fixed step size: \n",
    "\n",
    "$$ \n",
    "R ^2_{k + 1} \\leq (1 - \\mu \\cdot \\gamma)^k \\cdot R_0 ^ 2 + \\frac{2 \\gamma}{\\mu} \\cdot G ^ 2\n",
    "$$\n",
    "\n",
    "where $G^2 = \\frac{1}{n} \\sum_{i=1}^n{\\|f_i(x^{*}) \\| ^ 2}$ and $R^2_{k} = \\mathbb{E}[ \\| x^{k} - x ^{*} \\| ^ 2]$\n",
    "\n",
    "We can extend the inequality above for more flexibility: \n",
    "\n",
    "\\begin{align}\n",
    "R ^2_{k + 1} &\\leq (1 - \\mu \\cdot \\gamma)^k \\cdot R_0 ^ 2 + \\frac{2 \\gamma}{\\mu} \\cdot G ^ 2 \\\\ \n",
    "R ^2_{k + 1} &\\leq \\max(R_0 ^ 2, G ^ 2) \\cdot ((1 - \\mu \\cdot \\gamma)^k  + \\frac{2 \\gamma}{\\mu}) \\\\ \n",
    "\\end{align}\n",
    "\n",
    "Let's denote $h(\\gamma) = (1 - \\mu \\cdot \\gamma)^k  + \\frac{2 \\gamma}{\\mu}$ and \n",
    "$\\frac{dh}{d\\gamma} = \\frac{2}{\\mu} - \\mu \\cdot (1 - \\mu \\cdot \\gamma) ^ {k - 1}$. \n",
    "\n",
    "Since we know that $\\gamma < \\frac{1}{\\mu}$ and $\\mu < 1$, then we can see that $\\frac{dh}{d\\gamma} > 0$ since ($\\mu \\cdot (1 - \\mu \\gamma)^k < 1$) and Thus, $h(\\gamma)$ is minimal for $\\gamma = 0$ (in the interval $[0, \\frac{1}{\\mu}]$). However, we cannot use a step size of $0$.\n",
    "\n",
    "Thus, we resort to a practical tradeoff. for $ \\frac{1}{\\mu} \\geq \\gamma > 0$ SGD will converge in linear time to a neighborhood of the solution. The larger, $\\gamma$ is, the faster (Theoretically) the algorithm is guranteed to converge. However, The larger $\\gamma$ is, the larger the final neighborhood will be.\n",
    "\n",
    "Assuming we would like $$h_{K}(\\gamma) \\leq \\epsilon$$ for given $K$ steps and error $\\epsilon$, Then we can simply\n",
    "find the smallest $\\gamma$ satisfying such condition: \n",
    "\n",
    "\\begin{align}\n",
    "(1 - \\mu \\cdot \\gamma) ^ K &\\leq \\epsilon \\\\\n",
    "x ^ K &\\leq \\epsilon  && \\text{denote $1 - \\mu \\cdot \\gamma$ by $x$} \\\\\n",
    "e^ {log(x) \\cdot K} \\leq \\epsilon \\\\\n",
    "log(x) \\cdot K &\\leq \\log(\\epsilon) \\\\\n",
    "x &\\leq e ^ {\\frac{\\log(\\epsilon)}{K}} \\\\\n",
    "\\frac{1 - e ^ {\\frac{\\log(\\epsilon)}{K}}}{\\mu} &\\leq \\gamma\n",
    "\\end{align}\n",
    "\n",
    "Thus for given maximum number of steps $K$ and error $\\epsilon$, we can define the 'best' $\\gamma$ as: \n",
    "$$ \\gamma = \\frac{1 - e ^ {\\frac{\\log(\\epsilon)}{K}}}{\\mu} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f4df6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0438537146368508"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's define the different gammas\n",
    "def get_best_gamma(epsilon, K, mu):\n",
    "    return (1 - np.exp(np.log(epsilon) / K)) / mu\n",
    "\n",
    "EPSILON = 10 ** -10\n",
    "K = 10 ** 5\n",
    "BEST_GAMMA_SGD = get_best_gamma(EPSILON, K, REG_TILED)\n",
    "BEST_GAMMA_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26c73628",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BATCHES, Y_BATCHES = split_into_batches(X=X_train, y=y_train, batch_size=10, even_split=False)\n",
    "\n",
    "L_TILED, REG_TILED = set_L_Reg(X_train, y_train, X_BATCHES, Y_BATCHES)\n",
    "\n",
    "from functools import partial\n",
    "# a callable object with the main loss, gradient and hessian\n",
    "p1_stochastic_value = partial(p1_value_function, lam=REG_TILED)\n",
    "p1_stochastic_grad = partial(p1_gradient_function, lam=REG_TILED)\n",
    "p1_stochastic_h = partial(p1_hessian_function, lam=REG_TILED)\n",
    "\n",
    "# the estimation of 'L'\n",
    "assert abs(L_TILED / REG_TILED - 1000) <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "388ae099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 12))\n",
    "# gamma_inverse_k = lambda  k : 1 / (k + 1)\n",
    "# gamma_inverse_square_k = lambda  k :1 / np.sqrt(k + 1)\n",
    "# gamma_best_SGD = lambda _ : BEST_GAMMA_SGD\n",
    "# gamma_constant_L_TILED  = lambda _ : 1 / L_TILED\n",
    "\n",
    "# gamma_labels = ['1 / (k + 1)', '1 / sqrt(k + 1)', \"'best' gamma\", '1 / L']\n",
    "\n",
    "# for gamma_function_obj, gamma_label in zip([gamma_inverse_k, gamma_inverse_square_k, gamma_best_SGD, gamma_constant_L_TILED][-2:], gamma_labels[-2:]):\n",
    "#     x_points, criterions = simulation(data=X_train, \n",
    "#                                     labels=y_train,\n",
    "#                                     data_batches=X_BATCHES,\n",
    "#                                     label_batches=Y_BATCHES, \n",
    "#                                     optimization_algorithm=stoachastic_gradient_descent, \n",
    "#                                     function=p1_stochastic_value,\n",
    "#                                     grad_function=p1_stochastic_grad,\n",
    "#                                     gamma_k=gamma_function_obj, \n",
    "#                                     mode=normalized_criterion,\n",
    "#                                     K=10 ** 3)\n",
    "\n",
    "#     criterions = [np.log10(c) for c in criterions]\n",
    "#     plot_iterations(criterions=criterions, \n",
    "#                     x_label='iteration', \n",
    "#                     y_label=f'{normalized_criterion}',\n",
    "#                     plot_label=f'gamma: {gamma_label}', \n",
    "#                     show=False)    \n",
    "\n",
    "# plt.legend()\n",
    "# plt.title(\"Stochastic Gradient Descent with different step sizes\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56f586",
   "metadata": {},
   "source": [
    "### e). (5 points)__ \n",
    "\n",
    "In the previous hometask, we used the obtained solution of the optimization problem to predict answers on a test sample. \n",
    "\n",
    "Recall the essence: the original regression problem is a machine learning problem and using a linear model $g$ we can predict the values of labels $y$. \n",
    "\n",
    "Suppose we have a sample $x_i$, the model response for this sample is $g(w^*, x^i)$. \n",
    "\n",
    "Then the predictive rule can be formulated in the following rather natural way:\n",
    "$$\n",
    "y_i = \n",
    "\\begin{cases}\n",
    "1, & g(w^*, x^i) \\geq 0,\n",
    "\\\\\n",
    "-1, & g(w^*, x^i) < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "By making predictions on the test sample $X_{test}$, we can compare the result with the real labels $y_{test}$. The number of correctly guessed labels is the accuracy of the model.\n",
    "\n",
    "Look at the accuracy of the model trained with SGD. \n",
    "\n",
    "Repeat point c)-d), but now plot the accuracy dependence, not the convergence criterion. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10b8474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "    # make sure 'w' is a column vector\n",
    "    if w.ndim > 2: \n",
    "        raise ValueError(f\"Make sure that 'w' represents weights !!. Found: {w.shape}\")\n",
    "    \n",
    "    w = np.expand_dims(w, axis=-1) if w.ndim == 1 else w\n",
    "    \n",
    "    res = ((X @ w) >= 0).squeeze()\n",
    "\n",
    "    return 2 * res - 1\n",
    "\n",
    "def calculate_accuracy(predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "    ps = predictions.squeeze()\n",
    "    ls = labels.squeeze()\n",
    "    if ps.shape != ls.shape:\n",
    "        raise ValueError(f\"The predictions and the labels must have the same shape. Found: predictions: {ps.shape}. Labels: {ls.shape}\")\n",
    "    \n",
    "    return np.mean(ps == ls).item()\n",
    "\n",
    "def bar_plot(x: Sequence, \n",
    "             y: Sequence, \n",
    "             x_label: str, \n",
    "             y_label: str, \n",
    "             title: str,\n",
    "             fig_size: Tuple[int, int], \n",
    "             ax = None, \n",
    "             show:bool =True,\n",
    "             xticks=None) -> None:\n",
    "    plt.figure(figsize = fig_size)\n",
    "\n",
    "    plt.bar(x, y, color ='maroon', width = 0.4)\n",
    "\n",
    "    if xticks is not None:\n",
    "        plt.xticks(ticks=xticks, rotation=90)\n",
    "    else:\n",
    "        plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    if show: \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37693e73",
   "metadata": {},
   "source": [
    "### Point c: with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68ba8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = []\n",
    "# batch_sizes = [1, 10, 100, 1000]\n",
    "\n",
    "# for b in batch_sizes:\n",
    "#     # split the data\n",
    "#     data_batches, label_batches = split_into_batches(X=X_train, batch_size=b, y=y_train, even_split=False)\n",
    "#     # get the value 'L'\n",
    "#     l_tilde, reg_tilde = set_L_Reg(data=X_train, labels=y_train, data_batches=data_batches, label_batches=label_batches)\n",
    "    \n",
    "#     constant_gamma_k = lambda _ : 1 / l_tilde\n",
    "\n",
    "#     # redefine the funtions\n",
    "#     sto_value = partial(p1_value_function, lam=reg_tilde)\n",
    "#     sto_grad = partial(p1_gradient_function, lam=reg_tilde)\n",
    "\n",
    "#     # run the simulation \n",
    "#     x_points, criterions = simulation(data=X_train, \n",
    "#                                     labels=y_train, \n",
    "#                                     data_batches=data_batches,\n",
    "#                                     label_batches=label_batches, \n",
    "#                                     optimization_algorithm=stoachastic_gradient_descent, \n",
    "#                                     function=sto_value,\n",
    "#                                     grad_function=sto_grad,\n",
    "#                                     gamma_k=constant_gamma_k, \n",
    "#                                     mode=normalized_criterion)\n",
    "#     weight = x_points[-1]\n",
    "#     # predict\n",
    "#     predictions = predict(w=weight, X=X_test)\n",
    "#     acc = calculate_accuracy(predictions=predictions, labels=y_test)\n",
    "#     accuracies.append(acc)\n",
    "# print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "46902459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar_plot(x=[str(b) for b in batch_sizes], y=[acc - 0.9 for acc in accuracies]  , \n",
    "#          x_label='batch size', \n",
    "#          y_label='test accuracy', \n",
    "#          title='accuracy (- 0.9) vs batch size SGD', \n",
    "#          fig_size=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c8281",
   "metadata": {},
   "source": [
    "### Point d: with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97391c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma_inverse_k = lambda  k : 1 / (k + 1)\n",
    "# gamma_inverse_square_k = lambda  k :1 / np.sqrt(k + 1)\n",
    "# gamma_best_SGD = lambda _ : BEST_GAMMA_SGD\n",
    "# gamma_constant_L_TILED  = lambda _ : 1 / L_TILED\n",
    "\n",
    "# gamma_labels = ['1 / (k + 1)', '1 / sqrt(k + 1)', \"'best' gamma\", '1 / L']\n",
    "\n",
    "# accuracies = []\n",
    "\n",
    "# for gamma_function_obj, gamma_label in zip([gamma_inverse_k, gamma_inverse_square_k, gamma_best_SGD, gamma_constant_L_TILED], gamma_labels):\n",
    "#     x_points, criterions = simulation(data=X_train, \n",
    "#                                     labels=y_train,\n",
    "#                                     data_batches=X_BATCHES,\n",
    "#                                     label_batches=Y_BATCHES, \n",
    "#                                     optimization_algorithm=stoachastic_gradient_descent, \n",
    "#                                     function=p1_stochastic_value,\n",
    "#                                     grad_function=p1_stochastic_grad,\n",
    "#                                     gamma_k=gamma_function_obj, \n",
    "#                                     mode=normalized_criterion,\n",
    "#                                     K=10 ** 3)\n",
    "\n",
    "#     weight = x_points[-1]\n",
    "#     # predict\n",
    "#     predictions = predict(w=weight, X=X_test)\n",
    "#     acc = calculate_accuracy(predictions=predictions, labels=y_test)\n",
    "#     accuracies.append(acc)\n",
    "    \n",
    "# print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a5659576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar_plot(x=gamma_labels, y=[acc - 0.8 for acc in accuracies]  , \n",
    "#          x_label='gamma', \n",
    "#          y_label='test accuracy', \n",
    "#          title='SGD: test accuracy (- 0.9) vs gamma', \n",
    "#          fig_size=(8, 8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9184b",
   "metadata": {},
   "source": [
    "### f (30 points)__ \n",
    "\n",
    "Implement the \n",
    "\n",
    "SAGA (Section A.5 of [paper](https://arxiv.org/pdf/1905.11261.pdf)), \n",
    "\n",
    "SVRG (Section А.9 from [работы](https://arxiv.org/pdf/1905.11261.pdf)) and \n",
    "\n",
    "SARAH ([работа](https://arxiv.org/pdf/1703.00102.pdf)) methods for our problem. \n",
    "\n",
    "Solve the optimization problem on the training sample using the new implemented methods. \n",
    "\n",
    "Take $b = 10$, and hyperparameters of methods according to the theory (see corresponding papers). \n",
    "\n",
    "Write these parameters. \n",
    "\n",
    "Is the iteration number a fair for comparison of these methods? \n",
    "\n",
    "Figure out how to compare the results more honestly (running time is a good option, but you can't use it here). \n",
    "\n",
    "Draw the comparison plots of SGD (with step $\\gamma_k \\equiv \\frac{1}{\\tilde L}$), SAGA, SVRG and SARAH: \n",
    "\n",
    "1) value of convergence criterion versus your criterion, \n",
    "\n",
    "2) accuracy of predictions versus your criterion. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdc14a",
   "metadata": {},
   "source": [
    "#### SAGA\n",
    "\n",
    "SAGA (and the rest of the algorithms in this problem) were designed to optimize functions $F = f + h$, where $F = \\frac{1}{n} \\cdot \\sum_{i = 1}^n f_i$ and $h$ is regularization function. \n",
    "\n",
    "The algorithm makes use of the proxy operator defined as:\n",
    "\n",
    "$$\n",
    "Prox_{\\lambda}^{h} ~ = ~ \\argmin_{x \\in \\mathbb{R}^d} \\{ h(x) + \\frac{1}{2 \\lambda} \\| x - y\\| ^ 2 \\}\n",
    "$$\n",
    "\n",
    "However, in our problem we use $h(x)$ as a constant function $h(x) = 0 ~ \\forall x \\in \\mathbb{R}^d$. \n",
    "\n",
    "Thus the proxy operator in this case will be the identity function, and won't be implemented in a general form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9c9fc",
   "metadata": {},
   "source": [
    "The SAGA algorithm assumes that each function is $L$ smooth and $\\mu$ strongly convex. we have already chosen ,\n",
    "$$ \n",
    "\\tilde{L}  = max(L, L_1, L_2, ... L_s)\n",
    "$$\n",
    "\n",
    "and for each $f_i$, we have $\\mu = \\lambda$.\n",
    "\n",
    "The step size is chosen as: \n",
    "\n",
    "$$\\gamma = \\frac{1}{2(\\mu \\cdot n + L)}$$\n",
    "\n",
    "$n$ is the number of inner functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da09e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prepare the data for the SAGA algorithm.\n",
    "# first let's split the data into batches to define the inner functions\n",
    "X_10_BATCHES, Y_10_BATCHES = split_into_batches(X=X_train, y=y_train, batch_size=10, even_split=False)\n",
    "\n",
    "L_TILED, REG_TILED = set_L_Reg(X_train, y_train, X_10_BATCHES, Y_10_BATCHES)\n",
    "\n",
    "# a callable object with the main loss, gradient and hessian\n",
    "p1_stochastic_value = partial(p1_value_function, lam=REG_TILED)\n",
    "p1_stochastic_grad = partial(p1_gradient_function, lam=REG_TILED)\n",
    "p1_stochastic_h = partial(p1_hessian_function, lam=REG_TILED)\n",
    "\n",
    "# the estimation of 'L'\n",
    "assert abs(L_TILED / REG_TILED - 1000) <= 10\n",
    "\n",
    "SAGE_CONSTANT_GAMME = lambda _: 1 / (2 * len(X_10_BATCHES) + L_TILED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ac1b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saga(data: np.ndarray, \n",
    "        labels: np.ndarray,\n",
    "        data_batches: Sequence[np.ndarray],\n",
    "        label_batches: Sequence[np.ndarray],\n",
    "        function: callable,\n",
    "        grad_function: callable, \n",
    "        x_0: np.ndarray,                      \n",
    "        x_sol: np.ndarray=None,\n",
    "        K: int = 10 ** 3,\n",
    "        eps: float = 10 ** -5, \n",
    "        mode: str = normalized_criterion,\n",
    "        gamma_k: callable = None,                      \n",
    "        return_history: bool = False\n",
    "        ) -> Union[List[np.ndarray], np.ndarray]:\n",
    "    \n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if isinstance(mode , str) and mode not in [f_diff, x_diff, normalized_criterion, x_opt_diff]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion, x_opt_diff]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    if mode == x_opt_diff and x_sol is None: \n",
    "        raise ValueError(f\"using mode = {x_opt_diff} requires passing the solution to the problem\")\n",
    "\n",
    "    x_0 = np.expand_dims(x_0) if x_0.ndim == 1 else x_0\n",
    "\n",
    "    # create the array to save the gradients\n",
    "    phi_grads = np.concatenate([grad_function(X=data_batches[i], y=label_batches[i], w=x_0).reshape(-1, 1) for i in range(len(data_batches))], axis=1)\n",
    "    \n",
    "    x_current, x_previous = x_0, x_0\n",
    "    x_history = [x_current]\n",
    "    criterion_history = []\n",
    "\n",
    "    for k in tqdm(range(K)):\n",
    "        # generate the data index\n",
    "        data_index = random.randint(0, len(data_batches) - 1)\n",
    "        # data_index represents 'j' in the algorithm pseudo code\n",
    "\n",
    "        # save the k-th point\n",
    "        x_previous = x_current\n",
    "\n",
    "        # first extract the previous grad at 'data_index' \n",
    "        previous_phi_grad = phi_grads[:, [data_index]]            \n",
    "        next_phi_grad = grad_function(X=data_batches[data_index], y=label_batches[data_index], w=x_previous)\n",
    "        phi_grad_avg = np.mean(phi_grads, axis=1).reshape(-1, 1)\n",
    "\n",
    "        # calculate gamma\n",
    "        gamma = gamma_k(k)\n",
    "                \n",
    "        # compute the (k+1)-th point    \n",
    "        x_current = x_current - gamma * (next_phi_grad - previous_phi_grad + phi_grad_avg)\n",
    "\n",
    "        # make sure to save the new phi gradient\n",
    "        phi_grads[:, data_index] = next_phi_grad.squeeze()\n",
    "\n",
    "        # create the arguments to pass to the function: the entire data\n",
    "        func_args = {\"X\": data, 'y': labels, 'w': x_current}\n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(func_args), function(func_args))\n",
    "        \n",
    "        elif mode == x_diff:\n",
    "            diff = x_difference(x_current, x_previous)\n",
    "        \n",
    "        elif mode == normalized_criterion:\n",
    "            diff = norm(grad_function(**func_args), ord=2) / norm(grad_function(**{\"X\": data, 'y': labels, 'w': x_0}), ord=2)\n",
    "        \n",
    "        elif mode == x_opt_diff: \n",
    "            diff = norm(x_current - x_sol, ord=2)\n",
    "        else: \n",
    "            # the last case is where the criterion is passed as an argument\n",
    "            diff = mode(x_current)\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "        \n",
    "        if diff <= eps: \n",
    "            break\n",
    "\n",
    "    assert len(x_history) == k + 2, f\"expected {k + 2} points. Found: {len(x_history)}\"\n",
    "\n",
    "    # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return (x_history, criterion_history) if return_history else x_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dcaa51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# # run the simulation \n",
    "# x_points, criterions = simulation(data=X_train, \n",
    "#                                   labels=y_train,\n",
    "#                                   data_batches=X_10_BATCHES,\n",
    "#                                   label_batches=Y_10_BATCHES, \n",
    "#                                 optimization_algorithm=saga, \n",
    "#                                 function=p1_stochastic_value,\n",
    "#                                 grad_function=p1_stochastic_grad,\n",
    "#                                 gamma_k=SAGE_CONSTANT_GAMME, \n",
    "#                                 mode=normalized_criterion)\n",
    "\n",
    "# criterions = [np.log10(c) for c in criterions]\n",
    "\n",
    "# plot_iterations(criterions=criterions, \n",
    "#                 x_label='iteration', \n",
    "#                 y_label=f'{normalized_criterion}', \n",
    "#                 show=False)\n",
    "\n",
    "# plt.title(\"Saga\")\n",
    "# plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4d8a4",
   "metadata": {},
   "source": [
    "#### SVRG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac8b8f",
   "metadata": {},
   "source": [
    "The SVRG algorithm assumes that each function is $L$ smooth and $\\mu$ strongly convex. we have already chosen ,\n",
    "$$ \n",
    "\\tilde{L}  = max(L, L_1, L_2, ... L_s)\n",
    "$$\n",
    "\n",
    "and for each $f_i$, we have $\\mu = \\lambda$ ($\\lambda$ is set after choosing $L$)\n",
    "\n",
    "There are 2 hyperparameters: \n",
    "\n",
    "* $m$ the update frequence (The parameters) are updated $m$ at each iteration / epoch\n",
    "* $\\gamma$ the step size \n",
    "\n",
    "These 2 hyperparameters must be chosen such that: \n",
    "\n",
    "$$ \\frac{1}{ \\mu \\cdot \\gamma (1 - 2 L \\gamma)} + \\frac{2 L \\gamma }{1 - 2 L\\gamma } < 1$$\n",
    "\n",
    "The authors of the paper chose $\\gamma = \\frac{0.1}{L}$ and $m = O(n)$. I will choose the value $m = \\lfloor \\frac{n}{2} \\rfloor$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1a16724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition: 0.8756249999999999\n"
     ]
    }
   ],
   "source": [
    "# let's prepare the data for the SAGA algorithm.\n",
    "# first let's split the data into batches to define the inner functions\n",
    "X_10_BATCHES, Y_10_BATCHES = split_into_batches(X=X_train, y=y_train, batch_size=10, even_split=False)\n",
    "\n",
    "L_TILED, REG_TILED = set_L_Reg(X_train, y_train, X_10_BATCHES, Y_10_BATCHES)\n",
    "\n",
    "# a callable object with the main loss, gradient and hessian\n",
    "p1_stochastic_value = partial(p1_value_function, lam=REG_TILED)\n",
    "p1_stochastic_grad = partial(p1_gradient_function, lam=REG_TILED)\n",
    "p1_stochastic_h = partial(p1_hessian_function, lam=REG_TILED)\n",
    "\n",
    "# the estimation of 'L'\n",
    "assert abs(L_TILED / REG_TILED - 1000) <= 10\n",
    "\n",
    "SGVR_STEP_SIZE = 0.1 / L_TILED\n",
    "SGVR_UPDATE_FREQ = 20000\n",
    "\n",
    "# make sure the condition is satisfied\n",
    "COND_EXPRESSION = 1 / (REG_TILED * SGVR_STEP_SIZE * (1 - 2 * L_TILED * SGVR_STEP_SIZE) * SGVR_UPDATE_FREQ) + (2 * L_TILED * SGVR_STEP_SIZE) / (1 - 2 * L_TILED * SGVR_STEP_SIZE)\n",
    "\n",
    "assert COND_EXPRESSION < 1, f\"alpha must be less than 1. Found: {COND_EXPRESSION}\"\n",
    "\n",
    "print(f\"condition: {COND_EXPRESSION}\")\n",
    "\n",
    "SGVR_CONSTANT_GRAMMA = lambda _: SGVR_STEP_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "212dc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sgvr(data: np.ndarray, \n",
    "        labels: np.ndarray,\n",
    "        data_batches: Sequence[np.ndarray],\n",
    "        label_batches: Sequence[np.ndarray],\n",
    "        function: callable,\n",
    "        grad_function: callable, \n",
    "        x_0: np.ndarray,               \n",
    "        update_frequency: int = None,       \n",
    "        x_sol: np.ndarray=None,\n",
    "        K: int = 10 ** 3,\n",
    "        eps: float = 10 ** -5, \n",
    "        mode: str = normalized_criterion,\n",
    "        gamma_k: callable = None,                      \n",
    "        return_history: bool = False\n",
    "        ) -> Union[List[np.ndarray], np.ndarray]:\n",
    "    \n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if isinstance(mode , str) and mode not in [f_diff, x_diff, normalized_criterion, x_opt_diff]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion, x_opt_diff]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    if mode == x_opt_diff and x_sol is None: \n",
    "        raise ValueError(f\"using mode = {x_opt_diff} requires passing the solution to the problem\")\n",
    "\n",
    "\n",
    "    if update_frequency is None:\n",
    "        update_frequency = len(data_batches) // 2\n",
    "\n",
    "    x_current = np.expand_dims(x_0) if x_0.ndim == 1 else x_0\n",
    "\n",
    "    x_history = [x_current]\n",
    "    criterion_history = []\n",
    "\n",
    "    for k in tqdm(range(int(math.ceil(K)))):\n",
    "        # make sure to set 'x_previous' to 'x_current'\n",
    "        x_previous = x_current.copy()\n",
    "        # calculate gamma\n",
    "        gamma = gamma_k(k)\n",
    "\n",
    "        # create the arguments to pass to the function: the entire data\n",
    "        func_args = {\"X\": data, 'y': labels, 'w': x_current}\n",
    "\n",
    "        # calculate the gradient over the entire data\n",
    "        grad_data = grad_function(**func_args) \n",
    "\n",
    "        w_current = x_current\n",
    "        # sample 'update_frequency' batches\n",
    "        batch_indices = random.sample(range(len(data_batches)), k=update_frequency)\n",
    "        \n",
    "        for data_index in batch_indices:\n",
    "            batch_data, batch_label = data_batches[data_index], label_batches[data_index]\n",
    "            w_current = w_current - gamma * (grad_function(X=batch_data, y=batch_label,w=w_current) - \n",
    "                                             grad_function(X=batch_data, y=batch_label, w=x_current) + \n",
    "                                             grad_data)\n",
    "        \n",
    "        # set the k-th x\n",
    "        x_current = w_current\n",
    "\n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(func_args), function(func_args))\n",
    "        \n",
    "        elif mode == x_diff:\n",
    "            diff = x_difference(x_current, x_previous)\n",
    "        \n",
    "        elif mode == normalized_criterion:\n",
    "            diff = norm(grad_function(**func_args), ord=2) / norm(grad_function(**{\"X\": data, 'y': labels, 'w': x_0}), ord=2)\n",
    "        \n",
    "        elif mode == x_opt_diff: \n",
    "            diff = norm(x_current - x_sol, ord=2)\n",
    "        else: \n",
    "            # the last case is where the criterion is passed as an argument\n",
    "            diff = mode(x_current)\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "            \n",
    "        if diff <= eps: \n",
    "            break\n",
    "\n",
    "        assert len(x_history) == k + 2, f\"expected {k + 2} points. Found: {len(x_history)}\"\n",
    "\n",
    "    # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return (x_history, criterion_history) if return_history else x_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "394eb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prepare the data for the SAGA algorithm.\n",
    "# first let's split the data into batches to define the inner functions\n",
    "X_10_BATCHES, Y_10_BATCHES = split_into_batches(X=X_train, y=y_train, batch_size=10, even_split=False)\n",
    "\n",
    "L_TILED, REG_TILED = set_L_Reg(X_train, y_train, X_10_BATCHES, Y_10_BATCHES)\n",
    "\n",
    "# a callable object with the main loss, gradient and hessian\n",
    "p1_stochastic_value = partial(p1_value_function, lam=REG_TILED)\n",
    "p1_stochastic_grad = partial(p1_gradient_function, lam=REG_TILED)\n",
    "p1_stochastic_h = partial(p1_hessian_function, lam=REG_TILED)\n",
    "\n",
    "# the estimation of 'L'\n",
    "assert abs(L_TILED / REG_TILED - 1000) <= 10\n",
    "\n",
    "SAGE_CONSTANT_GAMME = lambda _: 1 / (2 * len(X_10_BATCHES) + L_TILED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8272a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# # run the simulation \n",
    "# x_points, criterions = simulation(data=X_train, \n",
    "#                                   labels=y_train,\n",
    "#                                   data_batches=X_10_BATCHES,\n",
    "#                                   label_batches=Y_10_BATCHES, \n",
    "#                                 optimization_algorithm=sgvr, \n",
    "#                                 function=p1_stochastic_value,\n",
    "#                                 grad_function=p1_stochastic_grad,\n",
    "#                                 gamma_k=SAGE_CONSTANT_GAMME, \n",
    "#                                 mode=normalized_criterion)\n",
    "\n",
    "# criterions = [np.log10(c) for c in criterions]\n",
    "\n",
    "# plot_iterations(criterions=criterions, \n",
    "#                 x_label='iteration', \n",
    "#                 y_label=f'{normalized_criterion}', \n",
    "#                 show=False)\n",
    "\n",
    "# plt.title(\"SVGR\")\n",
    "# plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ebd64",
   "metadata": {},
   "source": [
    "As in d.2), find the \"best\" step for SAGA, SVRG, SARAH. \n",
    "\n",
    "Draw the comparison plots for SGD, SAGA, SVRG, SARAH with the \"best\" steps: \n",
    "\n",
    "1) convergence criterion value, \n",
    "\n",
    "2) prediction accuracy. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0fa004b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19349804",
   "metadata": {},
   "source": [
    "# Problem 2. (25 points)__ \n",
    "This part of the assignment is related to __distributed with compression__ methods.  \n",
    "\n",
    "Let us simulate a distributed  environment. To do this, divide the training sample randomly and uniformly into $s=10$ parts (this was implemented in Problem 0). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b46824",
   "metadata": {},
   "source": [
    "## а). (5 points) \n",
    "Implement the $\\text{Rand}k$ and $\\text{Top}k$ compression operators, where $k$ is a parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96dce8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  6  7  5 13 12  4 18]\n",
      " [11  7  8  1 15  4  9 14]\n",
      " [15 19  3 14  2 14 10 19]\n",
      " [11 14  6 19 13  2 19 16]\n",
      " [15  3 15  8 15  7  5 12]\n",
      " [ 7 18  4 11  4  4 10 10]\n",
      " [ 5  7  2  4  6 17 18  6]\n",
      " [ 8 19 10  8 11  6 18 14]]\n",
      "##################################################\n",
      "[[ 0.  0.  0.  0.  0.  0.  0. 18.]\n",
      " [ 0.  0.  0.  0. 15.  0.  0.  0.]\n",
      " [15. 19.  0. 14.  0. 14.  0. 19.]\n",
      " [ 0.  0.  0. 19.  0.  0. 19.  0.]\n",
      " [15.  0. 15.  0. 15.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0. 17.  0.  0.]\n",
      " [ 0. 19. 10.  0.  0.  0. 18.  0.]]\n",
      "##################################################\n",
      "[[36. 24. 28. 20. 52. 48. 16. 72.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [32. 76. 40. 32. 44. 24. 72. 56.]]\n"
     ]
    }
   ],
   "source": [
    "def random_k_compression(x: np.ndarray, k: Union[int, float], column_vector: bool = True) -> np.ndarray:\n",
    "    if x.ndim > 2:\n",
    "        raise ValueError(f\"the input is expected to be a batch of 1 dimensional samples\")\n",
    "\n",
    "    if x.ndim == 1:\n",
    "        x = np.expand_dims(x, axis=-1)  \n",
    "    \n",
    "    if column_vector:\n",
    "        x = x.T\n",
    "\n",
    "    k = k if isinstance(k, int) else int(math.ceil(k * x.shape[1]))\n",
    "\n",
    "    # each column will represent a vector to compress  \n",
    "    coordinates = set(random.sample(population=range(x.shape[1]), k=k))\n",
    "\n",
    "    mask = np.asarray([[1 if index in coordinates else 0 for index in range(x.shape[1])] for _ in range(x.shape[0])])\n",
    "\n",
    "    compressed_x = x * mask * (x.shape[1] / k)\n",
    "\n",
    "    if column_vector:\n",
    "        return compressed_x.T\n",
    "    \n",
    "    return compressed_x\n",
    "\n",
    "\n",
    "def top_k_compression(x: np.ndarray, k: Union[int, float], column_vector: bool = True) -> np.ndarray:\n",
    "    if x.ndim > 2:\n",
    "        raise ValueError(f\"the input is expected to be a batch of 1 dimensional samples\")\n",
    "\n",
    "    if x.ndim == 1:\n",
    "        x = np.expand_dims(x, axis=-1)  \n",
    "    \n",
    "    if column_vector:\n",
    "        x = x.T\n",
    "    \n",
    "    k = k if isinstance(k, int) else int(math.ceil(k * x.shape[1]))\n",
    "\n",
    "\n",
    "    indices = np.argsort(x, axis=1)\n",
    "    indices = indices[:, -k:]\n",
    "    if indices.shape != (x.shape[0], k): \n",
    "        raise ValueError(f\"check the number of indices to be extracted. Expected: {k, x.shape[1]}. Found: {indices.shape}\")\n",
    "\n",
    "    mask = np.asarray([[1 if index in set(idx) else 0 for index in range(x.shape[1])] for idx in indices], dtype=np.float32) \n",
    "\n",
    "    # DON'T FORGET ABOUT THE COEFFICIENT\n",
    "    compressed_x = x * mask\n",
    "\n",
    "    if column_vector:\n",
    "        return compressed_x.T\n",
    "    \n",
    "    return compressed_x\n",
    "\n",
    "\n",
    "x = np.random.randint(low=1, high=20, size=(8, 8))\n",
    "print(x)\n",
    "print(\"#\" * 50)\n",
    "print(top_k_compression(x, k=2, column_vector=True))\n",
    "print(\"#\" * 50)\n",
    "print(random_k_compression(x, k=2, column_vector=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f0a52",
   "metadata": {},
   "source": [
    "## b). (10 points) \n",
    "\n",
    "Implement simple distributed gradient descent with compression operators. \n",
    "\n",
    "Compress information only from the devices to the server. \n",
    "\n",
    "Use compression operators $\\text{Rand}1\\%$, $\\text{Rand}5\\%$, $\\text{Rand}10\\%$, $\\text{Rand}20\\%$. As in d.2) of Problem 1, find the \"best\" steps for all cases. \n",
    "\n",
    "Draw the comparison plots with these \"best\" steps: \n",
    "\n",
    "1) convergence criterion value versus number of transmitted coordinates from the devices to the server, \n",
    "2) prediction accuracy versus number of transmitted coordinates from the devices to the server. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b2229021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prepare the different devices:\n",
    "# shuffle the data\n",
    "\n",
    "x_y_train = np.concatenate([X_train, y_train.reshape(-1, 1)], axis=1)\n",
    "np.random.shuffle(x_y_train)\n",
    "X_train, y_train = x_y_train[:, :-1], x_y_train[:, [-1]]\n",
    "\n",
    "x_y_test = np.concatenate([X_test, y_test.reshape(-1, 1)], axis=1)\n",
    "np.random.shuffle(x_y_test)\n",
    "X_test, y_test = x_y_test[:, :-1], x_y_test[:, [-1]]\n",
    "\n",
    "# split the entire data into devices\n",
    "\n",
    "device_size = int(math.ceil(len(X_train) / 10))\n",
    "\n",
    "DEVICES_DATA, DEVICES_LABELS = split_into_batches(X=X_train, y=y_train, even_split=False, batch_size=device_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3c09e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_TILED, REG_TILED = set_L_Reg(X_train, y_train, DEVICES_DATA, DEVICES_LABELS)\n",
    "\n",
    "# a callable object with the main loss, gradient and hessian\n",
    "p2_distributed_function = partial(p1_value_function, lam=REG_TILED)\n",
    "p2_distributed_grad = partial(p1_gradient_function, lam=REG_TILED)\n",
    "\n",
    "# the estimation of 'L'\n",
    "assert abs(L_TILED / REG_TILED - 1000) <= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d922b698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6499, 112), (6499, 1), (1625, 112), (1625, 1))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73a1a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def distributed_gradient_descent(\n",
    "                                data: np.ndarray,\n",
    "                                labels: np.ndarray,\n",
    "                                devices_data: Sequence[np.ndarray],\n",
    "                                devices_labels: Sequence[np.ndarray],\n",
    "                                function: callable,\n",
    "                                grad_function: callable,\n",
    "                                compression_function: callable,\n",
    "                                x_0: np.ndarray,\n",
    "                                x_sol: np.ndarray=None,\n",
    "                                K: int = 10 ** 3,\n",
    "                                eps: float = 10 ** -5, \n",
    "                                mode: str = normalized_criterion,\n",
    "                                gamma_k: callable = None,                      \n",
    "                                return_history: bool = False\n",
    "                                ) -> Union[List[np.ndarray], np.ndarray]:\n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if isinstance(mode , str) and mode not in [f_diff, x_diff, normalized_criterion, x_opt_diff]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion, x_opt_diff]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    if mode == x_opt_diff and x_sol is None: \n",
    "        raise ValueError(f\"using mode = {x_opt_diff} requires passing the solution to the problem\")\n",
    "\n",
    "    x_current = np.expand_dims(x_0) if x_0.ndim == 1 else x_0\n",
    "    x_history = [x_current]\n",
    "    criterion_history = []\n",
    "\n",
    "    for k in tqdm(range(K)):\n",
    "        x_previous = x_current.copy()\n",
    "\n",
    "        # the first step is to calculate the gradient at each device\n",
    "        device_grads_org = np.concatenate([grad_function(X=d_data, y=d_labels, w=x_current) for d_data, d_labels in zip(devices_data, devices_labels)], axis=1)\n",
    "        \n",
    "        # compress the gradient (as a batch)\n",
    "        compressed_grads = compression_function(device_grads_org)\n",
    "\n",
    "        # calculate the mean of the compressed gradients\n",
    "        mean_compressed_grad = np.mean(compressed_grads, axis=1)\n",
    "\n",
    "        if mean_compressed_grad.ndim == 1: \n",
    "            mean_compressed_grad = mean_compressed_grad.reshape(-1, 1)\n",
    "\n",
    "        # update with the mean of compressed gradients\n",
    "        x_current = x_current - gamma_k(k) * mean_compressed_grad\n",
    "\n",
    "        func_args = {\"X\": data, \"y\": labels, 'w': x_current}\n",
    "\n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(**func_args), function(**func_args))\n",
    "        \n",
    "        elif mode == x_diff:\n",
    "            diff = x_difference(x_current, x_previous)\n",
    "        \n",
    "        elif mode == normalized_criterion:\n",
    "            diff = norm(grad_function(**func_args), ord=2) / norm(grad_function(**{\"X\": data, 'y': labels, 'w': x_0}), ord=2)\n",
    "        \n",
    "        elif mode == x_opt_diff: \n",
    "            diff = norm(x_current - x_sol, ord=2)\n",
    "        else: \n",
    "            # the last case is where the criterion is passed as an argument\n",
    "            diff = mode(x_current)\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "            \n",
    "        if diff <= eps: \n",
    "            break\n",
    "\n",
    "        assert len(x_history) == k + 2, f\"expected {k + 2} points. Found: {len(x_history)}\"\n",
    "\n",
    "    # the function returns all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return (x_history, criterion_history) if return_history else x_history[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa99acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compression_percentages = [0.01, 0.05, 0.1, 0.2]\n",
    "# gamma_k = lambda _ : 1 / L_TILED\n",
    "\n",
    "# accuracies = []\n",
    "\n",
    "# for c in compression_percentages:   \n",
    "#     # create the function\n",
    "#     compression_function = partial(random_k_compression, k=c, column_vector=True)\n",
    "#     x_0 = set_up()\n",
    "#     x_points, criterions = distributed_gradient_descent(\n",
    "#                                 data=X_train,\n",
    "#                                 labels=y_train,\n",
    "#                                 devices_data=DEVICES_DATA,\n",
    "#                                 devices_labels=DEVICES_LABELS,\n",
    "#                                 function=p2_distributed_function,\n",
    "#                                 grad_function=p2_distributed_grad,\n",
    "#                                 compression_function=compression_function,\n",
    "#                                 x_0=x_0,\n",
    "#                                 gamma_k=gamma_k,\n",
    "#                                 mode=normalized_criterion, \n",
    "#                                 return_history=True,\n",
    "#                                 )   \n",
    "\n",
    "#     criterions = [np.log10(c) for c in criterions]\n",
    "#     plot_iterations(criterions=criterions, \n",
    "#                     x_label='iteration', \n",
    "#                     y_label=f'{normalized_criterion}', \n",
    "#                     plot_label=f'compression: {round(c, 3)}',\n",
    "#                     show=False)\n",
    "\n",
    "#     weight = x_points[-1]\n",
    "#     # predict\n",
    "#     predictions = predict(w=weight, X=X_test)\n",
    "#     acc = calculate_accuracy(predictions=predictions, labels=y_test)\n",
    "#     accuracies.append(acc)\n",
    "\n",
    "# print(accuracies)\n",
    "# plt.legend()\n",
    "# plt.title(f\"Distributed GD with random compression\")\n",
    "# plt.show()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ebc48448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar_plot(x=[str(int(c * 100)) for c in compression_percentages], y=[acc - 0.9 for acc in accuracies]  , \n",
    "#          x_label='compression coefficient', \n",
    "#          y_label='test accuracy', \n",
    "#          title='distributed gradient descent: test accuracy (-0.9) vs compression coefficient', \n",
    "#          fig_size=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7ed63",
   "metadata": {},
   "source": [
    "## c). (10 points)\n",
    "\n",
    "Now we use $\\text{Top}10\\%$ as compression operators. \n",
    "\n",
    "Compress the information only from the devices to the server. \n",
    "\n",
    "Use two methods: regular distributed gradient descent with compression operators, and distributed gradient descent with error feedback technique (see Algorithm 1 from [article](https://arxiv.org/abs/2002.12410)). \n",
    "\n",
    "For each method, select the step for the \"best\" convergence. \n",
    "\n",
    "Draw the comparison plots with these \"best\" steps: \n",
    "\n",
    "1) convergence criterion value versus number of transmitted coordinates from the devices to the server, \n",
    "2) prediction accuracy versus number of transmitted coordinates from the devices to the server. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fde35950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class\n",
    "class DistributedDevice:\n",
    "    def __init__(self, \n",
    "                 gradient_function: callable, \n",
    "                 data: np.ndarray,\n",
    "                 labels: np.ndarray,\n",
    "                 compression_function: callable\n",
    "                 ) -> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.gradient_function = gradient_function \n",
    "        self.comp = compression_function\n",
    "        self.error = np.zeros((data.shape[1], 1))\n",
    "\n",
    "    def local_grad(self, x_current: np.ndarray, step_size: float) -> np.ndarray:  \n",
    "        grad = self.gradient_function(**{\"X\": self.data, \"y\": self.labels, \"w\": x_current})\n",
    "\n",
    "        compressed_grad = self.comp(self.error + step_size * grad)\n",
    "\n",
    "        # update the error\n",
    "        self.error =  self.error + step_size * grad - compressed_grad\n",
    "        return compressed_grad\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e144a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_gradient_error_feedback(\n",
    "                                data: np.ndarray,\n",
    "                                labels: np.ndarray,\n",
    "                                devices_data: Sequence[np.ndarray],\n",
    "                                devices_labels: Sequence[np.ndarray],\n",
    "                                function: callable,\n",
    "                                grad_function: callable,\n",
    "                                compression_function: callable,\n",
    "                                x_0: np.ndarray,\n",
    "                                weights: np.ndarray = None,\n",
    "                                x_sol: np.ndarray=None,\n",
    "                                K: int = 10 ** 3,\n",
    "                                eps: float = 10 ** -5, \n",
    "                                mode: str = normalized_criterion,\n",
    "                                gamma_k: callable = None,                      \n",
    "                                return_history: bool = False\n",
    "                                ) -> Union[List[np.ndarray], np.ndarray]:\n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if isinstance(mode , str) and mode not in [f_diff, x_diff, normalized_criterion, x_opt_diff]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion, x_opt_diff]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    if mode == x_opt_diff and x_sol is None: \n",
    "        raise ValueError(f\"using mode = {x_opt_diff} requires passing the solution to the problem\")\n",
    "\n",
    "    weights = weights if weights is not None else np.ones(shape=(K + 1, 1))\n",
    "    # add another dimension if needed\n",
    "    weights = np.expand_dims(weights, axis=-1) if weights.ndim == 1 else weights\n",
    "\n",
    "    # define each of the devices\n",
    "    devices = [DistributedDevice(gradient_function=grad_function, \n",
    "                                 data=d_data, \n",
    "                                 labels=d_labels, \n",
    "                                 compression_function=compression_function) \n",
    "                for d_data, d_labels in zip(devices_data, devices_labels)]\n",
    "\n",
    "    x_current = np.expand_dims(x_0) if x_0.ndim == 1 else x_0\n",
    "    x_history = [x_current]\n",
    "    criterion_history = []\n",
    "\n",
    "    for k in tqdm(range(K)):\n",
    "        x_previous = x_current.copy()\n",
    "\n",
    "        gamma = gamma_k(k)\n",
    "\n",
    "        compressed_grads = np.concatenate([d.local_grad(x_current=x_current, step_size=gamma) for d in devices], axis=1)\n",
    "\n",
    "        # calculate the mean of the compressed gradients\n",
    "        mean_compressed_grad = np.mean(compressed_grads, axis=1)\n",
    "\n",
    "        if mean_compressed_grad.ndim == 1: \n",
    "            mean_compressed_grad = mean_compressed_grad.reshape(-1, 1)\n",
    "\n",
    "        # update with the mean of compressed gradients\n",
    "        x_current = x_current - gamma * mean_compressed_grad\n",
    "\n",
    "        func_args = {\"X\": data, \"y\": labels, 'w': x_current}\n",
    "\n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(**func_args), function(**func_args))\n",
    "        \n",
    "        elif mode == x_diff:\n",
    "            diff = x_difference(x_current, x_previous)\n",
    "        \n",
    "        elif mode == normalized_criterion:\n",
    "            diff = norm(grad_function(**func_args), ord=2) / norm(grad_function(**{\"X\": data, 'y': labels, 'w': x_0}), ord=2)\n",
    "        \n",
    "        elif mode == x_opt_diff: \n",
    "            diff = norm(x_current - x_sol, ord=2)\n",
    "        else: \n",
    "            # the last case is where the criterion is passed as an argument\n",
    "            diff = mode(x_current)\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "            \n",
    "        if diff <= eps: \n",
    "            break\n",
    "\n",
    "        assert len(x_history) == k + 2, f\"expected {k + 2} points. Found: {len(x_history)}\"\n",
    "\n",
    "    # unlike most methods, this function uses a weighted average as the numerical solutio\n",
    "    weighted_result = np.concatenate([x for x in x_history], axis=1) @ weights / np.sum(weights)\n",
    "\n",
    "    return (weighted_result, x_history, criterion_history) if return_history else weighted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c4f34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the regular distributed regime\n",
    "# gamma_k = lambda _ : 1 / L_TILED\n",
    "# # create the function\n",
    "# top10_compression = partial(top_k_compression, k=10, column_vector=True)\n",
    "# x_0 = set_up()\n",
    "# x_points, criterions = distributed_gradient_descent(\n",
    "#                             data=X_train,\n",
    "#                             labels=y_train,\n",
    "#                             devices_data=DEVICES_DATA,\n",
    "#                             devices_labels=DEVICES_LABELS,\n",
    "#                             function=p2_distributed_function,\n",
    "#                             grad_function=p2_distributed_grad,\n",
    "#                             compression_function=top10_compression,\n",
    "#                             x_0=x_0,\n",
    "#                             gamma_k=gamma_k,\n",
    "#                             mode=normalized_criterion, \n",
    "#                             return_history=True,\n",
    "#                             )   \n",
    "\n",
    "\n",
    "# criterions = [np.log10(c) for c in criterions]\n",
    "# plot_iterations(criterions=criterions, \n",
    "#                 x_label='iteration', \n",
    "#                 y_label=f'{normalized_criterion}', \n",
    "#                 plot_label=f'compression: top 10',\n",
    "#                 show=False)\n",
    "\n",
    "# # calculate the accuary for the regular distributed regime\n",
    "# weight = x_points[-1]\n",
    "# regular_distributed_predictions = predict(w=weight, X=X_test)\n",
    "# regular_distributed_acc = calculate_accuracy(predictions=regular_distributed_predictions, labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ae92c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the distributed regime with feedback\n",
    "# weight, x_points, criterions = distributed_gradient_error_feedback(\n",
    "#                             data=X_train,\n",
    "#                             labels=y_train,\n",
    "#                             devices_data=DEVICES_DATA,\n",
    "#                             devices_labels=DEVICES_LABELS,\n",
    "#                             function=p2_distributed_function,\n",
    "#                             grad_function=p2_distributed_grad,\n",
    "#                             compression_function=top10_compression,\n",
    "#                             x_0=x_0,\n",
    "#                             gamma_k=gamma_k,\n",
    "#                             mode=normalized_criterion, \n",
    "#                             return_history=True,\n",
    "#                             )   \n",
    "\n",
    "# criterions = [np.log10(c) for c in criterions]\n",
    "# plot_iterations(criterions=criterions, \n",
    "#                 x_label='iteration', \n",
    "#                 y_label=f'{normalized_criterion}', \n",
    "#                 plot_label=f'compression: top 10',\n",
    "#                 show=False)\n",
    "\n",
    "# distributed_with_feedback_predictions = predict(w=weight, X=X_test)\n",
    "# distributed_with_feedback_acc = calculate_accuracy(predictions=distributed_with_feedback_predictions, labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "81c0eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.legend()\n",
    "# plt.title(f\"Distributed GD with top10% compression: regular VS with error feedback\")\n",
    "# plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0f0db604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar_plot(x=['regular distributed regime', 'distributed with error feedback'], y=[regular_distributed_acc, distributed_with_feedback_acc]  , \n",
    "#          x_label='compression coefficient', \n",
    "#          y_label='test accuracy', \n",
    "#          title='distributed gradient descent: test accuracy (-0.9) vs compression coefficient', \n",
    "#          fig_size=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a83a3d",
   "metadata": {},
   "source": [
    "# Problem 3. (15 points)\n",
    "\n",
    "This part of the assignment is related to __distributed with local steps__ methods.\n",
    "\n",
    "As in the previous problem we simulate the distributed enviroment with $s = 10$. \n",
    "\n",
    "Implement simple Local GD/FedAvg method (Algorithm 1 of [paper](https://arxiv.org/pdf/1909.04746.pdf)), where you can choose the number of local steps $H$ as a parameter. \n",
    "\n",
    "Vary the number of local steps: $H = 1, 5, 10, 20$.\n",
    "\n",
    "Take hyperparameters of the method according to the theory (see the corresponding paper). \n",
    "\n",
    "Write these parameters. \n",
    "\n",
    "Draw the comparison plots for different $H$: \n",
    "\n",
    "1) value of convergence criterion versus number of communications, \n",
    "2) accuracy of predictions versus number of communications. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ae704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalGradientDescentDevice:\n",
    "    def __init__(self, \n",
    "                 gradient_function: callable, \n",
    "                 data: np.ndarray,\n",
    "                 labels: np.ndarray,\n",
    "                 num_local_steps: int\n",
    "                 ) -> None:\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.gradient_function = gradient_function \n",
    "        self.H = num_local_steps\n",
    "\n",
    "    def local_grad(self, x_current: np.ndarray, step_size: float) -> np.ndarray:  \n",
    "        x = x_current.copy()\n",
    "        for _ in range(self.H):\n",
    "            grad = self.gradient_function(**{\"X\": self.data, \"y\": self.labels, \"w\": x})\n",
    "            x = x - step_size * grad\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f34d2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_GD(\n",
    "    num_local_steps:int,\n",
    "    data: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    devices_data: Sequence[np.ndarray],\n",
    "    devices_labels: Sequence[np.ndarray],\n",
    "    function: callable,\n",
    "    grad_function: callable,\n",
    "    x_0: np.ndarray,\n",
    "    x_sol: np.ndarray=None,\n",
    "    K: int = 10 ** 3,\n",
    "    eps: float = 10 ** -5, \n",
    "    mode: str = normalized_criterion,\n",
    "    gamma_k: callable = None,                      \n",
    "    return_history: bool = False\n",
    "    ) -> Union[List[np.ndarray], np.ndarray]:\n",
    "\n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if isinstance(mode , str) and mode not in [f_diff, x_diff, normalized_criterion, x_opt_diff]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion, x_opt_diff]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    if mode == x_opt_diff and x_sol is None: \n",
    "        raise ValueError(f\"using mode = {x_opt_diff} requires passing the solution to the problem\")\n",
    "\n",
    "    # make sure the number of local_steps is at least 1\n",
    "    num_local_steps = max(1, num_local_steps)\n",
    "\n",
    "    # define the local devices\n",
    "    devices = [LocalGradientDescentDevice(gradient_function=grad_function,\n",
    "                                          data=d_data, \n",
    "                                          labels=d_labels,\n",
    "                                          num_local_steps=num_local_steps\n",
    "                                        ) for d_data, d_labels in zip(devices_data, devices_labels)]\n",
    "\n",
    "    x_current = np.expand_dims(x_0) if x_0.ndim == 1 else x_0\n",
    "    x_history = [x_current]\n",
    "    criterion_history = []\n",
    "\n",
    "    for k in tqdm(range(math.ceil(K / num_local_steps))):\n",
    "        x_previous = x_current.copy()\n",
    "\n",
    "        gamma = gamma_k(k)\n",
    "\n",
    "        local_xs = np.concatenate([d.local_grad(x_current=x_current, step_size=gamma) for d in devices], axis=1)\n",
    "\n",
    "        # calculate the mean of the compressed gradients\n",
    "        x_current = np.mean(local_xs, axis=1) \n",
    "        x_current = x_current.reshape(-1, 1) if x_current.ndim == 1 else x_current\n",
    "\n",
    "        func_args = {\"X\": data, \"y\": labels, 'w': x_current}\n",
    "\n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(**func_args), function(**func_args))\n",
    "        \n",
    "        elif mode == x_diff:\n",
    "            diff = x_difference(x_current, x_previous)\n",
    "        \n",
    "        elif mode == normalized_criterion:\n",
    "            diff = norm(grad_function(**func_args), ord=2) / norm(grad_function(**{\"X\": data, 'y': labels, 'w': x_0}), ord=2)\n",
    "        \n",
    "        elif mode == x_opt_diff: \n",
    "            diff = norm(x_current - x_sol, ord=2)\n",
    "        else: \n",
    "            # the last case is where the criterion is passed as an argument\n",
    "            diff = mode(x_current)\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "            \n",
    "        if diff <= eps: \n",
    "            break\n",
    "\n",
    "        assert len(x_history) == k + 2, f\"expected {k + 2} points. Found: {len(x_history)}\"\n",
    "\n",
    "    return  (x_history, criterion_history) if return_history else x_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e2ccf7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 71.15it/s]\n",
      "100%|██████████| 200/200 [00:06<00:00, 28.68it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 21.08it/s]\n",
      "100%|██████████| 50/50 [00:04<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9858461538461538, 0.9858461538461538, 0.9858461538461538, 0.9858461538461538]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_steps = [1, 5, 10, 20]\n",
    "gamma_k = lambda _ : 1 / L_TILED\n",
    "accuracies = []\n",
    "\n",
    "for l in local_steps:\n",
    "    # create the function\n",
    "    x_0 = set_up()\n",
    "    x_points, criterions = local_GD(num_local_steps=l,\n",
    "                                data=X_train,\n",
    "                                labels=y_train,\n",
    "                                devices_data=DEVICES_DATA,\n",
    "                                devices_labels=DEVICES_LABELS,\n",
    "                                function=p2_distributed_function,\n",
    "                                grad_function=p2_distributed_grad,\n",
    "                                x_0=x_0,\n",
    "                                gamma_k=gamma_k,\n",
    "                                mode=normalized_criterion, \n",
    "                                return_history=True,\n",
    "                                )   \n",
    "\n",
    "    criterions = [np.log10(c) for c in criterions]\n",
    "    plot_iterations(criterions=criterions, \n",
    "                    x_label='number of communications', \n",
    "                    y_label=f'{normalized_criterion}', \n",
    "                    plot_label=f'local steps: {l}',\n",
    "                    show=False)\n",
    "\n",
    "    weight = x_points[-1]\n",
    "    # predict\n",
    "    predictions = predict(w=weight, X=X_test)\n",
    "    acc = calculate_accuracy(predictions=predictions, labels=y_test)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(accuracies)\n",
    "plt.legend()\n",
    "plt.title(f\"Local Gradient Descent\")\n",
    "plt.show()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6fe7f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bar_plot(x=[str(int(10 ** 3 / l)) for l in local_steps], y=[acc for acc in accuracies]  , \n",
    "         x_label='number of communications', \n",
    "         y_label='test accuracy', \n",
    "         title='distributed gradient descent: test accuracy vs compression coefficient', \n",
    "         fig_size=(8, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

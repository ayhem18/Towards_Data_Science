{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3a6fa0",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "This is a homework assignment on the material from the 6th-9th classes. The deadline for submission is __23:59 on October 29__.\n",
    "\n",
    "- The homework is done in the same Jupyter Notebook.\n",
    "\n",
    "- The file should be renamed: __Group Number_First Name_Last Name__ (no spaces at the beginning or end). Example: __000_Ivan_Ivanov__.\n",
    "\n",
    "- Homework should be sent to __OptimizationHomework@yandex.ru__. Subject line: __Innopolis_Task number__ (without spaces at the beginning and end). For this assignment, the subject line is: __Innopolis_2__.\n",
    "\n",
    "- Place the solution to each problem/item after the condition.\n",
    "\n",
    "- Do not forget to add necessary explanations and comments.\n",
    "\n",
    "- All technical artifacts should be removed in the final version that will be sent for checking. By such artifacts we mean any cell outputs that are not commented in any way in the text, as well as any bulk/long technical output (even if it is commented in the text).\n",
    "\n",
    "- A full run of the solution (Kernel -> Restart & Run All) should run all cells without errors.\n",
    "\n",
    "- The maximum score for the assignment is 100.\n",
    "\n",
    "We wish you success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386bbbb",
   "metadata": {},
   "source": [
    "## Part 1. Solving the unconstrained optimization problem\n",
    "\n",
    "Consider the problem of empirical risk minimization:\n",
    "\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w \\|^2_2,\n",
    "\\end{equation}\n",
    "where $\\ell$ is the loss function, $g$ is the model, $w$ is the model parameters, $\\{x_i, y_i\\}_{i=1}^n$ is the data sample from feature vectors $x_i$ and labels $y_i$, $\\lambda > 0$ is the regularization parameter.\n",
    "\n",
    "We use the linear model $g(w, x) = w^T x$ and the logistic/sigmoidal loss function: $\\ell(z,y) = \\ln (1 + \\exp(-yz))$ (__Important: $y$ must take values $-1$ or $1$__). The resulting problem is called a logistic regression problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d9136",
   "metadata": {},
   "source": [
    "## __Problem 1. (20 points)__ Let us do some preparation work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1d1e6",
   "metadata": {},
   "source": [
    "\n",
    "### __а). (15 points)__ Write out the gradient and the Hessian for this problem. \n",
    "\n",
    "Prove that the problem is $\\mu$-strongly convex and has $L$-Lipschitz gradient with $\\mu = \\lambda$ and $L = \\lambda + \\frac{1}{4n} \\sum_{i=1}^n \\| x_i\\|^2_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed093c2",
   "metadata": {},
   "source": [
    "Let $L_s$ be the problem's function. Let's define $L_s = L_1 + L_2$, where\n",
    "\\begin{align}\n",
    "L_1 &= \\frac{1}{n}\\sum_{i=1}^{n} l(<w, x_i>, y_i) \\\\\n",
    "L_2 &= \\frac{\\lambda}{2} \\| w \\|_2 ^ 2\n",
    "\\end{align}\n",
    "$L_1$ is a sum, so let's define the derivative of each term.\n",
    "\n",
    "Let's denote $y_i$ by $y$, $x_i$ by $x$,and $z = <y.x, w>$\n",
    "\n",
    "Then, we have:\n",
    "\\begin{align}\n",
    "dz = <yx, dw>\n",
    "\\end{align}\n",
    "Defining $h = \\ln(1 + \\exp(-t))$, then $h^{'} = \\frac{-1}{1 + e^t}$ and:\n",
    "\n",
    "\\begin{align}\n",
    "f &= h(z)  && \\text{$f$ is a single term in the sum}\\\\\n",
    "df &= h'(z) \\cdot dz \\\\\n",
    "df &=  \\frac{-1}{1 + e^z} \\cdot <y.x, dw> \\\\\n",
    "df &=  \\frac{- <y.x, dw>}{1 + e^{<y.x, w>}}  \\\\\n",
    "\\implies \\nabla f &= -\\frac{y \\cdot x}{1 + e^{<y \\cdot x, w>}} \\\\\n",
    "\\implies \\nabla L_1 &= \\sum_{i = 1}^n -\\frac{y_i \\cdot x_i}{1 + e^{<y_i \\cdot x_i, w>}}\n",
    "\\end{align}\n",
    "\n",
    "As for $L_2$, we have:\n",
    "\\begin{align}\n",
    "d(L_2) &= \\lambda \\cdot <w, dw> \\\\\n",
    "\\implies \\nabla L_s &= \\frac{1}{n}\\sum_{i = 1}^n -\\frac{y_i \\cdot x_i}{1 + e^{<y_i \\cdot x_i, w>}} + \\lambda \\cdot w\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f27642",
   "metadata": {},
   "source": [
    "* 2nd order derivative: Hessian matrix:\n",
    "\n",
    "I will use the same approach as in the first part.\n",
    "\\begin{align}\n",
    "d^2(f) &= <d (\\nabla f), dw_1> \\\\\n",
    "d (\\nabla f) &= - \\frac{(1 + e^z) \\cdot d(y \\cdot x) - d(1 + e^z) \\cdot y \\cdot x}{(1 + e^z)^2} \\\\\n",
    "d(\\nabla f) &= \\frac{e ^ z \\cdot <y \\cdot x, dw> \\cdot y \\cdot x }{(1 + e^z)^2}&& \\text{differentiation with respect to $w \\implies d(y \\cdot x) = 0$} \\\\\n",
    "\\implies d(\\nabla f) &= \\frac{e ^ z \\cdot x\\cdot <x, dw>}{(1 + e^z)^2} && \\text{using $y \\in \\{1, -1\\}$} \\\\\n",
    "\\implies d(\\nabla f) &= \\frac{e ^ z \\cdot x\\cdot x^T \\cdot dw}{(1 + e^z)^2} \\\\\n",
    "\\implies d^2(f) &= <\\frac{e ^ z \\cdot x\\cdot x^T \\cdot dw}{(1 + e^z)^2}, dw_1> \\\\\n",
    "\\implies d^2(f) &= <\\frac{e ^ z \\cdot x\\cdot x^T}{(1 + e^z)^2} \\cdot dw_1, dw> && \\text{using $<Ab, c> = <A^Tc, b>$} \\\\\n",
    "\\implies \\nabla ^ 2 f &= \\frac{e ^ {<y \\cdot x, w>} \\cdot x\\cdot x^T}{(1 + e^{<y \\cdot x, w>})^2} \\\\\n",
    "\\implies \\nabla ^ 2 L_1 &= \\frac{1}{n} \\sum_{i=1}^n \\frac{e ^ {<y_i \\cdot x_i, w>} \\cdot x_i\\cdot x_i^T}{(1 + e^{<y_i \\cdot x_i, w>})^2}\n",
    "\\end{align}\n",
    "\n",
    "As for $\\nabla ^ 2 L_2 = λ$. Therefore,\n",
    "$$ \\nabla ^ 2 L_s = \\frac{1}{n} \\sum_{i=1}^n \\frac{e ^ {<y_i \\cdot x_i, w>} \\cdot x_i\\cdot x_i^T}{(1 + e^{<y_i \\cdot x_i, w>})^2} + λ \\cdot I_d  \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b375dc",
   "metadata": {},
   "source": [
    "For any vector $x$, the matrix $x \\cdot x^T$ is positive definite, since:\n",
    "\n",
    "\\begin{align}\n",
    " a^T \\cdot (x \\cdot x^T) \\cdot a &= b^T \\cdot b \\geq 0&& \\text{where $b = x ^ T \\cdot a$}\n",
    "\\end{align}\n",
    "and since, the term $\\frac{e ^ {<y_i \\cdot x_i, w>}}{(1 + e^{<y_i \\cdot x_i, w>})^2}$ is always positive, we can say taht $L_1$ is indeed convex.\n",
    "\n",
    "$I_d$ is positive definite. and Thus, $L_s$ is convex.\n",
    "\n",
    "$L_s$ is also strong convex with $\\mu = λ$ since $∇^2 L_s - λ \\cdot I_d$ is semi positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc284a",
   "metadata": {},
   "source": [
    "* Estimate: $L$\n",
    "\n",
    "Let's define $a_i = y_i \\cdot x_i$, then we have:\n",
    "\n",
    "$$\n",
    "\\nabla L_s = \\frac{1}{n}\\sum_{i = 1}^n -\\frac{a_i}{1 + e^{<a_i, w>}} + \\lambda \\cdot w\n",
    "$$\n",
    "The constant $L$ satisfies the following inequality:\n",
    "\n",
    "\\begin{align}\n",
    "\\| \\nabla L_s(w_1) - \\nabla L_s (w_2) \\| &\\leq L \\cdot \\|w_1 - w_2 \\| ~~ \\forall w_1, w_2 \\in \\mathbb{R}^{d} \\\\\n",
    "\\| \\frac{1}{n} \\cdot  \\sum_{i=1}^n -\\frac{a_i}{1 + e^{<a_i, w1>}} + \\lambda w_1 + \\frac{1}{n} \\cdot  \\sum_{i=1}^n \\frac{a_i}{1 + e^{<a_i, w_2>}} -  \\lambda \\cdot  w_2 \\| &\\leq L \\cdot \\|w_1 - w_2 \\|\n",
    "\\end{align}\n",
    "The inequality above is slightly challenging to solve. We proceed as follows:\n",
    "\n",
    "\\begin{align}\n",
    " \\|\\sum_{i = 1}^n x_i \\| & \\leq \\sum_{i = 1}^n \\|x_i\\| \\\\\n",
    " \\implies \\| \\frac{1}{n} \\cdot \\sum_{i=1}^n -\\frac{a_i}{1 + e^{<a_i, w1>}} + \\lambda w_1 + \\frac{1}{n} \\cdot \\sum_{i=1}^n \\frac{a_i}{1 + e^{<a_i, w_2>}} -  \\lambda \\cdot  w_2 \\| & \\leq \\frac{1}{n} \\cdot \\sum_{i = 1}^n \\| a_i \\| \\cdot | \\frac{1}{1 + e^{<a_i, w_1>}} - \\frac{1}{1 + e ^ {<a_i, w_2>}}| + \\lambda \\cdot \\|w_1 - w_2 \\|\n",
    "\\end{align}\n",
    "\n",
    "To estimate $| \\frac{1}{1 + e^{<a_i, w_1>}} - \\frac{1}{1 + e ^{<a_i, w_2>}}|$, we need the following result (cited from [this paper](https://www.emis.de/journals/AM/08-3/am1630.pdf)):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{|e^x - e^y|}{e^x + e^y} &\\leq \\frac{|x - y|}{2} ~~ \\forall x, y \\in \\mathbb{R} \\\\\n",
    "|\\frac{1}{1 + e^x} - \\frac{1}{1 + e^y} | &= |\\frac{e^y - e^x}{(1 + e^x) \\cdot (1 + e^y)}| \\\\\n",
    "(1 + e^x) \\cdot (1 + e^y) &\\geq (e^x + e^y) \\\\\n",
    "\\implies \\frac{|e^x - e^y|}{(1 + e^x) \\cdot (1 + e^y)} &\\leq \\frac{|e^x - e^y|}{e^x + e^y} \\\\\n",
    "\\implies \\frac{|e^x - e^y|}{(1 + e^x) \\cdot (1 + e^y)} &\\leq \\frac{|x - y|}{2}\n",
    "\\end{align}\n",
    "\n",
    "substituting, $x = <a_i, w_1>$ and $y = <a_i, w_2>$:\n",
    "\\begin{align}\n",
    "| \\frac{1}{1 + e^{<a_i, w_1>}} - \\frac{1}{1 + e ^{<a_i, w_2>}}| &\\leq \\frac{| <a_i, w_1> - <a_i, w_2> |}{2} = \\frac{|<a_i, w_1 - w_2>|}{2} \\\\\n",
    "|<a_i, w_1 - w_2>| &\\leq \\|a_i\\| \\cdot \\|w_1 - w_2 \\| && \\text{by Cauchy inequality} \\\\\n",
    "\\implies | \\frac{1}{1 + e^{<a_i, w_1>}} - \\frac{1}{1 + e ^{<a_i, w_2>}}| &\\leq \\frac{1}{2} \\cdot \\|a_i\\| \\cdot \\|w_1 - w_2 \\|\n",
    "\\end{align}\n",
    "\n",
    "Using this result:\n",
    "\n",
    "\\begin{align}\n",
    "\\| \\sum_{i=1}^n -\\frac{a_i}{1 + e^{<a_i, w1>}} + \\lambda w_1 + \\sum_{i=1}^n \\frac{a_i}{1 + e^{<a_i, w_2>}} -  \\lambda \\cdot  w_2 \\| &\\leq \\frac{1}{2n} \\cdot\n",
    "\\sum_{i = 1}^{n}\\|a_i\\| ^ 2 \\cdot \\|w_1 - w_2\\| + \\lambda \\|w_1 - w_2\\|\n",
    "= (\\frac{1}{n} \\sum_{i = 1}^{n}\\|a_i\\| ^ 2 + \\lambda) \\cdot \\|w_1 - w_2 \\|\n",
    "\\end{align}\n",
    "Thus, for \n",
    "$$L = \\frac{1}{2n} \\sum_{i = 1}^{n}\\|x_i\\| ^ 2 + \\lambda \\\\\n",
    "\\| \\nabla L_s(w_1) - \\nabla L_s (w_2) \\| \\leq L \\cdot \\|w_1 - w_2 \\|~~ \\forall w_1, w_2 \\in \\mathbb{R}^{d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61362fe",
   "metadata": {},
   "source": [
    "The _mushrooms_ dataset is attached. Use the following code to generate a matrix $X$ and vector $y$, which will store the sample $\\{x_i, y_i\\}_{i=1}^n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "#the file must be in the same directory as notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ef9c8",
   "metadata": {},
   "source": [
    "Let us change the vector $y$ so that $y_i$ takes values $-1$ and $1$. You can also do additional preprocessing of the data (with techniques from machine learning), but this is not additionally assessed in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * y - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabb63a",
   "metadata": {},
   "source": [
    "Let us divide the data into two parts: training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply some preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit it to the data\n",
    "X_trian = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform the test split\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d427e",
   "metadata": {},
   "source": [
    "### __b). (5 points)__ \n",
    "For the training part $X_{train}$, $y_{train}$, estimate the constant $L$. \n",
    "\n",
    "Set $\\lambda$ such that $\\lambda \\approx L / 1000$.  \n",
    "\n",
    "Realize in the code the calculation of the value, gradient and Hessian for our target function ($X$, $y$, $\\lambda$ should be given as a parameter so that it is possible to change them, not only to substitute fixed $X_{train}$, $y_{train}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List, Union\n",
    "\n",
    "def p1_verify_input(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # first let's make sure the input is as expected: X is expected to have samples as rows\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"the training is expected to have at least 2 samples\")\n",
    "    \n",
    "    num_samples, dim = X.shape \n",
    "    # make sure 'y' matches the number of samples\n",
    "    y = np.expand_dims(y, axis=1) if y.ndim == 1 else y\n",
    "\n",
    "    if y.shape != (num_samples, 1):\n",
    "        raise ValueError((f\"The number of labels is expected to match the number of samples\"\n",
    "                          f\"\\nFound: {y.shape} Expected: {(num_samples, 1)}\"))\n",
    "    # check 'w' as well:\n",
    "    w = np.expand_dims(w, axis=0) if w.ndim == 1 else w\n",
    "    # make sure the dimensions match\n",
    "    if w.shape != (dim, 1):\n",
    "        raise ValueError((f\"The weight matrix 'w' is expected as a column vector with length {dim}\\n\"\n",
    "                          f\"Expected: {(dim, 1)}. Found: {w.shape}\"))\n",
    "    return X, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec184b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1_value_function(X: np.ndarray, \n",
    "                   y: np.ndarray,\n",
    "                   w: np.ndarray, \n",
    "                   lam: float) -> float:\n",
    "    \"\"\" This function calculates different values of the function gives it parameters:\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The training data\n",
    "        y (np.ndarray): The labels\n",
    "        w (np.ndarray): the weights\n",
    "        lam (float): lambda: the regularization hyper-parameter\n",
    "\n",
    "    Returns:\n",
    "        float: The value for the function with parameter 'w'\n",
    "    \"\"\"\n",
    "    # first step is to verify the input\n",
    "    X, y, w = p1_verify_input(X, y, w)\n",
    "    \n",
    "    num_samples, _ = X.shape\n",
    "    # now as our input has been verified we proceed by defining some useful intermediate variables\n",
    "    A = X * y\n",
    "    preds = A @ w\n",
    "\n",
    "    # preds should be of shape: num_samples, 1\n",
    "    preds = np.expand_dims(preds, axis=-1) if preds.ndim == 1 else preds\n",
    "    # let's calculate the exponential function: \n",
    "    exp = np.exp(-preds) # exp now contains the at each row: exp(w_i^T * y_i* x_i)\n",
    "    assert exp.shape == (num_samples, 1), \" Make sure the shape of the exponential matrix is correct\"\n",
    "\n",
    "    # calculate L1\n",
    "    # exp contains the exponential of -w^T x_i in each row    \n",
    "    # then add 1 and apply log on each of them\n",
    "    # then the mean    \n",
    "\n",
    "    l1 = np.mean(np.log(1 + exp))\n",
    "    \n",
    "    # calculate L2 : lambda / 2 * the norm of 'w' squared\n",
    "    l2 = (lam / 2) * np.linalg.norm(w) ** 2\n",
    "\n",
    "    # sum the main loss and the regularization\n",
    "    l = l1 + l2\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1_gradient_function(X: np.ndarray, \n",
    "                      y: np.ndarray, \n",
    "                      w: np.ndarray, \n",
    "                      lam: float) -> np.ndarray:\n",
    "    # verify the input\n",
    "    X, y, w = p1_verify_input(X, y, w)\n",
    "\n",
    "    A = X * y\n",
    "    preds = A @ w \n",
    "\n",
    "    # preds contains the w^T x_i in each row\n",
    "    # apply the exponential on each\n",
    "    # A / (1 + np.exp(preds)) contains y_i * x_i / (1 + e^{w_i^T y_i x_i}) in each row: which a row vector\n",
    "\n",
    "    # then calculate the mean over axis=0 to get a row vector\n",
    "    \n",
    "    dl1 = np.mean(A / (1 + np.exp(preds)), axis=0)\n",
    "\n",
    "    assert dl1.shape == (X.shape[1], )\n",
    "\n",
    "    dl = - np.expand_dims(dl1, axis=-1) + lam * w\n",
    "\n",
    "    assert dl.shape == (X.shape[1], 1), \"The gradient is expected to be a column vector.\"\n",
    "    \n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ddb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1_hessian_function(X: np.ndarray, \n",
    "                   y: np.ndarray, \n",
    "                   w: np.ndarray, \n",
    "                   lam: float) -> np.ndarray:\n",
    "    # verify the input\n",
    "    X, y, w = p1_verify_input(X, y, w)\n",
    "    num_samples, dim = X.shape\n",
    "\n",
    "    A = X * y\n",
    "    preds = A @ w \n",
    "\n",
    "    # convert to np.128 to avoid numerical overflow\n",
    "    preds = preds.astype(np.float64)\n",
    "\n",
    "    # create the hessian matrix with zero values\n",
    "    hessian_matrix = np.zeros((dim, dim), dtype=np.float64)\n",
    "\n",
    "    exp_coeffs = 1 / (1 + np.exp(preds)) - 1 / ((1 + np.exp(preds)) ** 2)\n",
    "\n",
    "    # # write the expression of the coefficients at once to avoid numerical overflow\n",
    "    # exp_coeffs = np.exp(preds) / ((1 + np.exp(preds)) ** 2)\n",
    "\n",
    "    # # exp1 will contain the exp (w^T y_i x_i) in each row\n",
    "    # exp1 = np.exp(preds)\n",
    "    # # exp2 will contains (1 + exp (w^T y_i x_i)) ^ 2\n",
    "    # exp2 = (1 + exp1) ** 2\n",
    "\n",
    "    assert exp_coeffs.shape == (num_samples, 1)\n",
    "\n",
    "    # assert exp1.shape == (num_samples, 1) and exp2.shape == (num_samples, 1)\n",
    "\n",
    "    #iterate through each sample\n",
    "    for i in range(num_samples):\n",
    "        # first step calculate the x_i * x_i ^ T\n",
    "        # extract x_i: row vector in code\n",
    "        x_i = X[i, :]\n",
    "        \n",
    "        assert x_i.shape == (dim,), \"The row sample is expected to be 1 dimensional\"\n",
    "        # expand \n",
    "        x_i = np.expand_dims(x_i, axis=-1)\n",
    "\n",
    "        matrix_xi = x_i @ x_i.T\n",
    "        # add an assert to catch any errors with shapes\n",
    "        assert matrix_xi.shape == (dim, dim), \"Make sure the matrix x_i * x_i ^ T is computed correctly\"\n",
    "\n",
    "        hessian_xi = exp_coeffs[i][0] * matrix_xi\n",
    "        # time to add the coefficient associated with the matrix_xi\n",
    "        # hessian_xi = (exp1[i][0] / exp2[i][0]) * matrix_xi\n",
    "        hessian_matrix += hessian_xi\n",
    "\n",
    "    # make sure the shape of the hessian matrix \n",
    "    hessian_matrix.shape == (dim , dim), \"Make sure the hessian matrix is of the correct shape\"\n",
    "    return hessian_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8349ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(4, 5)\n",
    "\n",
    "n1 = np.linalg.norm(X) ** 2\n",
    "\n",
    "n2 = 0\n",
    "for xi in X: \n",
    "    n2 += np.linalg.norm(xi) ** 2\n",
    "\n",
    "np.isclose(n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3275a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p1_L_estimation(X: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 w: np.ndarray, \n",
    "                 lam: float) -> float:\n",
    "    \n",
    "    # return the estimation by the problem description\n",
    "    X, y, w = p1_verify_input(X, y, w) \n",
    "    \n",
    "    num_samples, _ = X.shape\n",
    "\n",
    "    L = np.linalg.norm(X) ** 2 / (4 * num_samples) + lam\n",
    "\n",
    "    L = min(L, 1000 * lam)\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377d566",
   "metadata": {},
   "source": [
    "## __Problem 2. (20 points)__ This part of the assignment is related to momentum and acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a287ec",
   "metadata": {},
   "source": [
    "### __a). (5 points)__ Implement the heavy ball method and Nesterov's accelerated gradient method. \n",
    "\n",
    "Just in case, we give here a variant of the function description for the gradient descent from the first assignment. \n",
    "\n",
    "You can use this format if you wish. Note that ``x_sol`` occurs in the code - this problem should be solved or criteria tied to ``x_sol`` should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def x_difference(x1: np.ndarray, x2: np.ndarray) -> float:\n",
    "    \"\"\"Computes the euclidean norm of the difference vector \n",
    "\n",
    "    Args:\n",
    "        x1 (np.ndarray): first vector\n",
    "        x2 (np.ndarray): second vector\n",
    "\n",
    "    Returns:\n",
    "        float: returns the norm of the difference\n",
    "    \"\"\"\n",
    "    return norm(x1 - x2, ord=2)\n",
    "\n",
    "def f_difference(f1: float, f2: float) -> float:\n",
    "    \"\"\"returns the absolute difference between 2 values \"\"\"\n",
    "    # the expression f_x_k - f_sol is equivalent since the problem is minimization,\n",
    "    # but the 'abs' function was used to make the method general and not only specific to the given problem\n",
    "    return abs(f1 - f2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the modes in terms of strings\n",
    "x_diff = 'x_k+1 - x_k'\n",
    "f_diff = 'f(x_k+1) - f(x_k)'\n",
    "normalized_criterion = 'df_xk / df_x0'\n",
    "\n",
    "\n",
    "def heavy_ball_method(function: callable, \n",
    "                      grad_function: callable, \n",
    "                      x_0: np.ndarray, \n",
    "                      gamma_k: callable,\n",
    "                      momentum_k: callable,  \n",
    "                      K = 10**3, \n",
    "                      eps = 10**-5, \n",
    "                      mode = normalized_criterion, \n",
    "                      return_history: bool = True\n",
    "                      ):\n",
    "    \n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if mode not in [f_diff, x_diff, normalized_criterion]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    # a boolean flag to determine whether to stop the iterative process\n",
    "    criterion_stop = False\n",
    "\n",
    "    # counter for the number of iterations\n",
    "    k = 0\n",
    "\n",
    "    # x_p0: stands for the x{k - 1}, x_p1: x{k - 2}\n",
    "    x_new, x_current, x_previous = x_0, x_0, x_0\n",
    "\n",
    "    # create a list to save the intermediate points  \n",
    "    x_history = [x_current]\n",
    "    criterion_history = [1]\n",
    "\n",
    "    while not criterion_stop:\n",
    "        x_previous = x_current\n",
    "        x_current = x_new\n",
    "        \n",
    "        # compute the step size\n",
    "        gamma = gamma_k(k)\n",
    "        mom = momentum_k(k)\n",
    "        \n",
    "        x_new = x_current - gamma * grad_function(x_current) + mom * (x_current - x_previous) \n",
    "        \n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(x_new), function(x_current))\n",
    "        elif mode == x_diff:\n",
    "            diff = x_difference(x_new, x_current)\n",
    "        else:\n",
    "            diff = norm(grad_function(x_current), p=2) / norm(grad_function(x_0), p=2)\n",
    "\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "        # add the current criterion to history \n",
    "        criterion_history.append(diff)\n",
    "        # increment 'k' \n",
    "        k += 1\n",
    "\n",
    "        # stop the iterative process either because the maximum number of iterations is reached \n",
    "        # or the difference is less than epsilon\n",
    "        criterion_stop = diff <= eps or k >= K\n",
    "    \n",
    "    assert len(x_history) == k + 1\n",
    "\n",
    "    # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return x_history, criterion_history if return_history else x_history[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272252c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naesterov_method(function: callable, \n",
    "                      grad_function: callable, \n",
    "                      x_0: np.ndarray, \n",
    "                      gamma_k: callable,\n",
    "                      momentum_k: callable,  \n",
    "                      K = 10**3, \n",
    "                      eps = 10**-5, \n",
    "                      mode = normalized_criterion, \n",
    "                      return_history: bool = True):\n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if mode not in [f_diff, x_diff, normalized_criterion]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    # a boolean flag to determine whether to stop the iterative process\n",
    "    criterion_stop = False\n",
    "\n",
    "    # counter for the number of iterations\n",
    "    k = 0\n",
    "\n",
    "    # set the variables for the parameters 'x', 'y'\n",
    "    x_current, x_previous, y_current = x_0, x_0, x_0\n",
    "\n",
    "    # create a list to save the intermediate points  \n",
    "    x_history = [x_current]\n",
    "    criterion_history = [1]\n",
    "\n",
    "    while not criterion_stop:\n",
    "        x_previous = x_current\n",
    "        \n",
    "        # compute the step size\n",
    "        gamma = gamma_k(k)\n",
    "        mom = momentum_k(k)\n",
    "        # compute the (k + 1)-th 'x' point\n",
    "        # compute the (k + 1)-th 'y' point\n",
    "\n",
    "        x_current = y_current - gamma * grad_function(y_current)\n",
    "        y_current = x_current - mom * (x_current - x_previous)\n",
    "        \n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(x_current), function(x_current))\n",
    "\n",
    "        elif mode == x_difference:\n",
    "            diff = x_difference(x_current, x_previous)        \n",
    "        else:\n",
    "            diff = norm(grad_function(x_current)) / norm(grad_function(x_0))\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "        # increment 'k' \n",
    "        k += 1\n",
    "\n",
    "        # stop the iterative process either because the maximum number of iterations is reached \n",
    "        # or the difference is less than epsilon\n",
    "        criterion_stop = diff <= eps or k >= K\n",
    "    \n",
    "    assert len(x_history) == k + 1\n",
    "\n",
    "    # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return x_history, criterion_history if return_history else x_history[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db781f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(function: callable, \n",
    "                     grad_function: callable, \n",
    "                     x_0: np.ndarray,                      \n",
    "                     K: int = 10 ** 3,\n",
    "                     eps: float = 10 ** -5, \n",
    "                     mode: str = normalized_criterion,\n",
    "                     gamma_k: callable = None,                      \n",
    "                     return_history: bool = False\n",
    "                     ) -> Union[List[np.ndarray], np.ndarray]:\n",
    "    \"\"\"This function applies the gradient descent algorithm for any given function\n",
    "\n",
    "    Args:\n",
    "        function (callable): a callable object that computes the value of the function at any given point\n",
    "        grad_function (callable): a callable object that computes the value of the function's gradient at any given point\n",
    "        x_0 (np.ndarray): the starting point\n",
    "        x_sol (np.ndarray): The analytical solution of the problem\n",
    "        K (int, optional): the maximum number of iterations. Defaults to 10**3.\n",
    "        eps (float, optional): As soon as the criterion goes below this value, the algorithm stops. Defaults to 10**-5.\n",
    "        mode (str, optional): Which criterion to consider. Defaults to x_opt_diff.\n",
    "        gamma_k (callable, optional): A callable object that determines the step size at the k-th iteration. Defaults to None.\n",
    "        greedy_step_func (callable, optional): _description_. Defaults to None.\n",
    "        polyak_step_func (callable, optional): _description_. Defaults to None.\n",
    "        return_history (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: _description_\n",
    "\n",
    "    Returns:\n",
    "        Union[List[np.ndarray], np.ndarray]: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if mode not in [f_diff, x_diff, normalized_criterion]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "\n",
    "\n",
    "    # a boolean flag to determine whether to stop the iterative process\n",
    "    criterion_stop = False\n",
    "\n",
    "    # counter for the number of iterations\n",
    "    k = 0\n",
    "    x_current, x_previous = x_0, x_0\n",
    "\n",
    "    # create a list to save the intermediate points  \n",
    "    x_history = [x_current]\n",
    "\n",
    "    while not criterion_stop:\n",
    "        # save the k-th point\n",
    "        x_previous = x_current\n",
    "\n",
    "        gamma = gamma_k(k)\n",
    "\n",
    "        # compute the (k+1)-th point\n",
    "        x_current = x_current - gamma * grad_function(x_current)  \n",
    "\n",
    "        # compute the criterion\n",
    "        if mode == x_diff:\n",
    "            diff = x_difference(x_previous, x_current)\n",
    "        elif mode == f_diff:\n",
    "            diff = f_difference(function(x_previous), function(x_current))\n",
    "        else:\n",
    "            diff = norm(grad_function(x_current)) / norm(grad_function(x_0))\n",
    "\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "\n",
    "        # increment 'k' \n",
    "        k += 1\n",
    "\n",
    "        # stop the iterative process either because the maximum number of iterations is reached \n",
    "        # or the difference is less than epsilon\n",
    "        criterion_stop = diff <= eps or k >= K\n",
    "    \n",
    "    assert len(x_history) == k + 1\n",
    "\n",
    "    # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return x_history if return_history else x_history[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d708445",
   "metadata": {},
   "source": [
    "### __b). (15 points)__ Solve an optimization problem on a train sample using two implemented methods. \n",
    "\n",
    "1, Fix a step $\\frac{1}{L}$ and search different values of momentum from $-1$ to $1$. \n",
    "\n",
    "2. Check also the momentum values equal to $\\frac{k}{k+3}$, $\\frac{k}{k+2}$, $\\frac{k}{k+1}$ ($k$ is the iteration number), and also $\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}$. \n",
    "\n",
    "3. You can choose your own starting point and convergence criterion; \n",
    "\n",
    "4. we recommend using the normalized version of the criterion, for example, $\\frac{\\| \\nabla f(x^k) \\|}{\\| \\nabla f(x^0) \\|}$, as well as using the same starting point and convergence criterion in Problems 3-4.\n",
    "\n",
    "In this task, you should draw three plots: \n",
    "\n",
    "1) the convergence criterion values from iteration number for the heavy ball method with different values of momentum, \n",
    "\n",
    "2) the convergence criterion values from iteration number for the accelerated gradient method with different values of momentum, and \n",
    "\n",
    "3) the convergence criterion values from iteration number for the two methods with the best choice of momentum for each, as well as the gradient descent.\n",
    "\n",
    "Remember to make conclusions and comment on the results. \n",
    "\n",
    "For example, reflect on whether convergence is always monotone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a885525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with setting up the different paramters needs \n",
    "\n",
    "# the regualization parameter\n",
    "REG = 10 ** -3\n",
    "# a callable object with the main loss\n",
    "p1_function = lambda x: p1_value_function(X=X_train, y=y_train, w = x, lam = REG)\n",
    "# callable object for the gradient function\n",
    "p1_grad = lambda x: p1_gradient_function(X=X_train, y=y_train, w = x, lam = REG)\n",
    "\n",
    "p1_H = lambda x: p1_hessian_function(X=X_train, y=y_train, w=x, lam=REG)\n",
    "# the estimation of 'L'\n",
    "L = p1_L_estimation(X=X_train, y=y_train, w=np.random.rand(X_train.shape[1], 1), lam=REG)\n",
    "\n",
    "# a callable object to return 1 / L as a constant step size\n",
    "gamma_k = lambda _ : 1 / L\n",
    "\n",
    "# create different callable objects for momentum\n",
    "m1 = lambda k: k / (k + 1)\n",
    "m2 = lambda k: k / (k + 2)\n",
    "m3 = lambda k: k / (k + 3)\n",
    "\n",
    "mLU = lambda _: (np.sqrt(L) - np.sqrt(REG)) / (np.sqrt(L) + np.sqrt(REG))\n",
    "\n",
    "momentum_functions = [m1, m2, m3, mLU]\n",
    "\n",
    "ms = np.linspace(start=-1, stop=1, num=8)\n",
    "print(ms)\n",
    "momentum_functions.extend([lambda _ : m for m in ms])\n",
    "\n",
    "# create the labels for each momentum step\n",
    "momentum_labels = [\"k / (k + 1)\", \"k / (k + 2)\", \"k / (k + 3)\", \"(sqrt(L) - sqrt(mu)) / (sqrt(L) + sqrt(mu))\"]\n",
    "# add the labels for the constant momentum steps\n",
    "momentum_labels.extend([str(round(k, 4)) for k in ms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# create the set up: x_0 and seed\n",
    "def set_up(seed: int = 69) -> np.ndarray:\n",
    "        # changing the seed mainly changes the starting point\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    w_0 = np.random.randn(X_train.shape[1], 1)\n",
    "    return w_0\n",
    "\n",
    "def simulation( optimization_algorithm: callable,\n",
    "                function: callable, \n",
    "                grad_function: callable, \n",
    "                gamma_k: callable=None,\n",
    "                momentum_k: callable=None,  \n",
    "                K = 10**3, \n",
    "                eps = 10**-5, \n",
    "                mode = normalized_criterion, \n",
    "                seed: int = 69\n",
    "               ):\n",
    "    \n",
    "    \n",
    "    x_0 = set_up(seed=seed)\n",
    "    \n",
    "    # get the points from the gradient descent\n",
    "    if momentum_k is not None:\n",
    "        x_points, criterions = optimization_algorithm(function=function, \n",
    "                                    grad_function=grad_function,  \n",
    "                                    x_0=x_0,\n",
    "                                    gamma_k=gamma_k,\n",
    "                                    momentum_k=momentum_k,\n",
    "                                    K = K,\n",
    "                                    eps=eps,\n",
    "                                    mode=mode, \n",
    "                                    return_history=True, \n",
    "                                    )\n",
    "        return x_points, criterions\n",
    "\n",
    "    x_points, criterions = optimization_algorithm(\n",
    "                                function=function, \n",
    "                                grad_function=grad_function,  \n",
    "                                x_0=x_0,\n",
    "                                gamma_k=gamma_k,\n",
    "                                K = K,\n",
    "                                eps=eps,\n",
    "                                mode=mode, \n",
    "                                return_history=True, \n",
    "                                )\n",
    "\n",
    "    return x_points, criterions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "def plot_iterations(criterions: List[float],\n",
    "                    start_index: int = 0, \n",
    "                    end_index: int = -1,\n",
    "                    plot_label: str = None,\n",
    "                    x_label: str = None,\n",
    "                    y_label: str = None,\n",
    "                    show:bool = True,\n",
    "                    ):\n",
    "    \n",
    "    end_index = (end_index + len(criterions)) % len(criterions)\n",
    "\n",
    "    if plot_label is None:\n",
    "        plt.plot(list(range(start_index, end_index)), criterions[start_index:end_index])\n",
    "    else:\n",
    "        plt.plot(list(range(start_index, end_index)), criterions[start_index:end_index], label=str(plot_label))\n",
    "    \n",
    "\n",
    "    plt.xlabel('iteration' if x_label is None else x_label)\n",
    "    plt.ylabel('criterion (log_{10} scale)' if x_label is None else y_label)\n",
    "    \n",
    "    if show:\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f42772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 20))\n",
    "# # let's start with plotting the results for the heavy_ball method\n",
    "# # make sure to save the best performing result\n",
    "\n",
    "# best_hb_mom = None\n",
    "# min_value_function = float('inf')\n",
    "\n",
    "# for mom_k, mom_k_label in zip(momentum_functions, momentum_labels):\n",
    "#     # run the simulation \n",
    "#     x_points, criterions = simulation(optimization_algorithm=heavy_ball_method, \n",
    "#                                       function=function,\n",
    "#                                       grad_function=grad,\n",
    "#                                       gamma_k=gamma_k, \n",
    "#                                       momentum_k=mom_k)\n",
    "    \n",
    "#     criterions = [np.log10(c) for c in criterions]\n",
    "\n",
    "#     value_last_point = function(x_points[-1])\n",
    "#     min_value_function = min(value_last_point, min_value_function)    \n",
    "#     if min_value_function < value_last_point:\n",
    "#         best_hb_mom = mom_k\n",
    "\n",
    "#     plot_iterations(criterions=criterions, \n",
    "#                     x_label='iteration', \n",
    "#                     y_label='||df(x_k)|| / ||df(x_0||)', \n",
    "#                     plot_label= f'k = {mom_k_label}',\n",
    "#                     show=False)\n",
    "    \n",
    "# plt.legend()\n",
    "# plt.title(\"Heavy Ball Convergence for different momentum steps\")\n",
    "# plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's display the results for the nesterov method\n",
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# # let's start with plotting the results for the heavy_ball method\n",
    "# # make sure to save the best performing result\n",
    "\n",
    "# best_nest_mom = None\n",
    "# min_value_function = float('inf')\n",
    "\n",
    "# for mom_k, mom_k_label in zip(momentum_functions, momentum_labels):\n",
    "#     # run the simulation \n",
    "#     x_points, criterions = simulation(optimization_algorithm=naesterov_method, \n",
    "#                                       function=function,\n",
    "#                                       grad_function=grad,\n",
    "#                                       gamma_k=gamma_k, \n",
    "#                                       momentum_k=mom_k)\n",
    "    \n",
    "#     criterions = [np.log10(c) for c in criterions]\n",
    "\n",
    "#     value_last_point = function(x_points[-1])\n",
    "#     min_value_function = min(value_last_point, min_value_function)    \n",
    "#     if min_value_function < value_last_point:\n",
    "#         best_nest_mom = mom_k\n",
    "\n",
    "#     plot_iterations(criterions=criterions, \n",
    "#                     x_label='iteration', \n",
    "#                     y_label='||df(x_k)|| / ||df(x_0||): log_{10}', \n",
    "#                     plot_label= f'k = {mom_k_label}',\n",
    "#                     show=False)\n",
    "    \n",
    "# plt.legend()\n",
    "# plt.title(\"Nesterov Convergence for different momentum steps\")\n",
    "# plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19349804",
   "metadata": {},
   "source": [
    "## __Problem 3. (30 points)__ \n",
    "In this part, we work with the conjugate gradient method. \n",
    "\n",
    "## __а). (5 points)__ Realize the Fletcher-Reeves and Polak-Ribier method. \n",
    "\n",
    "Describe how you will search for $\\alpha_k$ steps (both the algorithm and its initialization are interesting). \n",
    "\n",
    "Add to the algorithms the ability to do \"restarts\" (sometimes take $\\beta_k = 0$) with some frequency that can be customized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f0a52",
   "metadata": {},
   "source": [
    "__b). (10 points)__ Solve the optimization problem with two implemented methods, varying the frequency of \"restarts\" for each: $1$ (every iteration $\\beta_k = 0$), $10$ (every tenth iteration $\\beta_k = 0$), $100$, $1000$, no restarts.\n",
    "\n",
    "Draw three plots: 1) the convergence criterion values from iteration number for the Fletcher-Reeves method with different restart frequencies, 2) the convergence criterion values from iteration number for the Polak-Ribier method with different restart frequencies, 3) the convergence criterion values from iteration number for both methods with the best choice of restart frequency. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efddfc99",
   "metadata": {},
   "source": [
    "__c). (15 points)__ In this part we abstract from the regression problem. Let us study the properties of the conjugate gradient method for the quadratic problem:\n",
    "$$\\min_{x \\in \\mathbb{R}^d} \\left[\\tfrac{1}{2} x^T A x - b x \\right]$$\n",
    "with a positive definite symmetric matrix $A \\in \\mathbb{R}^{d \\times d}$ and some vector $b \\in \\mathbb{R}^d$. We need to learn how to generate a matrix $A$ with the possibility to specify its spectrum (eigenvalues). The previous assignment already asked us to do this. We suggest the following approach based on the decomposition $A = Q D Q^T$, where $D$ is a diagonal matrix formed from eigenvalues and $Q$ is orthogonal (it can be generated using the $QR$-decomposition of a random matrix).\n",
    "\n",
    "Suppose we have a quadratic problem whose matrix $A \\in \\mathbb{R}^{d \\times d}$ has clustered eigenvalues, meaning, that there exists some number of clusters $k \\leq d$ and values $\\tilde \\lambda_1 < \\ldots < \\tilde \\lambda_k$ such that for any $\\lambda_i$ eigenvalue of matrix $A$ there exists $j \\leq k$ such that $\\lambda_i \\in [(1 - p) \\tilde \\lambda_j; (1 + p) \\tilde \\lambda_j]$, where $p < 1$.\n",
    "\n",
    "Then we will need to generate clustered eigenvalues and then the matrix $A$. When generating the spectrum, try to make sure that all values in it are different. As a convergence criterion, use $\\frac{\\| x^k - x^* \\|^2_A} {\\| x^0 - x^* \\|^2_A}$, where $k$ is the iteration number and $\\| x \\|^2_A = \\langle x, Ax \\rangle$. \n",
    "\n",
    "Let us test the performance of the conjugate gradient method for different variants of eigenvalue clustering:\n",
    "\n",
    "1) Let $d = 100$, $k = 2$, $p = 0.05$, $\\tilde \\lambda_1 = 1$, there are 50 eigenvalues each in the clusters for $\\tilde \\lambda_1$ and $\\tilde \\lambda_2$. Vary the value of $\\tilde \\lambda_2$ from $10$ to $10^5$ (5 different values is enough). Plot the values of the convergence criterion from the iteration number for each value of $\\tilde \\lambda_2$ on one plot. Make a conclusion.\n",
    "\n",
    "2) Let $d = 100$, $k = 2$, $p = 0.05$, $\\tilde \\lambda_1 = 1$, $\\tilde \\lambda_2 = 1000$. Vary the number of eigenvalues in each cluster from $1$ to $99$ (5 different values is enough). Plot the values of the convergence criterion from the iteration number for each value of cluster size for $\\tilde \\lambda_1$ on one plot. Make a conclusion.\n",
    "\n",
    "3) Let $d = 100$, $p = 0.05$, $\\tilde \\lambda_1 = 1$, $\\tilde \\lambda_k = 1000$. Vary the number of clusters $k$ from 2 to 100 (5 different values is enough, include 100 - corresponds to a uniform distribution of eigenvalues). Plot the values of the convergence criterion from the number of iterations for each value of $k$ on one plot. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code и Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a4256",
   "metadata": {},
   "source": [
    "## __Problem 4. (25 points)__ Now let us talk about Newton method and quasi-Newton methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc437ecf",
   "metadata": {},
   "source": [
    "### __а). (5 points)__ \n",
    "\n",
    "For the regression problem, implement and run Newton method. Does it converge? \n",
    "\n",
    "If not, try running the gradient descent method for several iterations first before using Newton method. \n",
    "\n",
    "Vary the number of gradient descent steps. \n",
    "\n",
    "Plot the value of convergence criterion versus iteration number for the combination of gradient descent and Newton method with different number of gradient descent steps. \n",
    "\n",
    "Make a conclusion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dce8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method( function: callable, \n",
    "                    grad_function: callable, \n",
    "                    hessian_function: callable,\n",
    "                    x_0: np.ndarray, \n",
    "                    gamma_k: callable = None,\n",
    "                    K = 10**3, \n",
    "                    eps = 10**-5, \n",
    "                    mode = normalized_criterion, \n",
    "                    return_history: bool = True):\n",
    "\n",
    "    # the default step size is '1'\n",
    "    gamma_k = lambda _ : 1\n",
    "\n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if mode not in [f_diff, x_diff, normalized_criterion]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    # a boolean flag to determine whether to stop the iterative process\n",
    "    criterion_stop = False\n",
    "\n",
    "    # counter for the number of iterations\n",
    "    k = 0\n",
    "\n",
    "    # set the variables for the parameters 'x', 'y'\n",
    "    x_current, x_previous = x_0, x_0\n",
    "\n",
    "    # create a list to save the intermediate points  \n",
    "    x_history = [x_current]\n",
    "    criterion_history = [(1 if mode==normalized_criterion else 0)]\n",
    "\n",
    "    while not criterion_stop:\n",
    "        x_previous = x_current\n",
    "\n",
    "        gamma = gamma_k(k)\n",
    "        \n",
    "        current_hessian = hessian_function(x_current)\n",
    "        # if the current hessian contains 'nan' values\n",
    "        if np.sum(np.isnan(current_hessian)) > 0:\n",
    "            raise ValueError(f\"the hessian contains nan values\")\n",
    "        \n",
    "        # carry out with the inverse operation\n",
    "        inv_hessian = np.linalg.pinv(current_hessian)\n",
    "        if np.sum(np.isnan(inv_hessian)):\n",
    "            raise ValueError(f\"The inverse of the hessian matrix conains \")\n",
    "\n",
    "        # if np.sum(np.isnan(inv_hessian)) > 0 or np.sum(np.isnan(x_current)) or np.sum(np.isnan(current_hessian)):\n",
    "        #     raise ValueError(f\"one of the variables has 'nan' values.\")\n",
    "\n",
    "        # make sure to use @: the matrix multiplication operator and not '*', the element-wise multiplication\n",
    "        x_current =  x_current - gamma * np.matmul(inv_hessian , grad_function(x_current))\n",
    "\n",
    "        if mode == f_diff:\n",
    "            diff = f_difference(function(x_current), function(x_current))\n",
    "\n",
    "        elif mode == x_difference:\n",
    "            diff = x_difference(x_current, x_previous)\n",
    "        \n",
    "        else:\n",
    "            diff = norm(grad_function(x_current), p=2) / norm(grad_function(x_0), p=2)\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "        # increment 'k' \n",
    "        k += 1\n",
    "\n",
    "        # stop the iterative process either because the maximum number of iterations is reached \n",
    "        # or the difference is less than epsilon\n",
    "        criterion_stop = diff <= eps or k >= K\n",
    "    assert len(x_history) == k + 1\n",
    "\n",
    "    # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return x_history, criterion_history if return_history else x_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "x_0 = set_up(seed=69)\n",
    "\n",
    "# run the simple newton method\n",
    "x_points, criterions = newton_method(function=p1_function,\n",
    "                                     grad_function=p1_grad, \n",
    "                                     hessian_function=p1_H,\n",
    "                                     x_0=x_0, \n",
    "                                     mode=normalized_criterion, \n",
    "                                     K=100)\n",
    "\n",
    "criterions = [np.log10(c) for c in criterions]\n",
    "\n",
    "plot_iterations(criterions=criterions, \n",
    "                x_label='iteration', \n",
    "                y_label=f'{normalized_criterion}: log_{10}', \n",
    "                show=False)\n",
    "\n",
    "plt.title(f\"The convergence of Newton method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaed44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newton + gradient descent warm-ups !! \n",
    "def newton_gd(function: callable, \n",
    "            grad_function: callable, \n",
    "            hessian_function: callable,\n",
    "            x_0: np.ndarray, \n",
    "            gamma_gd: callable,\n",
    "            gamma_newton: callable = None,\n",
    "            gd_k=10 ** 2,\n",
    "            newton_k = 10**3, \n",
    "            eps = 10**-5, \n",
    "            mode = normalized_criterion, \n",
    "            return_history: bool = True):\n",
    "    \n",
    "        x0_newton = gradient_descent(function=function, \n",
    "                                grad_function=grad_function,\n",
    "                                x_0=x_0,\n",
    "                                gamma_k=gamma_gd,\n",
    "                                K=gd_k,\n",
    "                                eps=eps,\n",
    "                                return_history=False, \n",
    "                                mode=mode \n",
    "                                )\n",
    "        \n",
    "        # make sure to use the last value reached by the gradient descent algorithm\n",
    "        return newton_method(function=function, \n",
    "                             grad_function=grad_function, \n",
    "                             hessian_function=hessian_function, \n",
    "                             x_0=x0_newton,\n",
    "                             gamma_k=gamma_newton, \n",
    "                             K=newton_k,\n",
    "                             eps=eps, \n",
    "                             return_history=return_history\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = set_up(seed=69)\n",
    "\n",
    "for n in np.logspace(start=1, stop=3, num=2, base=10): \n",
    "    n = int(n)\n",
    "    x_points, criterions = newton_gd(function=p1_function,\n",
    "                                     grad_function=p1_grad, \n",
    "                                     hessian_function=p1_H,\n",
    "                                     x_0=x_0, \n",
    "                                     gamma_gd=lambda _ : 1 / L,\n",
    "                                     gd_k=n)\n",
    "\n",
    "    criterions = [np.log10(c) for c in criterions]\n",
    "\n",
    "    plot_iterations(criterions=criterions, \n",
    "                    plot_label=f'n = {np.log10(n)}',\n",
    "                    x_label='iteration', \n",
    "                    y_label='||df(x_k)|| / ||df(x_0||): log_{10}', \n",
    "                    show=False)\n",
    "    \n",
    "# set the labels, title and show the plots\n",
    "plt.legend()\n",
    "plt.title(f\"The convergence of Newton method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4941f7",
   "metadata": {},
   "source": [
    "### __b). (7 points)__ \n",
    "Implement the quasi-Newton method BFGS. \n",
    "\n",
    "Use it to solve the regression problem. \n",
    "\n",
    "Add it to the plot from the previous point about Newton method. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0dac25",
   "metadata": {},
   "source": [
    "The implementation below is mainly based on the algorithm description from this [link](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "\n",
    "The linear search for the step size satisfies the wolph conditions  mentioned [here](https://en.wikipedia.org/wiki/Wolfe_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83185040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(grad_function: callable, \n",
    "            x_k: np.ndarray, \n",
    "            p_k: np.ndarray):\n",
    "    return grad_function(x_k).T @ p_k\n",
    "\n",
    "\n",
    "def wolfe_conditions(function: callable, \n",
    "                     grad_function: callable,\n",
    "                     x_k: np.ndarray, \n",
    "                     p_k: np.ndarray, \n",
    "                     alpha: float, \n",
    "                     c1: float, \n",
    "                     c2: float, \n",
    "                     ):\n",
    "    next_x = x_k + alpha * p_k\n",
    "\n",
    "    d = descent(grad_function=grad_function, \n",
    "                x_k=x_k, \n",
    "                p_k=p_k,)\n",
    "\n",
    "    term1 = function(x_k) + c1 * alpha * d \n",
    "    if term1 > function(next_x):\n",
    "        return False\n",
    "\n",
    "    # time to check the 2nd condition\n",
    "    return descent(grad_function=grad_function, x_k=next_x, p_k=p_k) >= c2 * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10804205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_size_search(function: callable, \n",
    "                     grad_function: callable,\n",
    "                     x_k: np.ndarray, \n",
    "                     p_k: np.ndarray, \n",
    "                     c1: float = 10 ** -4,\n",
    "                     c2: float = 0.9, \n",
    "                     search_coef: float = 0.8,\n",
    "                     initial_alpha: float = 1,\n",
    "                     max_iterations: int = 10) -> float:\n",
    "    \n",
    "    alpha = initial_alpha    \n",
    "    good_alpha = False    \n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        good_alpha = wolfe_conditions(function=function, \n",
    "                                      grad_function=grad_function, \n",
    "                                      x_k=x_k,\n",
    "                                      p_k=p_k,\n",
    "                                      c1=c1,\n",
    "                                      c2=c2, \n",
    "                                      alpha=alpha)\n",
    "        if good_alpha: \n",
    "            print(f\"found a sufficiently good step size: {alpha}\")\n",
    "            break\n",
    "\n",
    "        alpha = search_coef * alpha\n",
    "         \n",
    "    return alpha    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(function: callable,\n",
    "         grad_function: callable,\n",
    "         x_0: np.ndarray,\n",
    "         B_0: np.ndarray = None,\n",
    "         eps: int = 10 ** -5,\n",
    "         K: int = 10 ** 3,\n",
    "         mode = normalized_criterion,\n",
    "         return_history:bool=True,\n",
    "\n",
    "         c1: float = 10 ** -4, \n",
    "         c2: float = 0.9,\n",
    "         search_coef: float = 0.8, \n",
    "         initial_step_size: float = 1, \n",
    "         max_search_iterations: int = 10):\n",
    "    \n",
    "     # the default value for the first B matrix is the identity matrix \n",
    "     B_0 = B_0 if B_0 is not None else np.eye(max(x_0.shape))\n",
    "    \n",
    "     # the first step is to make sure the 'mode' variable is defined above\n",
    "     if mode not in [f_diff, x_diff, normalized_criterion]:\n",
    "          raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion]}\"\n",
    "          f\"Found: {mode}\"))\n",
    "    \n",
    "     # a boolean flag to determine whether to stop the iterative process\n",
    "     criterion_stop = False\n",
    "\n",
    "     # set the variables for the parameters 'x', 'y'\n",
    "     x_current, x_previous = x_0, x_0\n",
    "     B_k = B_0\n",
    "\n",
    "     # create a list to save the intermediate points  \n",
    "     x_history = [x_current]\n",
    "     criterion_history = [(1 if mode==normalized_criterion else 0)]\n",
    "\n",
    "     while not criterion_stop:\n",
    "          x_grad = grad_function(x_current)\n",
    "          # define p_k as - grad * B\n",
    "          p_k = - B_k @ x_grad\n",
    "          # find the step\n",
    "          alpha_k = step_size_search(function=function,\n",
    "                                   grad_function=grad_function,\n",
    "                                   x_k=x_current, \n",
    "                                   p_k=p_k,\n",
    "                                   c1=c1, \n",
    "                                   c2=c2,\n",
    "                                   search_coef=search_coef, \n",
    "                                   initial_alpha=initial_step_size,\n",
    "                                   max_iterations=max_search_iterations,\n",
    "                                   )\n",
    "          \n",
    "          s_k = alpha_k * p_k \n",
    "          \n",
    "          x_current = x_current + s_k\n",
    "          \n",
    "          y_k = grad_function(x_current) - grad_function(x_previous)\n",
    "\n",
    "          sy = (s_k.T @ y_k).item()\n",
    "\n",
    "          # time to update the inverse of the matrix B_K\n",
    "          # the latter has a closed formula  \n",
    "          B_k = B_k + (sy + y_k.T @ B_k @ y_k) / (sy ** 2) - (B_k @ y_k @ s_k.T + s_k @ y_k.T @ B_k) / (sy)\n",
    "\n",
    "          if mode == f_diff:\n",
    "               diff = f_difference(function(x_current), function(x_current))\n",
    "          elif mode == x_difference:\n",
    "               diff = x_difference(x_current, x_previous)\n",
    "          else:\n",
    "            diff = norm(grad_function(x_current), p=2) / norm(grad_function(x_0), p=2)\n",
    "\n",
    "          criterion_history.append(diff)\n",
    "          # add 'x_current' to the history\n",
    "          x_history.append(x_current)\n",
    "          # increment 'k' \n",
    "          k += 1\n",
    "\n",
    "          # stop the iterative process either because the maximum number of iterations is reached \n",
    "          # or the difference is less than epsilon\n",
    "          criterion_stop = diff <= eps or k >= K\n",
    "\n",
    "     assert len(x_history) == k + 1\n",
    "\n",
    "     # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "     # and only the last point otherwise    \n",
    "     return x_history, criterion_history if return_history else x_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4230c3",
   "metadata": {},
   "source": [
    "### __c). (13 points)__ Let us again depart from regression and consider a one-dimensional minimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{x \\in \\mathbb{R}} f(x) = x \\arctan x - \\frac{1}{2} \\log (1 + x^2).\n",
    "\\end{equation}\n",
    "\n",
    "Solve this problem using Newton method. \n",
    "\n",
    "Draw convergence plots of the method for two different starting points $x^0 = 1.3$ and $x^0 = 1.5$. \n",
    "\n",
    "Make a conclusion.\n",
    "\n",
    "To achieve convergence of Newton method it is not necessary to resort to using another method as a stratum method. \n",
    "\n",
    "Realize two modifications of Newton method: damped (adding a step) and cubic Newton method (see [paper](https://link.springer.com/article/10.1007/s10107-006-0706-8)). \n",
    "\n",
    "Do these methods solve the convergence problem of Newton method for the starting point $x^0 = 1.5$? \n",
    "\n",
    "In the damped method, try taking a step from $0.5$ to $1$. \n",
    "\n",
    "Draw the convergence plots. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42862c",
   "metadata": {},
   "source": [
    "Let's start by computing the derivative and 2nd derivative of the problem's function: \n",
    "\n",
    "we have\n",
    "\n",
    "$$ f^{'} = \\arctan(x)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$ f^{''} = \\frac{1}{1 + x^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_newton_method(function: callable, \n",
    "                         derivative: callable, \n",
    "                         derivative_2: callable, \n",
    "                         x_0: float, \n",
    "                         gamma_k: callable=None,\n",
    "                         K: int = 10 ** 3,\n",
    "                         eps: float = 10  ** -5,\n",
    "                         mode: str = normalized_criterion,\n",
    "                         return_history: bool = True \n",
    "                         ):\n",
    "    \n",
    "    # the default step size is '1'\n",
    "    gamma_k = lambda _ : 1\n",
    "\n",
    "    # the first step is to make sure the 'mode' variable is defined above\n",
    "    if mode not in [f_diff, x_diff, normalized_criterion]:\n",
    "        raise ValueError((f\"the variable 'mode' is expected to belong to {[f_diff, x_diff, normalized_criterion]}\"\n",
    "                          f\"Found: {mode}\"))\n",
    "    \n",
    "    # a boolean flag to determine whether to stop the iterative process\n",
    "    criterion_stop = False\n",
    "    # counter for the number of iterations\n",
    "    k = 0\n",
    "    # set the variables for the parameters 'x', 'y'\n",
    "    x_current, x_previous = x_0, x_0\n",
    "\n",
    "    # create a list to save the intermediate points  \n",
    "    x_history = [x_current]\n",
    "    criterion_history = [(1 if mode==normalized_criterion else 0)]\n",
    "\n",
    "    while not criterion_stop:\n",
    "        x_previous = x_current\n",
    "        gamma = gamma_k(k)\n",
    "\n",
    "        current_d1 = derivative(x_current)\n",
    "        current_d2 = derivative_2(x_current)\n",
    "\n",
    "        x_current = x_current - gamma * current_d1 / current_d2 \n",
    "\n",
    "\n",
    "        if mode == f_diff:\n",
    "            diff = abs(function(x_current), function(x_current))\n",
    "\n",
    "        elif mode == x_difference:\n",
    "            diff = abs(x_current, x_previous)\n",
    "        else:\n",
    "            diff = abs(derivative(x_current) / derivative(x_0))\n",
    "\n",
    "        criterion_history.append(diff)\n",
    "        # add 'x_current' to the history\n",
    "        x_history.append(x_current)\n",
    "        # increment 'k' \n",
    "        k += 1\n",
    "\n",
    "        # stop the iterative process either because the maximum number of iterations is reached \n",
    "        # or the difference is less than epsilon\n",
    "        criterion_stop = diff <= eps or k >= K\n",
    "    \n",
    "    assert len(x_history) == k + 1\n",
    "    # the function will all intermediate points if the 'return_history' argument is set to True\n",
    "    # and only the last point otherwise    \n",
    "    return x_history, criterion_history if return_history else x_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4abd18",
   "metadata": {},
   "source": [
    "The newton Cubic method is designed for function $f: \\mathbb{R}^n \\rightarrow\\mathbb{R}$. Let's write the uni dimensional version of the algorithm.\n",
    "\n",
    "0. $x_0, x_1 \\in \\mathbb{R}$ where $x_0 \\neq x_1$, $\\sigma_1 > 0$, $\\theta > 0$, $k_B = \\frac{\\sigma_1}{6}$, $\\hat{y} = \\max(1, y)$, $t=0$\n",
    "1. Find the smallest intereger $i, 2^{i} \\cdot \\sigma_t \\geq 2 \\cdot \\sigma_1$\n",
    "    \n",
    "    1. compute $h_i = \\frac{2 k_B \\min(|x_t - x_{t - 1}|, y \\cdot |  f^{'}(x_t)|)}{(2^i \\cdot \\sigma_t)}$, \n",
    "    \n",
    "    $A_{t, i} = \\frac{f^{'}(x_t + h_i) - f^{'}(x_t)}{h_i}$ and  $B_{t,i} = A_{t, i}$\n",
    "    \n",
    "    2. consider \n",
    "        $$ M_{x_t, 2^i \\sigma_t}(y) = f(x_t) + f^{'}(x_t) \\cdot (y - x_t) + \\frac{1}{2} \\cdot B_{t, i} \\cdot (y - x_t)^2 + \\frac{2^i \\cdot \\sigma_t |y - x_t| ^ 3}{6}$$\n",
    "\n",
    "        compute an approxiate soution for: $\\min_{y \\in \\mathbb{R}^n}M_{x_t, 2^i \\sigma_t}(y)$\n",
    "\n",
    "    3. if $f(x_t) - f(x_{t, i}^+) \\geq \\frac{2^i \\cdot \\sigma_t}{12} |x_{t, i}^+ - x_t|^3 - \\frac{\\sigma_1}{12} |x_t - x_{t - 1}| ^ 3$ \n",
    "    \n",
    "    and \n",
    "    \n",
    "    $|f^{'} (x_{t, i}^+)| \\leq (2^i \\sigma_t) \\cdot \\max({|x_{t, i}^+ - x_t|, \\min({|x_t - x_{t - 1}|, \\hat{y} |f^{'} (x_t)|})}) ^ 2$ \n",
    "    \n",
    "    then $i_t = i$ and go to step 2. Otherwise go to step $1.1$\n",
    "\n",
    "2. $x_{t+1} = x^+_{t, i_t}$, $\\sigma_{t+1} = 2^{i_t - 1}$, $t = t + 1$ and go to step 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by building the different components of the algorithm\n",
    "import random\n",
    "import math\n",
    "\n",
    "def set_up(seed=69):\n",
    "    # the seed will determine the initial values of the algorithm parameters\n",
    "    random.seed(seed)\n",
    "    np.random.seed(random)\n",
    "    #let's start with x_0, x_1\n",
    "    x0 = random.random()\n",
    "    x1 = x_0 + 0.5\n",
    "    sigma_0 = random.random()\n",
    "    theta = random.random()\n",
    "    kb = sigma_0 / 6\n",
    "    y = max(1, 2 * random.random())\n",
    "\n",
    "    return x0, x1, sigma_0, theta, kb, y, \n",
    "\n",
    "def step1(sigma_0: float, sigma_t):\n",
    "    # the idea is to find the smallest 'i' such that 2 ** i * sigma_t >= 2 sigma_0\n",
    "    return int(math.ceil(np.log2(sigma_0 / sigma_t))) + 1\n",
    "\n",
    "def step1_1(x_current: float, \n",
    "            x_previous: float, \n",
    "            derivative: callable,\n",
    "            kb: float, \n",
    "            y: float,\n",
    "            i: int, \n",
    "            sigma: float) -> Tuple[float, float]:\n",
    "    \n",
    "    hi =  (2 * kb * min(abs(x_current - x_previous), y - derivative(x_current))) / (2 ** i * sigma)\n",
    "    B_ti = (derivative(x_current + hi) - derivative(hi)) / hi \n",
    "\n",
    "    return hi, B_ti\n",
    "\n",
    "\n",
    "def function_model(function: callable,\n",
    "                derivative: callable,\n",
    "                x_current:float,\n",
    "                i: int, \n",
    "                sigma: float, \n",
    "                bti: float,\n",
    "                ) -> callable:\n",
    "    return (lambda y:\n",
    "            function(x_current) + derivative(x_current) * (y - x_current) + 0.5 * bti * (y - x_current) ** 2 + (2 ** i * sigma * (y - x_current) ** 3) / (6)) \n",
    "\n",
    "\n",
    "def derivative_function_model(function: callable,\n",
    "                derivative: callable,\n",
    "                x_current:float,\n",
    "                i: int, \n",
    "                sigma: float, \n",
    "                bti: float,\n",
    "                ) -> callable:\n",
    "    return (lambda y: derivative(x_current) + bti * (y - x_current) + (2 ** i * sigma * (y - x_current) ** 2) / (2))\n",
    "    \n",
    "\n",
    "def approx_stop_condition(x_approx_sol: float, \n",
    "                          x_current: float,\n",
    "\n",
    "                          function: callable, \n",
    "                          derivative: callable, \n",
    "                          model: callable, \n",
    "                          model_derivative: callable,\n",
    "                          theta: float):\n",
    "    # the first condition is to have the model_function at the approximate solution, less than function(x_current)\n",
    "    c1 = model(x_approx_sol) <= function(x_current)\n",
    "    if not c1:\n",
    "        print(f\"First condition failed for {x_approx_sol}\")\n",
    "        return False\n",
    "\n",
    "    # the 2nd condition is to have the derivative of the model function at M to be less than\n",
    "    # x_approx - x_t squared and less than derivative of 'f' at x_current\n",
    "    return model_derivative(x_approx_sol) <= theta * min(abs(x_approx_sol - x_current) ** 2, derivative(x_current)) \n",
    "    \n",
    "def step_1_2(x_current: float, \n",
    "            x_previous: float, \n",
    "            function: callable,\n",
    "            derivative: callable,\n",
    "            kb: float, \n",
    "            y: float,\n",
    "            i: int, \n",
    "            bti: float,\n",
    "            sigma: float, \n",
    "            theta: float):\n",
    "    \n",
    "    # create the callable function for the model\n",
    "    model = function_model(function=function, derivative=derivative, x_current=x_current, i=i, sigma=sigma, bti=bti)\n",
    "    # create the callable function for the derivative of the model function \n",
    "    d_model = derivative_function_model(function=function,derivative=derivative, x_current=x_current, i=i, sigma=sigma)\n",
    "    \n",
    "    good_approx_sol = False\n",
    "    while not good_approx_sol:\n",
    "        # pass it to the gradient descent function\n",
    "        xti_plus = gradient_descent_scalar(function=model, derivative=d_model, K=10, x0=x_current, return_history=True)\n",
    "\n",
    "        # iterate through the different values, if one of them satisfies the conditions use it\n",
    "        for x in xti_plus:\n",
    "            if approx_stop_condition(x_approx_sol=x, x_current=x_current, function=function, derivative=derivative, model=model, model_derivative=d_model, theta=theta)\n",
    "                good_approx_sol = True\n",
    "                xti_plus = X\n",
    "                break\n",
    "    return xti_plus\n",
    "\n",
    "\n",
    "def step1_3_condition(x_current: float, \n",
    "            x_previous: float, \n",
    "            xti_plus: float,\n",
    "            function: callable, \n",
    "            derivative: callable, \n",
    "            i: int,\n",
    "            sigma_0: float, \n",
    "            sigma: float, \n",
    "            y:float\n",
    "            ):\n",
    "    # go through the first condition\n",
    "    c1 = function(x_current) - function(xti_plus) >= ((2 ** i * sigma) / (12)) * abs(xti_plus - x_current) ** 3 - ((sigma_0) / (12)) * abs(x_current - x_previous) ** 3\n",
    "    if not c1: \n",
    "        return False\n",
    "    \n",
    "    return derivative(xti_plus) <= (2 ** i * sigma) * max(abs(xti_plus - x_current), min(abs(x_current - x_previous), y * abs(derivative(x_current)))) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de188ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_newton_method(x_current: float, \n",
    "            x_previous: float, \n",
    "            xti_plus: float,\n",
    "            function: callable, \n",
    "            derivative: callable, \n",
    "            i: int,\n",
    "            sigma_0: float, \n",
    "            sigma: float, \n",
    "            y:float\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7ed63",
   "metadata": {},
   "source": [
    "### __Problem 5. (5 points)__ \n",
    "It remains to combine the results obtained in Problems 1-4. \n",
    "For this purpose, let us remember that the original regression problem is a machine learning problem and that the linear model $g$ can be used to predict the values of labels $y$. \n",
    "\n",
    "How do we use the final model for prediction? \n",
    "\n",
    "After answering the question, make predictions on a test sample $X_{test}$. \n",
    "\n",
    "Compare with the actual $y_{test}$ labels. \n",
    "\n",
    "The number of correctly guessed labels is the accuracy/accuracy of the model. \n",
    "Compare the gradient descent method, heavy ball method, accelerated gradient method, Fletcher-Reeves method, Polak-Ribier method, Newton method, BFGS.\n",
    "\n",
    "Construct two plots: the value of convergence criterion from running time and the prediction accuracy from running time. \n",
    "\n",
    "Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3413c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution  (Code и Markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3a6fa0",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "This is a homework assignment on the material from the 10th-15th classes. The deadline for submission is __23:59 on December 18__.\n",
    "\n",
    "- The homework is done in the same Jupyter Notebook.\n",
    "\n",
    "- The file should be renamed: __Group Number_First Name_Last Name__ (no spaces at the beginning or end). Example: __000_Ivan_Ivanov__.\n",
    "\n",
    "- Homework should be sent to __OptimizationHomework@yandex.ru__. Subject line: __Innopolis_Task number__ (without spaces at the beginning and end). For this assignment, the subject line is: __Innopolis_3__.\n",
    "\n",
    "- Place the solution to each problem/item after the condition.\n",
    "\n",
    "- Do not forget to add necessary explanations and comments.\n",
    "\n",
    "- All technical artifacts should be removed in the final version that will be sent for checking. By such artifacts we mean any cell outputs that are not commented in any way in the text, as well as any bulk/long technical output (even if it is commented in the text).\n",
    "\n",
    "- A full run of the solution (Kernel -> Restart & Run All) should run all cells without errors.\n",
    "\n",
    "- The maximum score for the assignment is 100.\n",
    "\n",
    "We wish you success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386bbbb",
   "metadata": {},
   "source": [
    "### Part 1. Solving the unconstrained optimization problem\n",
    "\n",
    "Consider again the empirical risk minimization:\n",
    "\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w\\|^2_2,\n",
    "\\end{equation}\n",
    "where $\\ell: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ is a loss function, $g : \\mathbb{R}^d \\times \\mathbb{R}^x \\to \\mathbb{R}$ is a model, $w$ are parameters of the model, $\\{x_i, y_i\\}_{i=1}^n$ is data of objects $x_i \\in \\mathbb{R}^x$ and labels $y_i \\in \\mathbb{R}$.\n",
    "\n",
    "We use the linear model $g(w, x) = w^T x$ and the logistic/sigmoid loss function: $\\ell(z,y) = \\ln (1 + \\exp(-yz))$ (__Important: $y$ must take values $-1$ or $1$__). \n",
    "\n",
    "As we already know, the resulting problem is called a logistic regression problem.\n",
    "\n",
    "This problem can be rewritten as follows:\n",
    "\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} f(w) := \\frac{1}{s} \\sum\\limits_{j=1}^s f_j(w) := \\frac{1}{s} \\sum\\limits_{j=1}^s \\left[\\frac{1}{b} \\sum\\limits_{i=1}^b l (g(w, x_{(j-1)b + i}), y_{(j-1)b + i}) + \\frac{\\lambda}{2} \\| w\\|^2_2\\right],\n",
    "\\end{equation}\n",
    "where $b$ is the batch size, $s$ is the number of batches, and $b s = n$ is the total sample size.\n",
    "\n",
    "The gradient of $f_j$:\n",
    "$$\n",
    "\\nabla f_j(w) = \\frac{1}{b} \\sum_{i=1}^b \\frac{-y_{(j-1)b + i} x_{(j-1)b + i}}{1 + \\exp(y_{(j-1)b + i} w^Tx_{(j-1)b + i})}.\n",
    "$$\n",
    "The Lipschitz constant of the gradient $\\nabla f_j$ can be estimated as $L_j = \\frac{1}{4b} \\sum\\limits_{i=1}^b \\| x_{(j-1)b + i} \\|^2_2$.\n",
    "\n",
    "Let us do some preparation work. The _mushrooms_ dataset is attached. Use the following code to generate a matrix $X$ and vector $y$, which will store the sample $\\{x_i, y_i\\}_{i=1}^n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1f7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "#the file must be in the same directory as notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f4a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ef9c8",
   "metadata": {},
   "source": [
    "Let us change the vector $y$ so that $y_i$ takes values $-1$ and $1$. You can also do additional preprocessing of the data (with techniques from machine learning), but this is not additionally assessed in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c6af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * y - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabb63a",
   "metadata": {},
   "source": [
    "Let us divide the data into two parts: train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8e9fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d427e",
   "metadata": {},
   "source": [
    "For $X_{train}$, $y_{train}$, estimate the constant $L$. Set $\\lambda$ such that $\\lambda \\approx L / 1000$.  Realize in the code the calculation of the value and gradient of $f$ ($X$, $y$, $\\lambda$ should be given as a parameter so that it is possible to change them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec184b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373d531",
   "metadata": {},
   "source": [
    "__Problem 0. (5 points)__  Implement the ability to uniformly divide the training part of the dataset into batches of size $b$ ($b$ is a parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377d566",
   "metadata": {},
   "source": [
    "## Problem 1. (50 points)__ This part of the assignment is related to __non-distributed stochastic__ methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eadf37",
   "metadata": {},
   "source": [
    "### a). (5 points)__ Implement the SGD method:\n",
    "$$\n",
    "w^{k+1} = w^k - \\gamma_k \\nabla f_{j_k} (w^k),\n",
    "$$\n",
    "where the number $j_k$ is generated independently and uniformly from $\\{1, \\ldots, s \\}$.\n",
    "\n",
    "Just in case, we give here a variant of the function description for the gradient descent from the first assignment. You can use this format if you wish. Note that ``x_sol`` occurs in the code - this problem should be solved or criteria tied to ``x_sol`` should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, nabla_f, x_0, x_sol, gamma_k,\n",
    "                     K = 10**3, eps = 10**-5, mode = 'x_k - x^*'):\n",
    "    '''\n",
    "        f - target function\n",
    "        nabla_f - gradient of the target function\n",
    "        x_0 - start point\n",
    "        x_sol - exact solution (it is needed for error calculation)\n",
    "        gamma_k - function for calculating the method step\n",
    "        K - number of iterations (by default 1e3)\n",
    "        eps - accuracy (by default 1e-5)\n",
    "        mode - convergence criterion \n",
    "               Values are either 'x_k - x^*' - then the convergence criterion will be |||x_k - x^*||,\n",
    "               or 'f(x_k) - f(x^*)' - then the convergence criterion will be f(x_k) - f(x^*),\n",
    "               or 'x_k+1 - x_k', or 'f(x_k+1) - f(x_k)' (the criteria will be similar)\n",
    "\n",
    "        The function returns the point at which the minimum is reached and the error vector\n",
    "    '''\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671dfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d708445",
   "metadata": {},
   "source": [
    "__b). (7 points)__ Solve the optimization problem on the training sample using the implemented method. Take $b = 10$, and the step is $\\gamma_k \\equiv \\frac{1}{\\tilde L}$. From the point of view of the theory, what should be taken as $\\tilde L$? Draw the convergence plot: the value of the convergence criterion ($\\frac{\\| \\nabla f(w^k)\\|}{\\| \\nabla f(w^0)\\|}$) from the iteration number. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee49367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7b6d6",
   "metadata": {},
   "source": [
    "__c). (7 points)__ Vary the batch size: $b = 1, 10, 100, 1000$, and take the step size equal to $\\gamma_k \\equiv \\frac{1}{\\tilde L}$ (note that $\\tilde L$ need to be recalculated for each $b$). Draw the convergence plot: the value of the convergence criterion from the iteration number for each $b$. Does this plot reflect a fair comparison? Why? Figure out how to compare the results to each other more honestly (running time is a good option, but you can't use it here) and draw a new comparison plot. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ee3e5",
   "metadata": {},
   "source": [
    "__d). (6 points)__ Let us fix $b = 10$ and try to change the strategy of choosing the step:\n",
    "\n",
    "1) $\\gamma_k \\equiv \\frac{1}{\\tilde L}$ as we did before,\n",
    "\n",
    "2) $\\gamma_k \\equiv \\text{const}$, selecting $\\text{const}$ to get the \"best\" convergence (explain what you mean by \"best\"),\n",
    "\n",
    "3) $\\gamma_k = \\frac{1}{\\sqrt{k + 1}}$, \n",
    "\n",
    "4) $\\gamma_k = \\frac{1}{k + 1}$.\n",
    "\n",
    "Draw the convergence plot: the value of the convergence criterion from the iteration number. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4df6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56f586",
   "metadata": {},
   "source": [
    "__e). (5 points)__ In the previous hometask, we used the obtained solution of the optimization problem to predict answers on a test sample. Recall the essence: the original regression problem is a machine learning problem and using a linear model $g$ we can predict the values of labels $y$. Suppose we have a sample $x_i$, the model response for this sample is $g(w^*, x^i)$. Then the predictive rule can be formulated in the following rather natural way:\n",
    "$$\n",
    "y_i = \n",
    "\\begin{cases}\n",
    "1, & g(w^*, x^i) \\geq 0,\n",
    "\\\\\n",
    "-1, & g(w^*, x^i) < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "By making predictions on the test sample $X_{test}$, we can compare the result with the real labels $y_{test}$. The number of correctly guessed labels is the accuracy of the model.\n",
    "\n",
    "Look at the accuracy of the model trained with SGD. Repeat point c)-d), but now plot the accuracy dependence, not the convergence criterion. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9184b",
   "metadata": {},
   "source": [
    "__f). (30 points)__ Implement the SAGA (Section A.5 of [paper](https://arxiv.org/pdf/1905.11261.pdf)), SVRG (Section А.9 from [работы](https://arxiv.org/pdf/1905.11261.pdf)) and SARAH ([работа](https://arxiv.org/pdf/1703.00102.pdf)) methods for our problem. Solve the optimization problem on the training sample using the new implemented methods. Take $b = 10$, and hyperparameters of methods according to the theory (see corresponding papers). Write these parameters. Is the iteration number a fair for comparison of these methods? Figure out how to compare the results more honestly (running time is a good option, but you can't use it here). Draw the comparison plots of SGD (with step $\\gamma_k \\equiv \\frac{1}{\\tilde L}$), SAGA, SVRG and SARAH: 1) value of convergence criterion versus your criterion, 2) accuracy of predictions versus your criterion. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ebd64",
   "metadata": {},
   "source": [
    "As in d.2), find the \"best\" step for SAGA, SVRG, SARAH. Draw the comparison plots for SGD, SAGA, SVRG, SARAH with the \"best\" steps: 1) convergence criterion value, 2) prediction accuracy. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa004b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19349804",
   "metadata": {},
   "source": [
    "__Problem 2. (25 points)__ This part of the assignment is related to __distributed with compression__ methods.\n",
    "\n",
    "Let us simulate a distributed  environment. To do this, divide the training sample randomly and uniformly into $s=10$ parts (this was implemented in Problem 0). \n",
    "\n",
    "__а). (5 points)__ Implement the $\\text{Rand}k$ and $\\text{Top}k$ compression operators, where $k$ is a parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dce8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution  (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f0a52",
   "metadata": {},
   "source": [
    "__b). (10 points)__ Implement simple distributed gradient descent with compression operators. Compress information only from the devices to the server. Use compression operators $\\text{Rand}1\\%$, $\\text{Rand}5\\%$, $\\text{Rand}10\\%$, $\\text{Rand}20\\%$. As in d.2) of Problem 1, find the \"best\" steps for all cases. Draw the comparison plots with these \"best\" steps: 1) convergence criterion value versus number of transmitted coordinates from the devices to the server, 2) prediction accuracy versus number of transmitted coordinates from the devices to the server. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7ed63",
   "metadata": {},
   "source": [
    "__c). (10 points)__ Now we use $\\text{Top}10\\%$ as compression operators. Compress the information only from the devices to the server. Use two methods: regular distributed gradient descent with compression operators, and distributed gradient descent with error feedback technique (see Algorithm 1 from [article](https://arxiv.org/abs/2002.12410)). For each method, select the step for the \"best\" convergence. Draw the comparison plots with these \"best\" steps: 1) convergence criterion value versus number of transmitted coordinates from the devices to the server, 2) prediction accuracy versus number of transmitted coordinates from the devices to the server. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3413c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution  (Code and Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a83a3d",
   "metadata": {},
   "source": [
    "__Problem 3. (15 points)__ This part of the assignment is related to __distributed with local steps__ methods.\n",
    "\n",
    "As in the previous problem we simulate the distributed enviroment with $s = 10$. Implement simple Local GD/FedAvg method (Algorithm 1 of [paper](https://arxiv.org/pdf/1909.04746.pdf)), where you can choose the number of local steps $H$ as a parameter. Vary the number of local steps: $H = 1, 5, 10, 20$. Take hyperparameters of the method according to the theory (see the corresponding paper). Write these parameters. Draw the comparison plots for different $H$: 1) value of convergence criterion versus number of communications, 2) accuracy of predictions versus number of communications. Make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your solution  (Code and Markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

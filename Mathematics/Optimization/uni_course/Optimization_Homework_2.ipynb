{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f3a6fa0",
      "metadata": {
        "id": "9f3a6fa0"
      },
      "source": [
        "# Homework 2\n",
        "\n",
        "This is a homework assignment on the material from the 6th-9th classes. The deadline for submission is __23:59 on October 29__.\n",
        "\n",
        "- The homework is done in the same Jupyter Notebook.\n",
        "\n",
        "- The file should be renamed: __Group Number_First Name_Last Name__ (no spaces at the beginning or end). Example: __000_Ivan_Ivanov__.\n",
        "\n",
        "- Homework should be sent to __OptimizationHomework@yandex.ru__. Subject line: __Innopolis_Task number__ (without spaces at the beginning and end). For this assignment, the subject line is: __Innopolis_2__.\n",
        "\n",
        "- Place the solution to each problem/item after the condition.\n",
        "\n",
        "- Do not forget to add necessary explanations and comments.\n",
        "\n",
        "- All technical artifacts should be removed in the final version that will be sent for checking. By such artifacts we mean any cell outputs that are not commented in any way in the text, as well as any bulk/long technical output (even if it is commented in the text).\n",
        "\n",
        "- A full run of the solution (Kernel -> Restart & Run All) should run all cells without errors.\n",
        "\n",
        "- The maximum score for the assignment is 100.\n",
        "\n",
        "We wish you success!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a386bbbb",
      "metadata": {
        "id": "a386bbbb"
      },
      "source": [
        "### Part 1. Solving the unconstrained optimization problem\n",
        "\n",
        "Consider the problem of empirical risk minimization:\n",
        "\\begin{equation}\n",
        "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w \\|^2_2,\n",
        "\\end{equation}\n",
        "where $\\ell$ is the loss function, $g$ is the model, $w$ is the model parameters, $\\{x_i, y_i\\}_{i=1}^n$ is the data sample from feature vectors $x_i$ and labels $y_i$, $\\lambda > 0$ is the regularization parameter.\n",
        "\n",
        "We use the linear model $g(w, x) = w^T x$ and the logistic/sigmoidal loss function: $\\ell(z,y) = \\ln (1 + \\exp(-yz))$ (__Important: $y$ must take values $-1$ or $1$__). The resulting problem is called a logistic regression problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a156acc",
      "metadata": {
        "id": "6a156acc"
      },
      "source": [
        "\n",
        "__Problem 1. (20 points)__ Let us do some preparation work.\n",
        "\n",
        "__а). (15 points)__\n",
        "\n",
        "Write out the gradient and the Hessian for this problem.\n",
        "\n",
        "Is the problem convex? Is $\\mu$ strongly convex?\n",
        "\n",
        "If so, how can $\\mu$ be estimated?\n",
        "\n",
        "Estimate the Lipschitz constant of the gradient $L$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EQq-Pe6MGmb-",
      "metadata": {
        "id": "EQq-Pe6MGmb-"
      },
      "source": [
        "Let $L_s$ be the problem's function. Let's define $L_s = L_1 + L_2$, where\n",
        "\\begin{align}\n",
        "L_1 &= \\frac{1}{n}\\sum_{i=1}^{n} l(<w, x_i>, y_i) \\\\\n",
        "L_2 &= \\frac{\\lambda}{2} \\| w \\|_2 ^ 2\n",
        "\\end{align}\n",
        "$L_1$ is a sum, so let's define the derivative of each term.\n",
        "\n",
        "Let's denote $y_i$ by $y$, $x_i$ by $x$,and $z = <y.x, w>$\n",
        "\n",
        "Then, we have:\n",
        "\\begin{align}\n",
        "dz = <yx, dw>\n",
        "\\end{align}\n",
        "Defining $h = \\ln(1 + \\exp(-t))$, then $h^{'} = \\frac{-1}{1 + e^t}$ and:\n",
        "\n",
        "\\begin{align}\n",
        "f &= h(z)  && \\text{$f$ is a single term in the sum}\\\\\n",
        "df &= h'(z) \\cdot dz \\\\\n",
        "df &=  \\frac{-1}{1 + e^z} \\cdot <y.x, dw> \\\\\n",
        "df &=  \\frac{- <y.x, dw>}{1 + e^{<y.x, w>}}  \\\\\n",
        "\\implies \\nabla f &= -\\frac{y \\cdot x}{1 + e^{<y \\cdot x, w>}} \\\\\n",
        "\\implies \\nabla L_1 &= \\sum_{i = 1}^n -\\frac{y_i \\cdot x_i}{1 + e^{<y_i \\cdot x_i, w>}}\n",
        "\\end{align}\n",
        "\n",
        "As for $L_2$, we have:\n",
        "\\begin{align}\n",
        "d(L_2) &= \\lambda \\cdot <w, dw> \\\\\n",
        "\\implies \\nabla L_s &= \\frac{1}{n}\\sum_{i = 1}^n -\\frac{y_i \\cdot x_i}{1 + e^{<y_i \\cdot x_i, w>}} + \\lambda \\cdot w\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3unsY0n4Ljfl",
      "metadata": {
        "id": "3unsY0n4Ljfl"
      },
      "source": [
        "* 2nd order derivative: Hessian matrix:\n",
        "\n",
        "I will use the same approach as in the first part.\n",
        "\\begin{align}\n",
        "d^2(f) &= <d (\\nabla f), dw_1> \\\\\n",
        "d (\\nabla f) &= - \\frac{(1 + e^z) \\cdot d(y \\cdot x) - d(1 + e^z) \\cdot y \\cdot x}{(1 + e^z)^2} \\\\\n",
        "d(\\nabla f) &= \\frac{e ^ z \\cdot <y \\cdot x, dw> \\cdot y \\cdot x }{(1 + e^z)^2}&& \\text{differentiation with respect to $w \\implies d(y \\cdot x) = 0$} \\\\\n",
        "\\implies d(\\nabla f) &= \\frac{e ^ z \\cdot x\\cdot <x, dw>}{(1 + e^z)^2} && \\text{using $y \\in \\{1, -1\\}$} \\\\\n",
        "\\implies d(\\nabla f) &= \\frac{e ^ z \\cdot x\\cdot x^T \\cdot dw}{(1 + e^z)^2} \\\\\n",
        "\\implies d^2(f) &= <\\frac{e ^ z \\cdot x\\cdot x^T \\cdot dw}{(1 + e^z)^2}, dw_1> \\\\\n",
        "\\implies d^2(f) &= <\\frac{e ^ z \\cdot x\\cdot x^T}{(1 + e^z)^2} \\cdot dw_1, dw> && \\text{using $<Ab, c> = <A^Tc, b>$} \\\\\n",
        "\\implies \\nabla ^ 2 f &= \\frac{e ^ {<y \\cdot x, w>} \\cdot x\\cdot x^T}{(1 + e^{<y \\cdot x, w>})^2} \\\\\n",
        "\\implies \\nabla ^ 2 L_1 &= \\frac{1}{n} \\sum_{i=1}^n \\frac{e ^ {<y_i \\cdot x_i, w>} \\cdot x_i\\cdot x_i^T}{(1 + e^{<y_i \\cdot x_i, w>})^2}\n",
        "\\end{align}\n",
        "\n",
        "As for $\\nabla ^ 2 L_2 = λ$. Therefore,\n",
        "$$ \\nabla ^ 2 L_s = \\frac{1}{n} \\sum_{i=1}^n \\frac{e ^ {<y_i \\cdot x_i, w>} \\cdot x_i\\cdot x_i^T}{(1 + e^{<y_i \\cdot x_i, w>})^2} + λ \\cdot I_d  \n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XimCyz5UjiYp",
      "metadata": {
        "id": "XimCyz5UjiYp"
      },
      "source": [
        "For any vector $x$, the matrix $x \\cdot x^T$ is positive definite, since:\n",
        "\n",
        "\\begin{align}\n",
        " a^T \\cdot (x \\cdot x^T) \\cdot a &= b^T \\cdot b \\geq 0&& \\text{where $b = x ^ T \\cdot a$}\n",
        "\\end{align}\n",
        "and since, the term $\\frac{e ^ {<y_i \\cdot x_i, w>}}{(1 + e^{<y_i \\cdot x_i, w>})^2}$ is always positive, we can say taht $L_1$ is indeed convex.\n",
        "\n",
        "$I_d$ is positive definite. and Thus, $L_s$ is convex.\n",
        "\n",
        "$L_s$ is also strong convex with $\\mu = λ$ since $∇^2 L_s - λ \\cdot I_d$ is semi positive definite."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EnGUOfmom2sp",
      "metadata": {
        "id": "EnGUOfmom2sp"
      },
      "source": [
        "* Estimate: $L$\n",
        "\n",
        "Let's define $a_i = y_i \\cdot x_i$, then we have:\n",
        "\n",
        "$$\n",
        "\\nabla L_s = \\frac{1}{n}\\sum_{i = 1}^n -\\frac{a_i}{1 + e^{<a_i, w>}} + \\lambda \\cdot w\n",
        "$$\n",
        "The constant $L$ satisfies the following inequality:\n",
        "\n",
        "\\begin{align}\n",
        "\\| \\nabla L_s(w_1) - \\nabla L_s (w_2) \\| &\\leq L \\cdot \\|w_1 - w_2 \\| ~~ \\forall w_1, w_2 \\in \\mathbb{R}^{d} \\\\\n",
        "\\| \\frac{1}{n} \\cdot  \\sum_{i=1}^n -\\frac{a_i}{1 + e^{<a_i, w1>}} + \\lambda w_1 + \\frac{1}{n} \\cdot  \\sum_{i=1}^n \\frac{a_i}{1 + e^{<a_i, w_2>}} -  \\lambda \\cdot  w_2 \\| &\\leq L \\cdot \\|w_1 - w_2 \\|\n",
        "\\end{align}\n",
        "The inequality above is slightly challenging to solve. We proceed as follows:\n",
        "\n",
        "\\begin{align}\n",
        " \\|\\sum_{i = 1}^n x_i \\| & \\leq \\sum_{i = 1}^n \\|x_i\\| \\\\\n",
        " \\implies \\| \\frac{1}{n} \\cdot \\sum_{i=1}^n -\\frac{a_i}{1 + e^{<a_i, w1>}} + \\lambda w_1 + \\frac{1}{n} \\cdot \\sum_{i=1}^n \\frac{a_i}{1 + e^{<a_i, w_2>}} -  \\lambda \\cdot  w_2 \\| & \\leq \\frac{1}{n} \\cdot \\sum_{i = 1}^n \\| a_i \\| \\cdot | \\frac{1}{1 + e^{<a_i, w_1>}} - \\frac{1}{1 + e ^ {<a_i, w_2>}}| + \\lambda \\cdot \\|w_1 - w_2 \\|\n",
        "\\end{align}\n",
        "\n",
        "To estimate $| \\frac{1}{1 + e^{<a_i, w_1>}} - \\frac{1}{1 + e ^{<a_i, w_2>}}|$, we need the following result (cited from [this paper](https://www.emis.de/journals/AM/08-3/am1630.pdf)):\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{|e^x - e^y|}{e^x + e^y} &\\leq \\frac{|x - y|}{2} ~~ \\forall x, y \\in \\mathbb{R} \\\\\n",
        "|\\frac{1}{1 + e^x} - \\frac{1}{1 + e^y} | &= |\\frac{e^y - e^x}{(1 + e^x) \\cdot (1 + e^y)}| \\\\\n",
        "(1 + e^x) \\cdot (1 + e^y) &\\geq (e^x + e^y) \\\\\n",
        "\\implies \\frac{|e^x - e^y|}{(1 + e^x) \\cdot (1 + e^y)} &\\leq \\frac{|e^x - e^y|}{e^x + e^y} \\\\\n",
        "\\implies \\frac{|e^x - e^y|}{(1 + e^x) \\cdot (1 + e^y)} &\\leq \\frac{|x - y|}{2}\n",
        "\\end{align}\n",
        "\n",
        "substituting, $x = <a_i, w_1>$ and $y = <a_i, w_2>$:\n",
        "\\begin{align}\n",
        "| \\frac{1}{1 + e^{<a_i, w_1>}} - \\frac{1}{1 + e ^{<a_i, w_2>}}| &\\leq \\frac{| <a_i, w_1> - <a_i, w_2> |}{2} = \\frac{|<a_i, w_1 - w_2>|}{2} \\\\\n",
        "|<a_i, w_1 - w_2>| &\\leq \\|a_i\\| \\cdot \\|w_1 - w_2 \\| && \\text{by Cauchy inequality} \\\\\n",
        "\\implies | \\frac{1}{1 + e^{<a_i, w_1>}} - \\frac{1}{1 + e ^{<a_i, w_2>}}| &\\leq \\frac{1}{2} \\cdot \\|a_i\\| \\cdot \\|w_1 - w_2 \\|\n",
        "\\end{align}\n",
        "\n",
        "Using this result:\n",
        "\n",
        "\\begin{align}\n",
        "\\| \\sum_{i=1}^n -\\frac{a_i}{1 + e^{<a_i, w1>}} + \\lambda w_1 + \\sum_{i=1}^n \\frac{a_i}{1 + e^{<a_i, w_2>}} -  \\lambda \\cdot  w_2 \\| &\\leq \\frac{1}{2n} \\cdot\n",
        "\\sum_{i = 1}^{n}\\|a_i\\| ^ 2 \\cdot \\|w_1 - w_2\\| + \\lambda \\|w_1 - w_2\\|\n",
        "= (\\frac{1}{n} \\sum_{i = 1}^{n}\\|a_i\\| ^ 2 + \\lambda) \\cdot \\|w_1 - w_2 \\|\n",
        "\\end{align}\n",
        "Thus, for \n",
        "$$L = \\frac{1}{2n} \\sum_{i = 1}^{n}\\|x_i\\| ^ 2 + \\lambda \\\\\n",
        "\\| \\nabla L_s(w_1) - \\nabla L_s (w_2) \\| \\leq L \\cdot \\|w_1 - w_2 \\|~~ \\forall w_1, w_2 \\in \\mathbb{R}^{d}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a61362fe",
      "metadata": {
        "id": "a61362fe"
      },
      "source": [
        "The _mushrooms_ dataset is attached. Use the following code to generate a matrix $X$ and vector $y$, which will store the sample $\\{x_i, y_i\\}_{i=1}^n$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1d1f7763",
      "metadata": {
        "id": "1d1f7763"
      },
      "outputs": [],
      "source": [
        "#the file must be in the same directory as notebook.\n",
        "dataset = \"mushrooms.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "41f4a066",
      "metadata": {
        "id": "41f4a066"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_svmlight_file\n",
        "data = load_svmlight_file(dataset)\n",
        "X, y = data[0].toarray(), data[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d6ef9c8",
      "metadata": {
        "id": "8d6ef9c8"
      },
      "source": [
        "Let us change the vector $y$ so that $y_i$ takes values $-1$ and $1$. You can also do additional preprocessing of the data (with techniques from machine learning), but this is not additionally assessed in any way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b9c6af0b",
      "metadata": {
        "id": "b9c6af0b"
      },
      "outputs": [],
      "source": [
        "y = 2 * y - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fabb63a",
      "metadata": {
        "id": "0fabb63a"
      },
      "source": [
        "Let us divide the data into two parts: training and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c8e9fd42",
      "metadata": {
        "id": "c8e9fd42"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "082d427e",
      "metadata": {
        "id": "082d427e"
      },
      "source": [
        "__b). (5 points)__ For the training part $X_{train}$, $y_{train}$, estimate the constant $L$. Set $\\lambda$ such that $\\lambda \\approx L / 1000$.  Realize in the code the calculation of the value, gradient and Hessian for our target function ($X$, $y$, $\\lambda$ should be given as a parameter so that it is possible to change them, not only to substitute fixed $X_{train}$, $y_{train}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de61cc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Tuple\n",
        "def oracle_call(X: np.ndarray, \n",
        "                y: np.ndarray, \n",
        "                w: np.ndarray, \n",
        "                lam: float) -> Tuple[float, np.ndarray, np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    This function calculates different values of the function gives it parameters:\n",
        "        1. its value f(w)\n",
        "        2. its derivative df(w)\n",
        "        3. its hessian matrix \n",
        "        4. an estimation of the Lipschitz constant 'L'\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): The training data\n",
        "        y (np.ndarray): The labels\n",
        "        w (np.ndarray): the weights\n",
        "        lam (float): lambda: the regularization hyper-parameter\n",
        "    \"\"\"\n",
        "    # first let's make sure the input is as expected: X is expected to have samples as rows\n",
        "    if X.ndim != 2:\n",
        "        raise ValueError(f\"the training is expected to have at least 2 samples\")\n",
        "    \n",
        "    num_samples, dim = X.shape \n",
        "    # make sure 'y' matches the number of samples\n",
        "    y = np.expand_dims(y, axis=0) if y.ndim == 1 else y\n",
        "\n",
        "    if y.shape != (num_samples, 1):\n",
        "        raise ValueError((f\"The number of labels is expected to match the number of samples\"\n",
        "                          f\"\\nFound: {y.shape[0]} Expected: {num_samples}\"))\n",
        "    # check 'w' as well:\n",
        "    w = np.expand_dims(w, axis=0) if w.ndim == 1 else w\n",
        "    # make sure the dimensions match\n",
        "    if w.shape != (dim, 1):\n",
        "        raise ValueError((f\"The weight matrix 'w' is expected to be of the same dimension\"\n",
        "                          f\"as the training sample\\nFound: {w.shape[0]}. Expected: {dim}\"))\n",
        "\n",
        "    # now as our input has been verified we proceed by defining some useful intermediate variables\n",
        "    A = X * y\n",
        "    preds = A @ w\n",
        "    # let's calculate the exponential function: \n",
        "    exp = np.exp(-preds) # exp now contains the at each row: exp(w_i^T * y_i* x_i)\n",
        "    \n",
        "    assert exp.shape == (num_samples, 1), \"Make sure the matrix of exponential is correct\"\n",
        "    \n",
        "    # calculate L1\n",
        "    l1 = np.mean(np.log(1 + np.exp(-preds)))\n",
        "    # calculate L2\n",
        "    l2 = (lam / 2) * np.linalg.norm(w)\n",
        "    \n",
        "    l = l1 + l2\n",
        "\n",
        "    # time to calculate the gradient of L\n",
        "    dl = - np.mean(A / (1 + np.exp(preds)), axis=0).T + lam * w\n",
        "\n",
        "    # we might need a for loop for the hessian \n",
        "    hessian = np.zeros((dim, dim), dtype=np.float32)\n",
        "    \n",
        "    exp1 = np.exp(preds)\n",
        "    exp2 = (1 + exp) ** 2\n",
        "\n",
        "    assert exp1.shape == (num_samples, 1) and exp2.shape == (num_samples, 1)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        x_i = X[i, :]\n",
        "        matrix_i = x_i.T @ x_i\n",
        "        # add the coefficients\n",
        "        h = (exp1[i][0] / exp2[i][0]) * matrix_i \n",
        "        hessian += h\n",
        "    \n",
        "    # make sure to devide by number of samples\n",
        "    hessian = hessian / num_samples + lam\n",
        "    \n",
        "    assert hessian.shape == (num_samples, num_samples)\n",
        "\n",
        "    # now time to estimate 'L'\n",
        "    L = np.linalg.norm(X) ** 2 / (2 * num_samples) + lam\n",
        "\n",
        "    L = max(L, 1000 * lam)\n",
        "\n",
        "    return l, dl, hessian, L\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec184b82",
      "metadata": {
        "id": "ec184b82"
      },
      "outputs": [],
      "source": [
        "#your solution (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2377d566",
      "metadata": {
        "id": "2377d566"
      },
      "source": [
        "__Problem 2. (20 points)__ This part of the assignment is related to momentum and acceleration.\n",
        "\n",
        "__a). (5 points)__ Implement the heavy ball method and Nesterov's accelerated gradient method.\n",
        "\n",
        "Just in case, we give here a variant of the function description for the gradient descent from the first assignment. You can use this format if you wish. Note that ``x_sol`` occurs in the code - this problem should be solved or criteria tied to ``x_sol`` should not be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be5209a",
      "metadata": {
        "id": "6be5209a"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(f, nabla_f, x_0, x_sol, gamma_k,\n",
        "                     K = 10**3, eps = 10**-5, mode = 'x_k - x^*'):\n",
        "    '''\n",
        "        f - target function\n",
        "        nabla_f - gradient of the target function\n",
        "        x_0 - start point\n",
        "        x_sol - exact solution (it is needed for error calculation)\n",
        "        gamma_k - function for calculating the method step\n",
        "        K - number of iterations (by default 1e3)\n",
        "        eps - accuracy (by default 1e-5)\n",
        "        mode - convergence criterion\n",
        "               Values are either 'x_k - x^*' - then the convergence criterion will be |||x_k - x^*||,\n",
        "               or 'f(x_k) - f(x^*)' - then the convergence criterion will be f(x_k) - f(x^*),\n",
        "               or 'x_k+1 - x_k', or 'f(x_k+1) - f(x_k)' (the criteria will be similar)\n",
        "\n",
        "        The function returns the point at which the minimum is reached and the error vector\n",
        "    '''\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671dfd0c",
      "metadata": {
        "id": "671dfd0c"
      },
      "outputs": [],
      "source": [
        "#your solution (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d708445",
      "metadata": {
        "id": "2d708445"
      },
      "source": [
        "__b). (15 points)__ Solve an optimization problem on a train sample using two implemented methods. Fix a step $\\frac{1}{L}$ and search different values of momentum from $-1$ to $1$. Check also the momentum values equal to $\\frac{k}{k+3}$, $\\frac{k}{k+2}$, $\\frac{k}{k+1}$ ($k$ is the iteration number), and if the target function is strongly convex, also $\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}$. You can choose your own starting point and convergence criterion; we recommend using the normalized version of the criterion, for example, $\\frac{\\| \\nabla f(x^k) \\|}{\\| \\nabla f(x^0) \\|}$, as well as using the same starting point and convergence criterion in Problems 3-4.\n",
        "\n",
        "In this task, you should draw three plots: 1) the convergence criterion values from iteration number for the heavy ball method with different values of momentum, 2) the convergence criterion values from iteration number for the accelerated gradient method with different values of momentum, and 3) the convergence criterion values from iteration number for the two methods with the best choice of momentum for each, as well as the gradient descent.\n",
        "\n",
        "Remember to make conclusions and comment on the results. For example, reflect on whether convergence is always monotone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee49367",
      "metadata": {
        "id": "3ee49367"
      },
      "outputs": [],
      "source": [
        "#your solution (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19349804",
      "metadata": {
        "id": "19349804"
      },
      "source": [
        "__Problem 3. (30 points)__ In this part, we work with the conjugate gradient method.\n",
        "\n",
        "__а). (5 points)__ Realize the Fletcher-Reeves and Polak-Ribier method. Describe how you will search for $\\alpha_k$ steps (both the algorithm and its initialization are interesting). Add to the algorithms the ability to do \"restarts\" (sometimes take $\\beta_k = 0$) with some frequency that can be customized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96dce8db",
      "metadata": {
        "id": "96dce8db"
      },
      "outputs": [],
      "source": [
        "#your solution  (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005f0a52",
      "metadata": {
        "id": "005f0a52"
      },
      "source": [
        "__b). (10 points)__ Solve the optimization problem with two implemented methods, varying the frequency of \"restarts\" for each: $1$ (every iteration $\\beta_k = 0$), $10$ (every tenth iteration $\\beta_k = 0$), $100$, $1000$, no restarts.\n",
        "\n",
        "Draw three plots: 1) the convergence criterion values from iteration number for the Fletcher-Reeves method with different restart frequencies, 2) the convergence criterion values from iteration number for the Polak-Ribier method with different restart frequencies, 3) the convergence criterion values from iteration number for both methods with the best choice of restart frequency. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a1a92f",
      "metadata": {
        "id": "73a1a92f"
      },
      "outputs": [],
      "source": [
        "#your solution (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efddfc99",
      "metadata": {
        "id": "efddfc99"
      },
      "source": [
        "__c). (15 points)__ In this part we abstract from the regression problem. Let us study the properties of the conjugate gradient method for the quadratic problem:\n",
        "$$\\min_{x \\in \\mathbb{R}^d} \\left[\\tfrac{1}{2} x^T A x - b x \\right]$$\n",
        "with a positive definite symmetric matrix $A \\in \\mathbb{R}^{d \\times d}$ and some vector $b \\in \\mathbb{R}^d$. We need to learn how to generate a matrix $A$ with the possibility to specify its spectrum (eigenvalues). The previous assignment already asked us to do this. We suggest the following approach based on the decomposition $A = Q D Q^T$, where $D$ is a diagonal matrix formed from eigenvalues and $Q$ is orthogonal (it can be generated using the $QR$-decomposition of a random matrix).\n",
        "\n",
        "Suppose we have a quadratic problem whose matrix $A \\in \\mathbb{R}^{d \\times d}$ has clustered eigenvalues, meaning, that there exists some number of clusters $k \\leq d$ and values $\\tilde \\lambda_1 < \\ldots < \\tilde \\lambda_k$ such that for any $\\lambda_i$ eigenvalue of matrix $A$ there exists $j \\leq k$ such that $\\lambda_i \\in [(1 - p) \\tilde \\lambda_j; (1 + p) \\tilde \\lambda_j]$, where $p < 1$.\n",
        "\n",
        "Then we will need to generate clustered eigenvalues and then the matrix $A$. When generating the spectrum, try to make sure that all values in it are different. As a convergence criterion, use $\\frac{\\| x^k - x^* \\|^2_A} {\\| x^0 - x^* \\|^2_A}$, where $k$ is the iteration number and $\\| x \\|^2_A = \\langle x, Ax \\rangle$.\n",
        "\n",
        "Let us test the performance of the conjugate gradient method for different variants of eigenvalue clustering:\n",
        "\n",
        "1) Let $d = 100$, $k = 2$, $p = 0.05$, $\\tilde \\lambda_1 = 1$, there are 50 eigenvalues each in the clusters for $\\tilde \\lambda_1$ and $\\tilde \\lambda_2$. Vary the value of $\\tilde \\lambda_2$ from $10$ to $10^5$ (5 different values is enough). Plot the values of the convergence criterion from the iteration number for each value of $\\tilde \\lambda_2$ on one plot. Make a conclusion.\n",
        "\n",
        "2) Let $d = 100$, $k = 2$, $p = 0.05$, $\\tilde \\lambda_1 = 1$, $\\tilde \\lambda_2 = 1000$. Vary the number of eigenvalues in each cluster from $1$ to $99$ (5 different values is enough). Plot the values of the convergence criterion from the iteration number for each value of cluster size for $\\tilde \\lambda_1$ on one plot. Make a conclusion.\n",
        "\n",
        "3) Let $d = 100$, $p = 0.05$, $\\tilde \\lambda_1 = 1$, $\\tilde \\lambda_k = 1000$. Vary the number of clusters $k$ from 2 to 100 (5 different values is enough, include 100 - corresponds to a uniform distribution of eigenvalues). Plot the values of the convergence criterion from the number of iterations for each value of $k$ on one plot. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a6a09b",
      "metadata": {
        "id": "44a6a09b"
      },
      "outputs": [],
      "source": [
        "#your solution (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f5a4256",
      "metadata": {
        "id": "1f5a4256"
      },
      "source": [
        "__Problem 4. (25 points)__ Now let us talk about Newton method and quasi-Newton methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc437ecf",
      "metadata": {
        "id": "dc437ecf"
      },
      "source": [
        "__а). (5 points)__ For the regression problem, implemnt and run Newton method. Does it converge? If not, try running the gradient descent method for several iterations first before using Newton method. Vary the number of gradient descent steps. Plot the value of convergence criterion versus iteration number for the combination of gradient descent and Newton method with different number of gradient descent steps. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e4edd3",
      "metadata": {
        "id": "71e4edd3"
      },
      "outputs": [],
      "source": [
        "#your solution  (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4941f7",
      "metadata": {
        "id": "ad4941f7"
      },
      "source": [
        "__b). (7 points)__ Implement the quasi-Newton method BFGS. Use it to solve the regression problem. Add it to the plot from the previous point about Newton method. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1fbc2f",
      "metadata": {
        "id": "db1fbc2f"
      },
      "outputs": [],
      "source": [
        "#your solution  (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd4230c3",
      "metadata": {
        "id": "cd4230c3"
      },
      "source": [
        "__c). (13 points)__ Let us again depart from regression and consider a one-dimensional minimization problem:\n",
        "\\begin{equation}\n",
        "\\min_{x \\in \\mathbb{R}} f(x) = x \\arctan x - \\frac{1}{2} \\log (1 + x^2).\n",
        "\\end{equation}\n",
        "Solve this problem using Newton method. Draw convergence plots of the method for two different starting points $x^0 = 1.3$ and $x^0 = 1.5$. Make a conclusion.\n",
        "\n",
        "To achieve convergence of Newton method it is not necessary to resort to using another method as a stratum method. Realize two modifications of Newton method: damped (adding a step) and cubic Newton method (see [paper](https://link.springer.com/article/10.1007/s10107-006-0706-8)). Do these methods solve the convergence problem of Newton method for the starting point $x^0 = 1.5$? In the damped method, try taking a step from $0.5$ to $1$. Draw the convergence plots. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ba1e2c",
      "metadata": {
        "id": "e6ba1e2c"
      },
      "outputs": [],
      "source": [
        "#your solution  (Code и Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a7ed63",
      "metadata": {
        "id": "a7a7ed63"
      },
      "source": [
        "__Problem 5. (5 points)__ It remains to combine the results obtained in Problems 1-4. For this purpose, let us remember that the original regression problem is a machine learning problem and that the linear model $g$ can be used to predict the values of labels $y$. How do we use the final model for prediction? After answering the question, make predictions on a test sample $X_{test}$. Compare with the actual $y_{test}$ labels. The number of correctly guessed labels is the accuracy/accuracy of the model. Compare the gradient descent method, heavy ball method, accelerated gradient method, Fletcher-Reeves method, Polak-Ribier method, Newton method, BFGS. Construct two plots: the value of convergence criterion from running time and the prediction accuracy from running time. Make a conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3413c1c",
      "metadata": {
        "id": "d3413c1c"
      },
      "outputs": [],
      "source": [
        "#your solution  (Code и Markdown)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based models\n",
    "In this Jupyter notebook, I am exploring the different functionalities offered by Sklearn to use tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## links to the datasets\n",
    "auto_mpg = \"https://assets.datacamp.com/production/repositories/1796/datasets/3781d588cf7b04b1e376c7e9dda489b3e6c7465b/auto.csv\"\n",
    "bike_sharing_demand = \"https://assets.datacamp.com/production/repositories/1796/datasets/594538f54a854b322d6e4c8031f3f31bc522d3e5/bikes.csv\"\n",
    "breast_concer = 'https://assets.datacamp.com/production/repositories/1796/datasets/0eb6987cb9633e4d6aa6cfd11e00993d2387caa4/wbc.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(breast_concer).iloc[:, :-1]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['diagnosis'], axis=1)\n",
    "diagnosis_mapper = {\"M\": 1, \"B\": 0}\n",
    "y = df['diagnosis'].apply(lambda x: diagnosis_mapper[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "max_depth = 6 \n",
    "random_state = 1\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=max_depth, random_state=random_state)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_2 = DecisionTreeClassifier(max_depth=8, random_state=random_state, criterion='entropy') # the criterion determines the metrics when splitting the tree nodes\n",
    "tree_2.fit(X_train, y_train)\n",
    "print(tree_2.score(X_test, y_test)) # using entropy\n",
    "print(tree.score(X_test, y_test)) # using gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(less_complex_tree_best.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baggin: Bootstrap Aggregation:\n",
    "This is an ensemble learning method. We take a number of the same machine learning algorithm and instead of feeding the whole dataset to the a single instance, we provide a random sample with replacement. As a single data sample/example can be picked multiple time, a number of samples are genearlly left out. The latter samples, gathered form what is known by the \"out of bag\" samples. They represent an unbiased test of the each model's instance referred to as the \"oob score\". The mean of these scores (also referred to as the oob score) can be used to estimate the ensemble model's performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the less_complex_tree_best model is the one with the least generalization error so far, let's experiment with it\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_c = BaggingClassifier(base_estimator=less_complex_tree_best, oob_score=True, random_state=seed, n_estimators=100) # let's set  the rest of the hyperparameters through GridSearchCV\n",
    "\n",
    "# bag_c_params = {\"n_estimators\": [100]}\n",
    "# bag_c_best = GridSearchCV(bag_c, param_grid=bag_c_params, cv=num_folds, n_jobs=-1)\n",
    "bag_c.fit(X_train, y_train)\n",
    "# print(bag_c.best_params_)\n",
    "\n",
    "y_pred = bag_c.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Bagging is not a model specific method. In other words, it can be used for any base estimator. Random forests can be seen as slightly more complicated than baggin using Decision Trees. As Random forests model adds additional randomness by setting a number ***d*** of features to be considered when splitting. Only a random subset of ***d*** features are considered when splitting each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is helpful to compare between the two following values:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "params = {\"max_depth\": [2,3,4,5,6,8], \"min_samples_leaf\": [0.02, 0.04, 0.05, 0.1, 0.12, 0.15], \"max_features\":[\"log2\", \"sqrt\"]}\n",
    "\n",
    "num_folds = 6\n",
    "tree = DecisionTreeClassifier(random_state=3)\n",
    "best_tree = GridSearchCV(tree, param_grid=params, cv=num_folds)\n",
    "\n",
    "best_tree.fit(X_train, y_train)\n",
    "\n",
    "print(best_tree.best_params_)\n",
    "\n",
    "y_train_pred = best_tree.predict(X_train)\n",
    "print(\"First model performance: \")\n",
    "print(best_tree.score(X_train, y_train))\n",
    "print(cross_val_score(best_tree, X_train, y_train, cv=num_folds).mean())\n",
    "\n",
    "# the differenence might be significant between the train error and the cross validation error which is an indicator of overfitting\n",
    "# let's try to use a slightly less complex model\n",
    "\n",
    "seed = 3\n",
    "\n",
    "less_complex_tree = DecisionTreeClassifier() #(max_depth=4, min_samples_leaf= 0., max_features='sqrt', random_state=seed)\n",
    "less_complex_params = {\"min_samples_leaf\": [0.02, 0.03,0.04, 0.05, 0.08, 0.09, 0.1, 0.12]}\n",
    "less_complex_tree_best = GridSearchCV(less_complex_tree, param_grid=less_complex_params)\n",
    "less_complex_tree_best.fit(X_train, y_train)\n",
    "print(\"Second model performance: \")\n",
    "\n",
    "print(less_complex_tree_best.score(X_train, y_train))\n",
    "print(cross_val_score(less_complex_tree_best, X_train, y_train, cv=num_folds).mean())\n",
    "\n",
    "# thanks to this hyperparameters tuning, the model is performing significantly better on both training and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf1 = RandomForestClassifier(random_state=seed, n_estimators=250, oob_score=True)\n",
    "rf1.fit(X_train, y_train)\n",
    "print(rf1.oob_score_)\n",
    "y_pred = rf1.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# RanomdForestClassifier determines the importance of each feature\n",
    "\n",
    "importances = pd.Series(data=rf1.feature_importances_,\n",
    "                        index= df.drop(['diagnosis'], axis=1).columns).sort_values()\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Unlike Bagging, Boosting is another ensemble learning method where weak learners classifiers learn sequentially. The two main types of boosting \n",
    "### Adaboost:\n",
    "the name is an abbreviation for ***Adaptive boosting*** where the first learner learns its coefficients from the initial data. The next learner, will have a different version of the data where the misclassified instances have larger weights/coefficients. The same process keeps going on for all individual learners. Additionally, each instance is assigned a coefficient based on its training error. The latter determines its contribution to the final result.\n",
    "* $ 0 < \\gamma \\leq 1$ parameter is used where $\\alpha_i = \\gamma \\cdot \\alpha_i$ to shrink the associated parameters.\n",
    "* n: number of learners\n",
    "There should be balance between the two hyperpameters as one increases, the other should generally be decreased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "print(ada.score(X_test, y_test))\n",
    "ada_y_pred = ada.predict(X_test)\n",
    "print(confusion_matrix(y_test, ada_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_y_pred_prob = ada.predict_proba(X_test)\n",
    "ada_y_pred = ada.predict(X_test).reshape(-1, 1)\n",
    "pred_perc = (np.amax(ada_y_pred_prob, axis=1)).reshape(-1, 1)\n",
    "\n",
    "arrays = [pred_perc, ada_y_pred.astype(int), y_test.reshape(-1, 1).astype(int)]\n",
    "\n",
    "# for a in arrays:\n",
    "#     print(a.shape)\n",
    "\n",
    "final_array = np.concatenate(arrays, axis=1)\n",
    "\n",
    "# print(final_array)\n",
    "pred_res_df = pd.DataFrame(final_array, columns=['percentage', 'prediction', 'label'])\n",
    "\n",
    "print(pred_res_df.head(10))\n",
    "# let's consider the probabilities associated with misclassified instances\n",
    "print(pred_res_df[pred_res_df['prediction'] != pred_res_df['label']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Gradient boosting is known to be a powerful ensemble learning technique. Unlike adaptive boosting, the gradient version does not modify the weights associated with the training samples. Assuming $n$ estimators, the first estimators is trained on training data, the residual errors as calculated $r_1 = y_1 - \\hat{y}_1$. the second training estimator is not fed $y_1$ but $r_1$ and $r_{i + 1} = r_{i} - \\hat{y}_i$. The final predictions is generally calculated as $ \\hat{y}_1 + \\gamma \\cdot \\sum_{i=2}^{n} \\hat{y}_i$ \\\n",
    "Gradient boosting uses only CART as its base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4, # the maximum depth of a single tree inside the model\n",
    "            n_estimators=200, # the number of trees\n",
    "            random_state=2) # assures reproducability\n",
    "gb.fit(X_train, y_train)\n",
    "gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic GB\n",
    "Gradient boosting is used mainly with trees. Yet, trees use exhaustive search when determining the best split, which might lead to using the same features for each split potenitally hurting the performance. Such issue is addressed by SGB where additional randomness is injected into Gradient Boosting in two ways:\n",
    "1. training each model on a subset of the training samples\n",
    "2. limiting the number of features a tree can use to split its nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, # a single tree's maximum depth \n",
    "            subsample=0.9, # the fraction of samples each tree is trained on\n",
    "            max_features=0.75, # the number of features each tree can consider\n",
    "            n_estimators=200, # the number of inner trees\n",
    "            random_state=2) # ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Well, there is no much theory behind hyperparameter tuning. It might be necessary to consider the different hyperparameters associated with each model before tuning. There are multiple approaches to be consisedered. Sklearn covers two of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider a tree classifier\n",
    "\n",
    "params = {\"max_depth\": [2,3,4,5,6,8], \"min_samples_leaf\": [0.02, 0.04, 0.05, 0.1, 0.12, 0.15], \"max_features\":[\"log2\", \"sqrt\"]}\n",
    "\n",
    "tree_basic = DecisionTreeClassifier()\n",
    "tree_basic.fit(X_train, y_train)\n",
    "tree_searcher = GridSearchCV(estimator=tree_basic, # the model to tune, compulsory parameter\n",
    "                                param_grid=params, # \n",
    "                                scoring='roc_auc', # the scording used to evaluate the best model\n",
    "                                cv = num_folds, # number of folds in the cross_validation estimation\n",
    "                                n_jobs=-1, # use every processor available\n",
    "                                refit=True) # set to fetch the best estimator directly\n",
    "tree_searcher.fit(X_train, y_train)\n",
    "tree_best = tree_searcher.best_estimator_\n",
    "tree_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to tune a RandomForest model\n",
    "rf = RandomForestClassifier()\n",
    "print(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

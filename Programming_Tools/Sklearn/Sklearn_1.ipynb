{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn\n",
    "This is the most widely used libary for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "churn_df = pd.read_csv(\"Utility_files/datasets_course1_DC/telecom_churn_clean.csv\")\n",
    "# print(churn_df.head())\n",
    "y = churn_df[\"churn\"].values\n",
    "features = [\"account_length\", \"customer_service_calls\"]\n",
    "X = churn_df[features].values\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=6)\n",
    "model.fit(X, y)\n",
    "\n",
    "acc_len_max = churn_df[\"account_length\"].max() \n",
    "acc_len_min =  churn_df[\"account_length\"].min()\n",
    "cus_se_ca_max = churn_df[\"customer_service_calls\"].max()\n",
    "cus_se_ca_min = churn_df[\"customer_service_calls\"].min()\n",
    "\n",
    "# create 10 random numbers in the range of values of each of the 2 features\n",
    "X_new_1 = np.random.randint(acc_len_min, acc_len_max, size=(10, 1))\n",
    "X_new_2 = np.random.randint(cus_se_ca_min, cus_se_ca_max, size=(10, 1))\n",
    "print(X_new_1)\n",
    "print(X_new_2)\n",
    "X_new = np.concatenate([X_new_1, X_new_2], axis=1)\n",
    "print(X_new)\n",
    "\n",
    "pred = model.predict(X_new)\n",
    "print(pred.reshape(10,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_name = \"churn\"\n",
    "X = churn_df.drop(y_name, axis=1).values\n",
    "y = churn_df[y_name]\n",
    "\n",
    "test_size = 0.3\n",
    "random_state=21\n",
    "x_train, x_test,y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "print(\"PRINT MODEL's ACCURACY\")\n",
    "print(knn.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best hyperparameter for the model:\n",
    "knn = None\n",
    "k_test_acc = {}\n",
    "k_train_acc = {}\n",
    "for k in range(1, 25):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "    k_test_acc[k] = knn.score(x_test, y_test)\n",
    "    k_train_acc[k] = knn.score(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "sales_df = pd.read_csv(\"Utility_files/datasets_course1_DC/advertising_and_sales_clean.csv\")\n",
    "\n",
    "X = sales_df.drop([\"sales\", \"influencer\"], axis=1).values\n",
    "y = sales_df[\"sales\"].values\n",
    "\n",
    "test_size=0.3\n",
    "random_sate=42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "# evaluating the model\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# R^2 is the default metric for Linear regression\n",
    "r_squared = reg.score(X_test, y_test)\n",
    "rmse = mean_squared_error(y_pred, y_test, squared=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATION\n",
    "Dividing the data into training and test data might not be enough for securing a good performance. A more robust evaluation is the Cross validation or more technically the $K$ fold crossvalidation as follows:\n",
    "1. divide the data in $K$ sets\n",
    "2. At each time use one fold as testing set and the rest $K - 1$ ensembled as training dataset\n",
    "3. calculate the metric of interest with each fold, and having a final performce metric by applying statistical method to the ensemble of metrics obtained in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ayhem18/Ayhem18/DEV/Data_science/Towards_Data_science/Programming_Tools/Sklearn/Sklearn_1.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Towards_Data_science/Programming_Tools/Sklearn/Sklearn_1.ipynb#ch0000008?line=4'>5</a>\u001b[0m kf \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39mn_splits, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Towards_Data_science/Programming_Tools/Sklearn/Sklearn_1.ipynb#ch0000008?line=5'>6</a>\u001b[0m solver \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Towards_Data_science/Programming_Tools/Sklearn/Sklearn_1.ipynb#ch0000008?line=6'>7</a>\u001b[0m reg \u001b[39m=\u001b[39m LinearRegression(solver\u001b[39m=\u001b[39msolver)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Towards_Data_science/Programming_Tools/Sklearn/Sklearn_1.ipynb#ch0000008?line=7'>8</a>\u001b[0m cv_scores \u001b[39m=\u001b[39m cross_val_score(reg, X, y, cv\u001b[39m=\u001b[39mkf)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayhem18/Ayhem18/DEV/Data_science/Towards_Data_science/Programming_Tools/Sklearn/Sklearn_1.ipynb#ch0000008?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(cv_scores)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "n_splits = 6\n",
    "random_state = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "reg = LinearRegression()\n",
    "cv_scores = cross_val_score(reg, X, y, cv=x)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "The regularization techniques can be used to prevent overfitting. Yet, with the wrong hyperparameters it can have a negative side effect. Here is a live demo in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge # This molde is regularized linear regression with L2 method\n",
    "# test for different values of alphas\n",
    "\n",
    "alpha_hyper = [10 ** exp for exp in range(-4, 5)]\n",
    "\n",
    "ridge_scores = []\n",
    "for alpha in alpha_hyper:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    ridge_scores.append(ridge.score(X_test, y_test))\n",
    "\n",
    "print(ridge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso # this model is regularized linear regression with the sum of absolute values of parameters\n",
    "\n",
    "lasso = Lasso(alpha=0.3)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_coeff = lasso.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df = pd.read_csv(\"Utility_files/datasets_course1_DC/diabetes_clean.csv\")\n",
    "print(db_df.head())\n",
    "X = db_df.drop(\"diabetes\", axis=1).values\n",
    "y = db_df[\"diabetes\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_pred, y_test)) # this is wrong !!\n",
    "print(classification_report(y_pred, y_test)) # this is wrong !! \n",
    "# it should be as follows:\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred)) # the ground-truth data is passed before the predictions\n",
    "print(classification_report(y_test, y_pred))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning.\n",
    "It can be done using two main approaches. Either GridSearch:setting the possible values for the hyperparameters and trying all the different combinations. Yet, this might not be optimal with a large number of parameters: Thus we use Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_grid = {\"alpha\": np.array([10 ** i for i in range(-4, 4)])}\n",
    "n_splits = 6\n",
    "random_state = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "lasso_cv = GridSearchCV(lasso, params_grid, cv=kf)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "print(lasso_cv.score(X_test,y_test))\n",
    "print(lasso_cv.best_score_)\n",
    "print(lasso_cv.best_params_)\n",
    "\n",
    "# the same principle can be applied with RandomSearchCV.\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy_Variables\n",
    "In order for the sklean package to function properly, all inputs must be numerical. The general approach to convert categorical variables it to expand the possible values and convert them to binary variables while dropping the initial categorical variable. In other words: assume a categorical variables has the following values: $[v_1, v_2..., v_n]$ Then $v_i$ will be converted into a binary variable where usually $1$ will mean that the input vector satisfies that value and $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = pd.read_csv(\"Utility_files/datasets_course1_DC/music_clean.csv\").iloc[:, 1:]\n",
    "print(music_df.head())\n",
    "music_dummies = pd.get_dummies(music_df[\"genre\"], drop_first=True)\n",
    "print(music_dummies.head())\n",
    "# music_df = pd.concatenate(music_df, music_dummies, axis=1) # add the new \"binary\" variables to the original DataFrame\n",
    "# music_df.drop(\"genre\", inplace=True) # drop the old categorial column\n",
    "\n",
    "#### NOTE: if there is only one categorical column in a dataframe, then teh get_dummies will do all of the work for us: returning the correct\n",
    "# number of additional columns after conversion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing DATA\n",
    "It might be necessary to fill the missing data with some values out of the present data. we can use sklearn for effecting data imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# let's import the diabetes data\n",
    "music = pd.read_csv(\"Utility_files/datasets_course1_DC/music_clean.csv\")\n",
    "X_cat = music[\"genre\"].values.reshape(-1, 1) # since it is only one column\n",
    "X_num = music.drop([\"genre\", \"popularity\"], axis=1).values\n",
    "y = music[\"popularity\"].values\n",
    "\n",
    "random_state = np.random.randint(50)\n",
    "X_cat_train, X_cat_test, y_train, y_test = train_test_split(X_cat, y, random_state=random_state)\n",
    "X_num_train, X_num_test, y_train, y_test = train_test_split(X_num, y, random_state=random_state)\n",
    "\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "X_cat_train = imputer_cat.fit_transform(X_cat_train)\n",
    "X_cat_test = imputer_cat.transform (X_cat_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_num = SimpleImputer(strategy='mean') # the default btw\n",
    "X_num_train = imputer_num.fit_transform(X_num_train)\n",
    "X_num_test = imputer_num.transform(X_num_test)\n",
    "\n",
    "X_train = np.append(X_num_train, X_cat_train, axis=1)\n",
    "X_test = np.append(X_num_test, X_cat_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(music.isna().sum().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is possible to use the Pipeline model as well \n",
    "from sklearn.pipeline  import Pipeline\n",
    "# initialize an imputer\n",
    "\n",
    "num_imp = SimpleImputer()\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "steps = [(\"imputer\", num_imp), (\"logistic_regression\", log_reg)]\n",
    "\n",
    "pip = Pipeline(steps)\n",
    "pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pip.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centring and scaling\n",
    "Imputing values is not the sole preprocessing step. Data should be at the same range of values. There are two main approaches:\n",
    "1. normalization: subtract by min and divide by the range: all values would now range from $0$:minimum to $1$: maximum\n",
    "2. standarization: substract the mean and divide by the variance: the new data is of mean $0$ and variance $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = music.drop(\"genre\", axis=1).values\n",
    "y = music[\"genre\"].values\n",
    "random_state = np.random.randint(50)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "y_test_scaled = scaler.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08488e93894ea7be7272109919d40edb52233f14daf834f5f2387122a81730e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "In this Notebook, I will focus more on Linear models and their applications using sklearn tool kit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first example: uing Logitic regression and SVM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "digits = datasets.load_digits()\n",
    "print(type(digits.data))\n",
    "print(digits.data[0])\n",
    "\n",
    "data = digits.data\n",
    "target = digits.target\n",
    "\n",
    "# import scaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "# target_scaled = scaler.transform(target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, target)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "# we can either use predict()\n",
    "y_pred = lr.predict(X_test) # this will return the final classification determined by the treshhold\n",
    "y_prob_pred = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the different effects regularization have on Logistic Regression\n",
    "# we have our scaled data set to scaled_data and the target values set to target\n",
    "random_state = 23\n",
    "X_1, X_test, y1, y_test = train_test_split(data_scaled, target, test_size=0.2, random_state=random_state)\n",
    "# now X_1 represents both cross validation and training sets \n",
    "# let's divide them: As we want the validation set to represent 0.2 of the original dataset, we need 0.25 out of the (train+validatation) dataset\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_1, y1, test_size=0.25, random_state=random_state)\n",
    "\n",
    "log_reg = None\n",
    "# C_index represents the 1/lambda where lambda is the regularization parameter\n",
    "lambda_reg = np.random.rand(20) * 0.100005 # 20 random values that belong to the interval [0, 0.15[\n",
    "lambda_reg = np.sort(lambda_reg)\n",
    "print(lambda_reg)\n",
    "val_errors = []\n",
    "train_errors = []\n",
    "models = []\n",
    "solver = 'liblinear'\n",
    "for c in lambda_reg:\n",
    "    log_reg = LogisticRegression(C=1 / c, solver=solver).fit(X_train, y_train) # setting the regularization hyperparmeter\n",
    "    models.append(log_reg) # saving the model for later use \n",
    "    train_errors.append(1 - log_reg.score(X_train, y_train)) # saving the train errors\n",
    "    val_errors.append(1 - log_reg.score(X_val, y_val)) # saving the validation errors\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lambda_reg, train_errors,'-b', label='train error')\n",
    "ax.plot(lambda_reg, val_errors, '--r', label='validation error')\n",
    "leg = ax.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = np.argmin(val_errors)\n",
    "print(models[index_min].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cells above are using the default parameters: the l2: regularization using the sum of square errors.\n",
    "# the second penalty or regularization techniques employed with Linear models is \"l1\" which uses the sum of errors (abosulte differences)\n",
    "# \"L1\" is usually referred to as features selection while L2 is referred to as shrinkage.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', solver=solver)\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The multiclass classification\n",
    "## There are two main techniques to extend Logistic Regresison to multi-class classification problems\n",
    "## the first and default is One-vs-rest, where the model is trained to build k models, where each model tackles the bnary\n",
    "## classification problem (y==k) and then the final classification is the one with the highest confidence.\n",
    "\n",
    "## the second is the multinomial approach and tackles the problem directly: the cost function is modified to fit such purpose. \n",
    "\n",
    "# Fit one-vs-rest logistic regression classifier\n",
    "lr_ovr = LogisticRegression()\n",
    "lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# Fit softmax classifier\n",
    "lr_mn = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\") # requires setting the parameters as follows\n",
    "lr_mn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IT is important to note that Logistic Regression is not the idea tool for multi-class classification.\n",
    "## assuming we have classifier log_reg, if the classifier log_reg_class_k = log_reg.fit(X_train, y_train == k)\n",
    "## is classifying the class k poorly then the complete model would find great diffculties classifying any example correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "X = X[:, :2] # consider only the two first features\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X, y)\n",
    "\n",
    "print(\"total number of samples {}\".format(len(X)))\n",
    "print(\"number of support vectors {}\".format(len(svm.support_)))\n",
    "\n",
    "print(svm.support_)\n",
    "X_small = X[svm.support_, :]\n",
    "y_small = y[svm.support_]\n",
    "\n",
    "svm_small = SVC(kernel='linear').fit(X_small, y_small)\n",
    "\n",
    "X_random = np.random.rand(100, 2)\n",
    "\n",
    "y_pred = svm.predict(X_random)\n",
    "y_pred_small = svm.predict(X_random)\n",
    "print((y_pred == y_pred_small).all())\n",
    "\n",
    "# the support vector machine learns the same decision boundaries for the two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the non-linear are more powerful than linear ones. One way to think about the RBF SVM (the default setting) is that it conducts complex\n",
    "## transformations on the linear data.T\n",
    "\n",
    "# # Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]} # this parameter determines the complexity of the boundary: \n",
    "# larger gamma values mean more sensitivity, and large possibility of overfitting.\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X, y)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV params {'penalty': 'l2', 'loss': 'log_loss', 'alpha': 1e-05}\n",
      "Best CV accuracy 0.9414676358601592\n",
      "Test accuracy of best grid search hypers: 0.95\n"
     ]
    }
   ],
   "source": [
    "# it is crucial to point out that Sklearn offers an additional class called  SGDClassifier. This is a linear classifier that used stochastic\n",
    "# gradient descent as its main optimization algorithm which scaled quite well with large datasets by design.\n",
    "# we can have SVM and LogReg models that use SGD as their optimization algorithm using this class.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# svm with SGD = SGDC(loss = 'hinge')\n",
    "# LogReg with SGD = SGCD(loss = 'log')\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "             'loss':[\"hinge\", \"log_loss\"], 'penalty':[\"l1\", \"l2\"]}\n",
    "searcher = RandomizedSearchCV(linear_classifier, parameters, cv=10)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08488e93894ea7be7272109919d40edb52233f14daf834f5f2387122a81730e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

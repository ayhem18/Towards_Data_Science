{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "## 1. Introduction\n",
    "Pandas is the number one Python-tool kit for data analysis and manipulation.\n",
    "## 2 Series data structure\n",
    "### 2.1 Creating Series\n",
    "The series data structure can be seen as a one dimentional array wihtin the Pandas library. It built upon the numpy array which boosts performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [2 ** 5, 2**6, 2 ** 7]\n",
    "pd.Series(numbers) # pandas infer the type automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One desirable feature of Pandas is type deduction. if a sequence like object is passed to pd.Series(), then the ***None*** type would be converted to a special float value: ***Nan***: not a number if the elements are numerical.\n",
    "\n",
    "It is important to keep in mind that the value ***Nan*** is not, by any means, equivalent to the value ***None***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nan == None) \n",
    "print(np.nan == np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value ***Nan*** is built differently in the computer for efficiency reasons. It can be tested using the np.isnan() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to pandas Series, it is a mixture of lists and dictionaries. the indexes can be either numbers or other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([5,6,7,8,9])\n",
    "s2 = pd.Series({\"0\": 1, \"1\": 2, \"2\": 3, \"3\": 4})\n",
    "\n",
    "print(s1)\n",
    "# print(\"#########################################################################################\")\n",
    "print(s2)\n",
    "print(s2[\"1\"])\n",
    "print(s2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_index = [\"sxx\", \"swz\", \"mmu\"]\n",
    "s3 = pd.Series([3,4,5], index=s3_index) # the length of the parameters and the passed index must match\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\"A+\": 95, \"A\": 90, \"B\": 75, \"C\": 60, \"D\": 50} # we can see that the index keyword argument overwrites the dictionary construction operator\n",
    "s = pd.Series(scores, index=['A', \"B+\", 'B', 'C', 'D', 'F'])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Querying Pandas Series\n",
    "creating the series is one step in the data analysis progress. Querying is the real deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s[3] == s['C'])\n",
    "print(s.iloc[3] == s[3] == s.loc['C'] == s['C'])\n",
    "# the safest option is to use the loc and iloc operators for example\n",
    "s_exp = pd.Series({99:\"oh\", 100:\"shit\", 101:\"damn\"})\n",
    "try:\n",
    "    s_exp[0]\n",
    "except KeyError:\n",
    "    print(\"You should use the iloc attribute. Here give it a try:\")\n",
    "    print(s_exp.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to keep in mind that built-in methods make use of vectorization which enables parallelly distributed processing leading to dramatic speed ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 100\n",
    "# this Python feauture is called magic functions: it will run the cell on many iterations and estimate the average execution time\n",
    "\n",
    "series = pd.Series(np.random.randint(0,5000, 1000))\n",
    "## The slow approach\n",
    "total = 0\n",
    "for val in series:\n",
    "    total += val\n",
    "print(total / len(series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 100\n",
    "# the vectorized: fast approach\n",
    "print(series.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the same principle, we can manipulate the series as if they were numpy array to make use of the vectorization\n",
    "n = np.random.randint(-500, 500)\n",
    "print(n)\n",
    "print(series.head(10)) # print the first 10 elements\n",
    "series_plus_n = series + n \n",
    "print(series_plus_n.head(10))\n",
    "series_plus_n -= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the other approach would be to iterate and set the values manually: the slow approach\n",
    "for label, value in series_plus_n.iteritems():\n",
    "    series_plus_n.loc[label] = value + n\n",
    "print(series_plus_n.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 DataFrame\n",
    "pd.DataFrame is the core of the pandas' framework.\n",
    "### 3.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name Class  Score\n",
      "s1    a   Phy     85\n",
      "s2    b  Chem     90\n",
      "s3    c   Phy     88\n",
      "\n",
      "HERE IS AN IDENTICAL DATAFRAME\n",
      "\n",
      "   Name Class  Score\n",
      "s1    a   Phy     85\n",
      "s2    b  Chem     90\n",
      "s3    c   Phy     88\n"
     ]
    }
   ],
   "source": [
    "## out of series\n",
    "s1 = pd.Series({\"Name\": 'a', \"Class\": \"Phy\", \"Score\": 85})\n",
    "s2 = pd.Series({\"Name\": 'b', \"Class\": \"Chem\", \"Score\": 90})\n",
    "s3 = pd.Series({\"Name\": 'c', \"Class\": \"Phy\", \"Score\": 88})\n",
    "df1 = pd.DataFrame([s1, s2, s3], index=['s1', 's2', 's3'])\n",
    "print(df1)\n",
    "# pandas dataframe can be made out of pandas series: each constituting a row \n",
    "\n",
    "## directly from list of dictionaries\n",
    "print(\"\\n\",\"\\n\",sep=\"HERE IS AN IDENTICAL DATAFRAME\")\n",
    "df2 = pd.DataFrame([{\"Name\": 'a', \"Class\": \"Phy\", \"Score\": 85}, \n",
    "{\"Name\": 'b', \"Class\": \"Chem\", \"Score\": 90}, \n",
    "{\"Name\": 'c', \"Class\": \"Phy\", \"Score\": 88}], index=['s1', 's2', 's3'])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Accessing and dropping columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Having the series initialization approach in mind, We can see how the loc and iloc operators work.\n",
    "s4 = df1.loc['s1'] # it will return a row\n",
    "print(s4)  \n",
    "try: \n",
    "    df1.loc['Name']\n",
    "except:\n",
    "    print(\"WE can't use loc exclusively with columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to select based on columns:\n",
    "name_col = df1['Name'] # pd.Series object\n",
    "print(type(name_col))\n",
    "name_col_df = df1[['Name']] # pd.DataFrame object\n",
    "print(type(name_col_df))\n",
    "mul_col = df1[['Name', 'Class']] # it can only be a dataFrame object\n",
    "print(mul_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best approach is to use loc and iloc\n",
    "print(df1.loc[['s1', 's2'], ['Name', 'Score']], \"\\n\") # the names and scores of s1 and s2\n",
    "print(df1.loc[:, 'Name'], type(df1.loc[:, 'Name']), sep=\"\\n\") # the names of all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting can be tricky\n",
    "df_copy = df1.copy() # making a copy\n",
    "\n",
    "df_copy.drop(\"s1\", inplace=True, axis=0) # axis=1 means the columns, axis=0 indicates the rows\n",
    "print(df_copy)\n",
    "# adding is just simple\n",
    "df_copy['another_col'] = [1,2] # the length of the column should match the length of the index attribute\n",
    "print(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Loading and Indexing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading  data\n",
    "df = pd.read_csv(\"utility_files/titatic_comp_train.csv\", index_col=0) # setting the id to be the index\n",
    "print(df.head())\n",
    "# certain columns might not be of lcear meaning , thus renaming them might reveal necessary\n",
    "print(\"\\n\", \"\\n\", sep=\"After name modifiction\")\n",
    "new_df = df.rename(columns={\"PassengerId\": \"\", \"SibSp\": \"num_siblings_spouses\", \"parch\": \"num_parent_child\"})\n",
    "print(new_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the name change only if the values are identical in the dictionary. Thus, it might easily raise errors\n",
    "# a better approach is to apply a certain function on each column name: stripping white spaces, convert to lower case, upper case\n",
    "# or even capatilizing\n",
    "\n",
    "new_df  = new_df.rename(mapper=str.strip, axis=1)\n",
    "print(new_df.head())\n",
    "# we can see through that approach that only one function can be applied at a time, which might not be efficient\n",
    "print(\"\\n\", \"\\n\", sep=\"The better approach\")\n",
    "mapper = {\"Sibsp\": \"Num_siblings_spouses\", \"Parch\": \"Num_parent_child\"}\n",
    "new_cols = [col.strip().lower().capitalize() for col in new_df.columns]\n",
    "new_cols = [mapper[col] if col in mapper else col for col in new_cols]\n",
    "new_df.columns = new_cols\n",
    "print(new_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Querying data \n",
    "Among the most important data manipulation techinques in the boolean masking applied both to pandas data structures and numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose only the passengers from the 2nd or upper classes\n",
    "non_poor_mask = new_df['Pclass'] <= 2\n",
    "# print(non_poor_mask)\n",
    "non_poor_df = new_df.where(non_poor_mask)\n",
    "print(non_poor_df.loc[:,[\"Pclass\"]]) \n",
    "# the rows that do not meet the condition set by the boolean mask are not dropped out of the table.\n",
    "# instead they are set as Nan (for numerical values) and None for Object typed values\n",
    "# Thus a finalized command is:\n",
    "non_poor_df = new_df.where(non_poor_mask).dropna()\n",
    "print(non_poor_df.loc[:, [\"Pclass\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More systactic sugar:\n",
    "non_poor_df_2 = new_df[new_df['Pclass'] <= 2]\n",
    "print(non_poor_df_2.loc[:, [\"Pclass\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous examples considered only a single filtering condition. This scenarion is quite unlikely in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_poor_with_relatives_df = new_df[(new_df['Pclass'] <= 2) & (new_df['Num_siblings_spouses'] > 0)] \n",
    "# for more than one condition the syntax might end up a bit more complicated\n",
    "print(non_poor_with_relatives_df.loc[:, ['Pclass', 'Num_siblings_spouses']])\n",
    "# it might be a good idea to use an independent boolean mask for complicated conditions the pass it directly to the DataFrame in question\n",
    "rich_with_no_relatives_mask = (new_df[\"Pclass\"] == 1) & (new_df['Num_siblings_spouses'] == 0) # the parentheses are needed \n",
    "print(new_df[rich_with_no_relatives_mask].loc[:, [\"Pclass\", 'Num_siblings_spouses']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can set the index of the DataFrame either from initialization by setting the corresponding values for the parameters \n",
    "# or later using a number of methods\n",
    "\n",
    "df = pd.read_csv(\"utility_files/titatic_comp_train.csv\") \n",
    "# print(df.head())\n",
    "df2 = df.set_index(\"PassengerId\")\n",
    "print(df2.head(), \"\\n\", sep=\"\\n\")\n",
    "\n",
    "df3 = df2.reset_index() # now the index is the default numerical series starting from 0\n",
    "print(df3.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can have composite indices as follows:\n",
    "df4 = df.set_index(['Pclass', 'PassengerId'])\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Missing Data\n",
    "Pandas.DataFrame is equipped with efficient features to handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the data set is already clean, let's set some values to be None\n",
    "n_rows = len(df.index)\n",
    "n_cols = len(df.columns)\n",
    "\n",
    "for _ in range(400):\n",
    "    df.iloc[np.random.randint(0, n_rows), np.random.randint(0, n_cols)] = None\n",
    "# we can create a mask for missing values as follows:\n",
    "is_null_mask = df.isnull()\n",
    "# print(is_null_mask.head(100))\n",
    "\n",
    "no_na_df = df.dropna()\n",
    "# filling the missing values instead of remvoving them\n",
    "df_ffill = df.fillna(method='ffill') # this method fills na values with the directly next valid one\n",
    "print(df_ffill) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the method ***pd.fillna()*** it is better to refer to the [documention](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) \n",
    "\n",
    "This [link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) is for the ***pd.replace()*** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\n",
    "obj1 = pd.Series(sdata)\n",
    "print(obj1)\n",
    "states = ['California', 'Ohio', 'Oregon', 'Texas']\n",
    "obj2 = pd.Series(sdata, index=states)\n",
    "print(obj2)\n",
    "obj3 = pd.isnull(obj2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = obj2['California']\n",
    "print(obj2['California'] != x)\n",
    "\n",
    "print(obj2['California'] == None) \n",
    "print(obj3['California'])\n",
    "math.isnan(obj2['California'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = pd.DataFrame(np.array([[1,2],[2,3]]))\n",
    "df_np = pd.DataFrame(pd.Series([1,2,3]))\n",
    "print(df_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(\"Name\", axis=1)\n",
    "\n",
    "s4 = pd.Series(['d', 'Shit', 120])\n",
    "s5 = pd.Series(['e', 'Shit2', 106])\n",
    "s6 = pd.Series(['f', 'Shit3', 114])\n",
    "s7 = pd.Series(['g', 'Shit4', 112])\n",
    "s4 = pd.Series()\n",
    "df2 = pd.DataFrame([s4, s5, s6, s7], columns=[\"Name\", \"Class\", \"Score\"])\n",
    "print(df2)\n",
    "df1 = pd.concat([df1, df2],ignore_index=True, axis=0)\n",
    "print(df1)\n",
    "print(df1[df1['Score'].gt(105) & df1['Score'].lt(115)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

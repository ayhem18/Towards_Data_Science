{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a DataFrame\n",
    "* df.info()\n",
    "* df.describe()\n",
    "* df.head()\n",
    "* df.shape // attribute not a method\n",
    "* df.values: numpy array \n",
    "* df.columns\n",
    "* df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the following data sets\n",
    "homelessness_url = \"https://assets.datacamp.com/production/repositories/5386/datasets/1a0ab2e8557930ec06473c16521874e516a216ae/homelessness.csv\"\n",
    "avocado_dataset = \"https://assets.datacamp.com/production/repositories/5386/datasets/5528f46cc712c9083a6881f787fc9b34ab53d5ea/avoplotto.pkl\"\n",
    "temperature = \"https://assets.datacamp.com/production/repositories/5386/datasets/47f5fde162bae3549ca7d5c26fb4c4639f100f28/temperatures.csv\"\n",
    "walmart_sales = \"https://assets.datacamp.com/production/repositories/5386/datasets/5110afec30fc30bc5f3cf67b188d1513c3d6d940/sales_subset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeless_df = pd.read_csv(homelessness_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sorting\n",
    "we can use the df.sort_values(column_name): sorts the dataframe by the column values. Check the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homeless_df.head())\n",
    "# homeless_df = homeless_df.iloc[:, 1:]\n",
    "# print(homeless_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeless_df = homeless_df.iloc[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homeless_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can select multiple columns\n",
    "print(homeless_df[[\"state\", \"state_pop\"]]) \n",
    "# this is equivalent to\n",
    "print(homeless_df.loc[:, [\"state\", \"state_pop\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider statistics. Pandas has functions for both summary and accumulative statistics\n",
    "# pandas had built-in functions such as median, mean, max, min and a large number of routine statistical procedures\n",
    "# sometime we want more, right ?\n",
    "def iqr(col):\n",
    "    return col.quantile(0.75) - col.quantile(0.25)\n",
    "\n",
    "sales = pd.read_csv(walmart_sales).iloc[:, 1:]\n",
    "print(sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales['weekly_sales'].agg(iqr))\n",
    "print(sales.drop(['is_holiday', 'type', 'date'], axis=1).apply(iqr, axis=0)) # apply the custom functions tovery column in the data frame exceppt the non numerical ones \n",
    "print(sales.drop(['is_holiday', 'type', 'date'], axis=1).agg([np.mean, np.median])) # agg is a great choice as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "there are different ways to aggregate a pandas table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the old way\n",
    "print(sales.head())\n",
    "print(sales.groupby(['type'])['weekly_sales'].agg(np.mean))\n",
    "print(\"#\" * 100)\n",
    "print(sales.groupby(['type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another great way to aggregate is to use the pivot table thingy\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0.0, margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting might be tricky when done manually. Pandas offers a number of built-in functions to perform this type of tasks\n",
    "\n",
    "## dropping duplicated\n",
    "# non_double = df.drop_duplicates(subset=[\"column to consider for uniqueness\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices are also a tricky topic as things might get twircky really easily\n",
    "\n",
    "print(sales.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocados = pd.read_pickle(avocado_dataset)\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(avocados.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = avocados.groupby(\"size\")['nb_sold'].agg(sum)\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.plot(kind='bar')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = avocados.groupby('date')['nb_sold'].agg(sum)\n",
    "\n",
    "# Create a line plot of the number of avocados sold by date\n",
    "nb_sold_by_date.plot(kind='line', x='date', y='nb_sold', rot=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of avg_price vs. nb_sold with title\n",
    "avocados.plot(kind='scatter', x='nb_sold', y='avg_price', title=\"Number of avocados sold vs. average price\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering joins:\n",
    "Filtering joins are special kinds of join that are not natively supported by Pandas. Yet, can be generated by additional manipulation.\n",
    "### Semi joins\n",
    "Semi joins are sementically quite similar to inner joins. Yet with two main differences:\n",
    "* returns only the columns from the left table\n",
    "* no duplicates even with one to many relationship\n",
    "### anti-joins\n",
    "Anti joins returns:\n",
    "* the left table, excluding the intersection(inner join)\n",
    "* returns only the columns from the left table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenating dataframes\n",
    "df1, df2, df3\n",
    "pd.concatenate([df1, df2, df3], sort=True, ignore_index=True, join='inner', keys=[k1, k2, k3]) \n",
    "\n",
    "# sort the column by names, \n",
    "# sets a uniform index from 0 to n-1,\n",
    "# use only columns common in the passed dataframes\n",
    "# keys, make the index composite, cannot use with ignore_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pandas merge function offers additional functionality.\n",
    "pd.merge(df1, df2, validate='one_to_one') # the keyword argument validate can be set to ['ono_to_one','one_to_many'...]\n",
    "# if the dataframes do not follow this relation, then an error is raise\n",
    "# a similar function is provided in the concatenate function\n",
    "\n",
    "pd.concatenate([dfs], verify_integrity=True) # this will raise an error if the resulting dataframe contains duplicate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use merge_ordered when the order of the rows matter.\n",
    "sp500 = \"https://assets.datacamp.com/production/repositories/5486/datasets/6666955f71f936ab5fc3b0ee1eb595e19c126c01/S&P500.csv\"\n",
    "sp = pd.read_csv(sp500)\n",
    "world_bank = \"https://assets.datacamp.com/production/repositories/5486/datasets/6ef405912a3801f3ae59d2dd57573f80d598c1fb/WorldBank_GDP.csv\"\n",
    "gdp = pd.read_csv(world_bank)\n",
    "print(sp.head())\n",
    "print(gdp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge_asof(df1, df2) # functions almost exactly as the usual merge but with a slight twick to it.\n",
    "# it will match to the closest value in the right depeding on the direction.\n",
    "pd.merge_asof(df1, df2, direction='forward') # match with the closest value that is greater or equal\n",
    "pd.merge_asof(df1, df2, direction='backward') # math with the closest value that is smaller or equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Long formats: pd.melt()\n",
    "Tabular data can generally be stored in 2 different ways:\n",
    "1. The common, ***WIDE*** format:   \n",
    "    * each column represents a feature, qualilty, aspect of the subject in question\n",
    "    * each row represents an instance of the subject in question.\n",
    "2. The ***LONG*** format:\n",
    "    * certain columns would represents features while the other are denoted as ***$variable_i$*** adjacent to another column denoted by ***$value_i$***. An instance of a subject can appear in different rows.\n",
    "\n",
    "Even though the wide format might be more understandable by humans, the Long format is easier to work with for computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006414dea9a04848ce797b510a25f3f28ac8668e3d3244e777242cca6bed477f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

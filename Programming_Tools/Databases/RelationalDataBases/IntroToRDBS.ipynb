{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Relational Databases\n",
    "This jupyter notebook is created to save the notes from the ***Introduction to Relational Databases*** course from the ***IBM Data Engineer*** specialization offered on Coursera."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of data\n",
    "* structured: follows a strict / rigid schema that can be organized into columns and rows.\n",
    "* semi-structured: the data still conforms to a certain structure, and follows certain patterns. Nevertheless, it cannot be stored in form of a table\n",
    "* unstructured: this data is characterized by its complexity. It does not respect any specific schema, rules or semantics. It is generally stored in NoSQL databases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information and Data models\n",
    "* information models: are designed on a conceptual level: an abstract representation of an idea or a system. It generally describes the relationships between the different components\n",
    "* data models: are concrete implementations of information models. They are specific, detailed and implementation oriented. \n",
    "* The most common data model is the relational model. The latter organizes data into a simple data-structure: tables. Such choices achieves:\n",
    "    1. logical independence: every logical block can uniquely represented by its own entity (table)\n",
    "    2. physical independence: \n",
    "    3. physical storage independence.\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## Relationships\n",
    "* The relational model can be also seen as a entity-relation model. An entity represents an object, component of our system existing independently of any other object. A good data model would consider entities as irreductible, building blocks of the system in question. The system will represent a complex set of relations between these irreductible essential components.\n",
    "* An attribute on the other hand represents a specific characteristic, information, associated with an entity. An attribute informs about the entity and only exists with in the entity. \n",
    "* In the relation model language, an ***ENTITY*** is a ***TABLE*** and an ***ATTRIBUTE*** is a ***COLUMN*** in that table. \n",
    "### Relationship Components\n",
    "* an entity is represented with a rectangle with attributes represented as ovales linked to the entity's rectangle.\n",
    "* The crow's foot notation is pretty simple. A thorough explanation can be found in this [link](https://vertabelo.com/blog/crow-s-foot-notation/)\n",
    "* Most relation data types use data-typing. Having specific, predetermined data types offer a number of advantages:\n",
    "    1. data integrity: the correctness of the data can be verified\n",
    "    2. correct sorting, range collection\n",
    "    3. calculations on numerical data\n",
    "* a table can be formally referred to as ***relation***. A relation is characterized by:\n",
    "    1. degree: the number of attributes\n",
    "    2. cardinatily: number of tuples / rows\n",
    "* a relation schema: is a conceptual design of the an entity. A relation instance is a concrete implemented table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases Architectures\n",
    "A database architecture can be generally categorized according to the number of physical sytems (layers) involved overall.\n",
    "* 1-tier architectude: where the database is in the same system  as the application (generally a computer). It is generally used for development/testing purposes and is generally limited for very few users.\n",
    "* 2-tier: as you have expected, it is spread accross 2 systems and is generally refered to as client-server system. The application can be written in any programming language. It connects to an API (Generally PL-dependent), that creates a database client. The latter connects to the database server. On a high level, the latter can be broken into 3 main components or layers:\n",
    "    1. database access layer: controlling the accessing to the data stored in the system\n",
    "    2. database engine: compling the queries and returning the results\n",
    "    3. the actual physical storage: where the actual data is stored.\n",
    "\n",
    "* For security and performance reasons, the database is generally no longer accessed directly by users with the exception of administrators. A middle layer referred to as the Application Layer contains the business logic as well as a database driver to connect to the database server. The application tier is reduced (in this topology) to a mere interface. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For high availability and performance, the traditional one-server database architecture was abandoned as it sets performance limitations, single point system failure... \n",
    "* Distributed storage systems are more common nowadays. For example:\n",
    "    1. shared disk architectures: where the physical storage is still stored in a single physical point while having multiple servers capable of accessing it. This enables higher throughput as many clinets can require operations on the database systems in the same time. A database server going down is no longer an issue as a client can be quickly rewined to another server.\n",
    "    2. Replication: the physical stroage is replicate across different servers. This increase throughput while also decreasing latency. Nevertheless, it introduces consistency and concurrency problems: performance and data-integrity tradeoffs. REplicas can be geographically close: high availability replicas or physically distant to account for the large scale disasters: disaster recovery replica.\n",
    "    3. sharding and parititioning: dividing the data storage into multiple nodes (servers/storages) each equipped with its own computational resources. With the proper clustering of data: A significant increase in performance can be achieved while introducing parallelism for more complex and demanding queries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL statements\n",
    "1. DDL: data definition language: define, change, or remove SQL objects. They are mainly:\n",
    "* CREATE\n",
    "* ALTER\n",
    "* TRUNCATE (drop columns)\n",
    "* DROP (drop a table)\n",
    "2. DML: data manipulation language: read, modify data. They are also referred to as CRUD operations: (CREATE, READ, UPDATE, DELETE)\n",
    "* insert (Add a column)\n",
    "* select (select certain columns)\n",
    "* UPDATE (modify)\n",
    "* DELETE (delete rows)\n",
    "## CREATING TABLES:\n",
    "* can be done through GUIs (for noobies), administrative APIS, and using SQL create statements (generally though scripts)!!\n",
    "* let's consider the CREATE statement:\n",
    "<br/>\n",
    "CREATE table_name(  \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; col_name_1 data_type_1 optional_params,   \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; col_name_2 data_type_2 optional_params,  \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; col_name_3 data_type_3 optional_params  \n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Altering Tables\n",
    "* The alter operations consist of several sub-operations:\n",
    "1. adding columns:  \n",
    "&nbsp;&nbsp; ALTER TABLE table_name   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ADD COLUMN col_name_1 type_1,  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ADD COLUMN col_name_2 type_2,  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ADD COLUMN col_name_3 type_3  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ADD COLUMN col_name_4 type_4;  \n",
    "\n",
    "2. changing the data type of a certain column:  \n",
    "&nbsp;&nbsp; ALTER TABLE table_name   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; ALTER COLUMN col_name SET DATA TYPE new_data_type;\n",
    "\n",
    "3. dropping an existing column:  \n",
    "&nbsp;&nbsp; ALTER TABLE table_name   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; DROP COLUMN col_name ;\n",
    "\n",
    "4. DROP TABLE table_name; \n",
    "5. TRUNCATE TABLE table_name IMMEDIATE;: this command deletes all the data in a table (while keeping the data types)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movement utilities\n",
    "Data engineers often move data around for several reasons:\n",
    "* populate databases\n",
    "* create copys for development and testing purposes\n",
    "* save a snapshot of a database for disaster recovery\n",
    "* create new table from external data sources\n",
    "* add entries to existing tables  \n",
    "<br/>\n",
    "The data movement can be broadly classified into 3 main procedures:\n",
    "* backup and restores, the first one being converting the entire database to a file or a set of file encapsulating the entire database while the 2nd is the same operation in the opposite direction \n",
    "* import and export: the first is insert data from file to a table, while the second is saving a number of selected rows into a target file\n",
    "* load table is a faster, less reliable version of import: The latter is executed as a series of insert statements preserving data integrity by performing a number of checks on the types and the constraints. the Load operations are more efficiet performance-wise as they write directly to the physical storage skipping the different checks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database hierarchy\n",
    "A Relational database system consists of the following:\n",
    "* instance\n",
    "* * database\n",
    "* * * schema\n",
    "* * * * database objects \n",
    "\n",
    "1. instance: represents a logical boundary of databases, configurations and objects. It can be thought of a set (groop) of isolated non-iteracting databases. Not all RDBs use the concept of instance.\n",
    "2. a database is the actual storage system of technological system. Each database has its own name, configurations files and tables: database server environment\n",
    "3. a schema (generally) is a database object that allow logical grouping of different database objects. the combination of the schema's name and the table's name represent a unique identifier for a database table. Some of the relational database create system schema storing meta data about the configurations of the database, schema, users, their access rights...\n",
    "4. database objects such as tables, constraints, indexes, views, aliases...\n",
    "* Primary keys are used to uniquely identify rows in a database\n",
    "* indexes are used to increase performance, eliminate the need to sort the data and guarantee uniqueness. On the other hand, they occupy disk space as well as decrease the performance on **INSERT**, **UPDATE**, **DELETE** queries (as each of those will not only modify the rows but also the indexes themselves) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "* normalization can be considered the process of eliminating redundancy and duplicated data.\n",
    "* Redundancy elimination comes with the cost of introducing larger number of tables: more complexity. A table is said to be normalized when it is normalized to the required level. A database is said to be normalized when each of its tables is normalized.  \n",
    "1. NF1  \n",
    "_short for normal form 1, and have 4 basic rules:  \n",
    "_ each column must contain atomic values  \n",
    "_ each column must save data of the same type.  \n",
    "_ each column must have a unique name  \n",
    "_ the order of columns should not affect data management.  \n",
    "2. NF2  \n",
    "_ a dependency or slightly fancier: functional dependency is the fact of having a column value determined solely by value(s) in other column(s).  \n",
    "_ the columns determining the other values usually constitute a primary key. In tables having composite keys (mainly table representing relationships and not strong entities) \n",
    "Having a column depending only one super key is called partial dependency. The latter should be avoided.  \n",
    "3. NF3  \n",
    "_ All columns in a table must depend solely on the primary key.  \n",
    "_ if a column depends on one column that itself depends on the primary key, then the table suffers from transitional dependency.  \n",
    "4. BCNF  \n",
    "_ NF3  \n",
    "_ a non-prime key cannot determine a super key: either a primary key or column that belongs to the compound primary key.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constraints\n",
    "There are 6 main constraints defined as follows:\n",
    "1. entity integrity constraint: the primary key (one or many super keys) should be unique identifiers of each tuple / row.\n",
    "2. referential integrity constraint: the foreign key should refere a primary key of at least one existing tuple\n",
    "3. semantic integrit constraint: correctness of the meaning of the data\n",
    "4. domain constraint: determine permissible values for a given attribute\n",
    "5. null constraint: specifying attributes that cannot be null\n",
    "6. check constraint: limit the values accepted by an attribute."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Database Design\n",
    "A good database design is a crucial for any data-driven project. A database design process can be broken into $3$ major steps:\n",
    "1. requirements analysis\n",
    "2. logical design\n",
    "3. physical design\n",
    "\n",
    "### Requirement analysis\n",
    "This step is mainly about understanding the system to implements, its different components, the relationships betweem them. In other words, identifying the building blocks of the system and their interactions. Make sure to use any previous database, either electronic or paper-based. The latter provides a better understanding of the data: its values, it usages and etc. The previous work should not represent a basis for the new database design. They are concrete examples of the data usage. Furthermore, interviewing the current customers as well as potential customers is a great source of insight. The output of this phase should be a mean to validate the designer's understanding of the system when shared with the stakeholders.\n",
    "### Logical Design\n",
    "* The objects identified during the previous stage are mapped to entities, relationships and attributes. Generally, objects are people, things, locations or events. An object not fitting into any of these categories should be reconsidered as it might be an attribute characterizing a different entity. \n",
    "* Assuming the entities were already defined, it is time to consider attributes. Even though the requirement analysis indicates the presence of certain composite fields, it is recommended to break them into smaller, atomic field providing more sorting options: \n",
    "$\\begin{align}\n",
    "name \\to first~name~and~last~name \\\\\n",
    "address \\to ~street~and~city~district\n",
    "\\end{align}$\n",
    "* Assuming the attributes were laid down proerly, the relationships between different entities should be now considered. A many to many relationship should genreally be avoided. A many to many relationship should always be modeled by an separate entity.\n",
    "* The next step is usually normalization. Depending on the system's purpose and performance measures, the database can be normalized to 2nd or higher norms. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Design\n",
    "This is the step of converting the logical design and the ERD diagrams into actual database-tables. Among the details to consider:\n",
    "* data types, naming conventions\n",
    "* constraints\n",
    "* indexes: (which fields are more reserached)\n",
    "* performance enhancements...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL For Data science in Python\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

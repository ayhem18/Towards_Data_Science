{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks Using Tf\n",
    "In this notebook, I am solidyfing the theoritical aspects by building simple convolutional neural networks using TensorFlow.\n",
    "### Links: \n",
    "* [Flatten layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)\n",
    "* [Fully-connected layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "* [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)\n",
    "* [Convolution Layer (2D)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n",
    "* [Padding layer (2D)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding2D) \n",
    "* [Max Pooling layer (2D)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)\n",
    "* [model Evaluation](https://www.tutorialspoint.com/keras/keras_model_evaluation_and_prediction.htm)\n",
    "* [Batch Normalization Layer](https://www.bing.com/search?q=tf+keras+batch+normalization&cvid=490c6233dd0f4713b72790c8e5700313&aqs=edge..69i57j69i60.4340j0j1&pglt=515&FORM=ANNTA1&PC=U531)\n",
    "* [Add layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add)\n",
    "* [Random initializer](tf.keras.initializers.RandomUniform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl # tensor flow layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well let's try to build a convolutional neural network\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tfl.ZeroPadding2D(3, input_shape=(64, 64, 3)), # padding both all 4 sides with 3\n",
    "    tfl.Conv2D(filters=32, kernel_size=(7, 7), strides=(1, 1)), # creates a convuolutional layer with 32 filters of size (7,7) and stride value 1\n",
    "    tfl.BatchNormalization(axis=3),\n",
    "    tfl.ReLU(), # performs the activation function on the input values coming from the previous layer\n",
    "    tfl.MaxPool2D(), # performs Max pooling operation, the default window size is (2, 2)\n",
    "    tfl.Flatten(), # flattens the output of the previous layer\n",
    "    tfl.Dense(1, activation='sigmoid') # a fully-connected layer with one unit and a sigmoid activation function\n",
    "])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "# displaying the inner details of the model seems like a reasonable idea\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "Let's explain the output of the previous cell:\n",
    "* zero Padding: the layer receives an input of (64, 64, 3), as the padding is only 2D then only the first two dimensions are concerned. The padding is 3 thus the new dimensions are $(64 + 3\\cdot 2, 64 + 3 \\cdot 2, 3)$ = $(70, 70, 3)$\n",
    "* since the convolutional layer has a stride $s=1$ and a $32$ filters of size $f = 7$ then the output's shape is $(70 - 7 + 1, 70 - 7 + 1, 32)$ = $(64, 64, 32)$\n",
    "* the batch normalization layer does not change its input's shape as it normalizes the values relatively to the whole batch\n",
    "* the Relu layer is an activation layer that modifies the values without affecting the shape                       \n",
    "* MaxPooling layer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_happy_dataset(data_train_path, data_test_path):\n",
    "    \n",
    "    train_dataset = h5py.File(data_train_path, \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # train set labels\n",
    "\n",
    "    test_dataset = h5py.File(data_test_path, \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_path = 'utility_files/train_happy.h5'\n",
    "data_test_path = 'utility_files/test_happy.h5'\n",
    "\n",
    "# let's load the data to train the model\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset(data_train_path, data_test_path)\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "\n",
    "X_test = X_test_orig/255.\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=12, batch_size=(X_train.shape[0] // 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test) \n",
    "# the first value represents the value of the loss \n",
    "# the second represents the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will build a convolutional model using the functional API\n",
    "\n",
    "def convolutional_model(input_image, input_shape):\n",
    "    \"\"\"\n",
    "    This method implements the forward pass according to the following schema:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE\n",
    "\n",
    "    Arguments:\n",
    "    input_img -- input dataset, of shape (input_shape)\n",
    "\n",
    "    Returns:\n",
    "    model -- TF Keras model (object containing the information for the entire training process) \n",
    "    \"\"\"\n",
    "\n",
    "    # the main idea of the Functional API is to build more flexible neural networks that do not necessarily follow the linear/sequential architecture\n",
    "    # creating each of the models separately\n",
    "\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "    conv1 = tfl.Conv2D(filters=8, kernel_size=(4,4), padding='SAME', strides=(1, 1)) # the convolution operation will not shrink the object in question\n",
    "    activation1 = tfl.ReLU() # default parameters\n",
    "    max_pool1 = tfl.MaxPool2D((8, 8), strides=(8, 8), padding='SAME') # no shrinkage\n",
    "\n",
    "    conv2 = tfl.Conv2D(filters=16, kernel_size=(2,2), strides=(1,1), padding='SAME')\n",
    "    activation2 = tfl.ReLU()\n",
    "    max_pool2 = tfl.MaxPool2D((4, 4), strides=(4,4), padding='SAME')\n",
    "\n",
    "    flatten = tfl.Flatten()\n",
    "    fully_connected_layer = tfl.Dense(6, activation='softmax') # return the probabilities that the image belongs to one of the 6 classes\n",
    "\n",
    "    # creating the graph: how each layer is connected to the other\n",
    "    x = conv1(input_img)\n",
    "    x = activation1(x)\n",
    "    x = max_pool1(x)\n",
    "    x = conv2(x)\n",
    "    x = max_pool2(x)\n",
    "    x = flatten(x)\n",
    "    outputs = fully_connected_layer(x)\n",
    "    # declare the model\n",
    "    hand_sign_model = tf.keras.Model(inputs=input_img, outputs= outputs, name='hand_sign_number_detector')\n",
    "\n",
    "    return hand_sign_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convolutional_model((64, 64, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Network\n",
    "More detailed explanation of Residual networks can be found through this [link](https://github.com/ayhem18/Towards_Data_science/blob/master/Machine_Learning/CNN/CNN_2.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import RandomUniform as random_uniform\n",
    "from tensorflow.keras.initializers import GlorotUniform as glorot_uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following code builds a residual block slightly more powerful than the one explained in the theoretical part.\n",
    "## this function builds a three-componenet residual block\n",
    "\n",
    "## personnal attempt\n",
    "def identity_block(X, f, filters, training=True, initializer=random_uniform):\n",
    "    assert len(filters) == 3\n",
    "    # retreive the number of filters associated with each component\n",
    "    f1, f2, f3 = filters\n",
    "    input_shape = X.shape\n",
    "    # component one\n",
    "    input = tf.keras.Input(shape=input_shape)\n",
    "    conv1 = tfl.Conv2D(filters=f1, kernel_initializer=initializer(seed=0), padding='valid')\n",
    "    batnorm1 = tfl.BatchNormalization(axis=3)\n",
    "    activation = tfl.Relu()\n",
    "\n",
    "    conv2 = tfl.Conv2D(filters=f2, kernel_initializer=initializer(seed=0), padding='valid')\n",
    "    batnorm2 = tfl.BatchNormalization(axis=3)\n",
    "\n",
    "    conv3 = tfl.Conv2D(filters=f3, kernel_initializer=initializer(seed=0), padding='valid')\n",
    "    batnorm3 = tfl.BatchNormalization(axis=3)\n",
    "\n",
    "    # first component\n",
    "    x = conv1(input)\n",
    "    x = batnorm1(x)\n",
    "    x1 = activation(x)\n",
    "\n",
    "    # second component\n",
    "    x = conv2(x1)\n",
    "    x = batnorm2(x)\n",
    "    x2 = activation(x)\n",
    "\n",
    "    # apply convolution and normalization \n",
    "    x = conv3(x2)\n",
    "    x = batnorm3(x)\n",
    "\n",
    "    x = x.add(x1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, training=True, initializer=random_uniform):\n",
    "    \"\"\"\n",
    "    Implementation of the identity residual block: Residual block where the input and output are of the same dimensions\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    training -- True: Behave in training mode\n",
    "                False: Behave in inference mode\n",
    "    initializer -- to set up the initial weights of a layer. Equals to random uniform initializer\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "        \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    relu = tfl.ReLU()\n",
    "\n",
    "    # First component of main path\n",
    "    X = tfl.Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = tfl.BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
    "    X = relu(X)\n",
    "    \n",
    "    ## Second component of main path (≈3 lines)\n",
    "    ## Set the padding = 'same'\n",
    "    X = tfl.Conv2D(filters=F2, kernel_size=1, strides=(1,1), padding='same', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = tfl.BatchNormalization(axis=3)(X, training=training)\n",
    "    X = relu(X) \n",
    "\n",
    "    ## Third component of main path (≈2 lines)\n",
    "    ## Set the padding = 'valid'\n",
    "    X = tfl.Conv2D(filters=F3, kernel_size=f, strides=(1,1), padding='same', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = tfl.BatchNormalization(axis=3)(X, training=training) \n",
    "    \n",
    "    ## Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = tfl.Add()([X, X_shortcut])\n",
    "    X = relu(X) \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous block did not take into account the cae of different input and output dimensions. The following code addresses this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## personal attemtp\n",
    "def convolutional_block(X, f, filters, s = 2, training=True, initializer=glorot_uniform):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block: residual block with possibly different input/output dimensions\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    training -- True: Behave in training mode\n",
    "                False: Behave in inference mode\n",
    "    initializer -- to set up the initial weights of a layer. Equals to Glorot uniform initializer, \n",
    "                   also called Xavier uniform initializer.\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    relu = tfl.ReLU()    \n",
    "\n",
    "    X = tfl.Conv2D(filters=F1, kernel_size=1, strides=(1,1), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = tfl.BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
    "    X = relu(X)\n",
    "    \n",
    "\n",
    "\n",
    "    ## Second component of main path (≈3 lines)\n",
    "    X = tfl.Conv2D(filters=F2, kernel_size=f, strides=(1,1), padding='same', kernel_initializer=initializer(seed=0))(X)\n",
    "\n",
    "    X = tfl.BatchNormalization(axis=3)(X, training=training) \n",
    "    X = relu(X) \n",
    "\n",
    "    ## Third component of main path (≈2 lines)\n",
    "    X = tfl.Conv2D(filters=F3, kernel_size=1, strides=(1,1), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = tfl.BatchNormalization(axis=3)(X, training=training)\n",
    "    \n",
    "    ##### SHORTCUT PATH ##### (≈2 lines)\n",
    "    X_shortcut = tfl.Conv2D(filters=F3, kernel_size=1, strides=(s, s), padding='same', kernel_initializer=initializer(seed=0))(X_shortcut)\n",
    "    X_shortcut = tfl.BatchNormalization(axis=3)(X_shortcut, training=training)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "\n",
    "    # Final step: Add shortcut value to main path (Use this order [X, X_shortcut]), and pass it through a RELU activation\n",
    "    X = tfl.Add()([X, X_shortcut])\n",
    "    X = relu(X)\n",
    "\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code builds my first deep neural network using the blocks introduced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
    "    \"\"\"\n",
    "    Stage-wise implementation of the architecture of the popular ResNet50:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> FLATTEN -> DENSE \n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    # define the input tensor to work with\n",
    "    X_input = tfl.Input(input_shape)\n",
    "\n",
    "    ## define a Relu acitvation layer as it is used extensively\n",
    "    relu = tfl.ReLU()\n",
    "\n",
    "    ## pad the input with zeros\n",
    "    X = tfl.ZeroPadding2D(padding=(3, 3))(X_input)\n",
    "    \n",
    "    ## stage 1\n",
    "    X = tfl.Conv2D(filters=64, kernel_size=7, strides=(2,2), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = tfl.BatchNormalization(axis=3)(X)\n",
    "    X = relu(X)\n",
    "    X = tfl.MaxPooling2D(pool_size=(3, 3), strides=(2,2))(X)\n",
    "\n",
    "    ## stage 2\n",
    "    X = convolutional_block(X, f=3, s=1, filters=[64, 64, 256])\n",
    "    X = identity_block(X, f=3, filters=[64, 64, 256])\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "\n",
    "    # stage 3\n",
    "    X = convolutional_block(X, f=3, s=2, filters=[128, 128, 512])\n",
    "    X = identity_block(X, f=3, filters=[128, 128, 512])\n",
    "    X = identity_block(X, f=3, filters=[128, 128, 512])\n",
    "    X = identity_block(X, f=3, filters=[128, 128, 512])\n",
    "\n",
    "    # stage 4\n",
    "    X = convolutional_block(X, f=3, s=2, filters=[256, 256, 1024])\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024])\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024])\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024])\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024])\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024])\n",
    "\n",
    "    ## stage 5\n",
    "    X = convolutional_block(X, f=3, s=2, filters=[512, 512, 2048])\n",
    "    X = identity_block(X, f=3, filters=[512, 512, 2048])\n",
    "    X = identity_block(X, f=3, filters=[512, 512, 2048])\n",
    "\n",
    "    ## final stage\n",
    "    X = tfl.AveragePooling2D(pool_size=(2,2))(X)\n",
    "    X = tfl.Flatten()(X)\n",
    "    X = tfl.Dense(classes, activation='softmax', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    model = tf.keras.Model(inputs = X_input, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50 = ResNet50()\n",
    "res50.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "res50.fit(X_train, Y_train, epochs=12, batch_size=(X_train.shape[0] // 20))\n",
    "res50.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmenter():\n",
    "    '''\n",
    "    Create a Sequential model composed of 2 layers\n",
    "    Returns:\n",
    "        tf.keras.Sequential\n",
    "    '''\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "    tfl.RandomFlip(\"horizontal\"),\n",
    "    tfl.RandomRotation(0.2),\n",
    "    ])    \n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = data_augmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = data_augmenter()\n",
    "\n",
    "assert(augmenter.layers[0].name.startswith('random_flip')), \"First layer must be RandomFlip\"\n",
    "assert augmenter.layers[0].mode == 'horizontal', \"RadomFlip parameter must be horizontal\"\n",
    "assert(augmenter.layers[1].name.startswith('random_rotation')), \"Second layer must be RandomRotation\"\n",
    "assert augmenter.layers[1].factor == 0.2, \"Rotation factor must be 0.2\"\n",
    "assert len(augmenter.layers) == 2, \"The model must have only 2 layers\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation \n",
    "\n",
    "def data_augmenter_2():\n",
    "    da = tf.keras.Sequential([])\n",
    "    da.add(RandomFlip(\"horizontal\"))\n",
    "    da.add(RandomRotation(0.2))\n",
    "    return da\n",
    "\n",
    "da = data_augmenter_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08488e93894ea7be7272109919d40edb52233f14daf834f5f2387122a81730e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

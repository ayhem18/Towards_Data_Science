{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Feature engineering is the art (yes art) of manually creating new features to feed to the model with the goal of boosting performance. In this notebook, I will explore (and save) a number of general and well-known techniques.\n",
    "This notebook follows two sources:\n",
    "* Kaggle [course](https://www.kaggle.com/learn/feature-engineering)\n",
    "* DataCamp [course]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "Before proceeding in creating new features, it is recommended to evalute a baseline relatively powerful model, with the initial features. A model performance's improvement reflects the usefulness of the new synthetized features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mututal Information\n",
    "### Definition\n",
    "Having a large number of features might be extremely overwhelming. Thus, the first step is to reduce the available set of features into a smaller one that might serve as a starting point to build a powerful prediction model. Later, more features can be incorporated in the process.\n",
    "Mutual Information, unlike correlation is capable of capturing any type of relationship between the target and the feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details\n",
    "In simple terms, MI reflects the extent to which the knowledge of one variable (feature) reduces the uncertainty of another (target).  \n",
    "On a technical note, MI is based on information technology measurement refferred to as ***entropy***. The larger the entropy, the more uncertain the variable is, (the less correlated / tied to each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Notes\n",
    "There are important poitns to keep in mind:\n",
    "* Each feature is evaluated separately. In other words, a feature might quite powerful when combined with other features. Yet, it might not as significant on its own\n",
    "* A high MI score reflects a **potentially** useful feature. A transformation such as log, exponential, polynomial might be needed to take see the relationship between the target and the feature.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying\n",
    "The kaggle course offers a great opportunity to practice this technique on the [***Ames***](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data?select=data_description.txt) dataset. In the next section, we will consider a couple of related ideas that might represents the basics for mode advanced techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the defaults of matplotlib\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to check this [link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.factorize.html) for better grasp of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function to calculate the MI scores given data and labels\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def calculate_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    # convert non-numerical data to numerical data\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # fill the Nan values in the numerical columns with 0 temporarily\n",
    "    for col in X.select_dtypes(np.number):\n",
    "        X[col] = X[col].fillna(0)\n",
    "\n",
    "    # all data with an int dtype should be considered discrete (in the MI calculation)\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"ames.csv\"\n",
    "df = pd.read_csv(train_file).rename(columns={\"SalePrice\": \"y\"})\n",
    "Y = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\"YearBuilt\", \"MoSold\", \"ScreenPorch\"]\n",
    "sns.relplot(\n",
    "    x=\"value\", y=Y, col=\"variable\", data=df.melt(id_vars=Y, value_vars=features), facet_kws=dict(sharex=False),\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(Y)\n",
    "mi_scores = calculate_mi_scores(X, y)\n",
    "best_20_feats = list(mi_scores.head(20).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the features with the highest MI scores, it is a good idea to search for features that interact with one or more of the relevant features. This might empower the model quite well.  \n",
    "Creating new features might be tricky. Here are a couple of tips on how to discover and creat new features;\n",
    "* have a better understanding of the features and the field of interest. A domain knowledge can be inspiring.\n",
    "* Study previous work\n",
    "* Data visualization is one of the most important tools.\n",
    "\n",
    "### general ideas:\n",
    "1. It might be fruitful to apply a number of transformations such as ***log, exponential, polynomial***. The latters are mainly used for normalization purposes especially where the data is relatively skewed.  \n",
    "2. The more complex the combincation the hard it is for the model to learn it by its own. Combincation involving different arithmetical operators are quite powerful: They are generally inspired by domain knowledge.\n",
    "3. There is a certain type of feature generally determined by the absence of presence of certain factor. Such generally come together and it might be useful to group them all together.\n",
    "4. Certain features are quite complex: such as strings. They can be broken down as certain parts represent particular information (again research is quite helpful in this regard)\n",
    "5. if two features seem to interact it might be useful to group one by the other and apply a number of aggregations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Feature Engineering\n",
    "creating features. Here are some guidelines:\n",
    "* Linear models learn sums and differences naturally, but can't learn anything more complex.\n",
    "* Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n",
    "* Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n",
    "* Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n",
    "* Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(random_state=3, n_splits=5, shuffle=True )\n",
    "scoring = \"neg_mean_squared_log_error\"\n",
    "\n",
    "def score(X, y, model):\n",
    "    X = X.copy()\n",
    "    # the function factorize converts non-numerical data to numerical \n",
    "    for col in X.select_dtypes(['category', 'object']):\n",
    "        X[col], _ = X[col].factorize()\n",
    "    # fill the numerical values by 0: temporary solution\n",
    "    for col in X.select_dtypes(np.number):\n",
    "        X[col]= X[col].fillna(0)\n",
    "    score = cross_val_score(model, X, y, cv=kf, scoring=scoring)\n",
    "    return np.sqrt(- score.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_20_feats.append(Y)\n",
    "df_20 = df.copy()[best_20_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X20 = df_20.copy()\n",
    "y20 = X20.pop(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17436623262344367\n",
      "0.17965724821215492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "\n",
    "rf_base = rfr(n_estimators=200, max_depth=5, random_state=3)\n",
    "# so\n",
    "print(score(X, y, rf_base)) \n",
    "\n",
    "print(score(X20, y20, rf_base)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create a number of new features:\n",
    "* LivLotRatio = the ration of GrLivArea to LotArea\n",
    "* Spaciousness = Sum of FirstFlrSf and SecondFlrSf divided by TotarlRoomAbvGr: the average space by room \n",
    "* TotalOutSurface = sum of all porches and WoodDeckSf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_1 dataframe to store the new features synthetized\n",
    "X_1 = pd.DataFrame()  # dataframe to hold new features\n",
    "\n",
    "X_1[\"LivLotRatio\"] = X['GrLivArea'] / X['LotArea']\n",
    "X_1[\"Spaciousness\"] = (X['FirstFlrSF'] + X['SecondFlrSF']) / X['TotRmsAbvGrd']\n",
    "X_1[\"TotalOutsideSF\"] = X['WoodDeckSF'] +  X[\"OpenPorchSF\"]+ X[\"EnclosedPorch\"] + X[\"Threeseasonporch\"] + X[\"ScreenPorch\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we find an interaction between a categorical and continous feature, consider the following procedure:\n",
    "1. create get_dummies of the categorial features: X_new = pd.get_dummies(df.cat, suffixe='meaningful')\n",
    "2. X_new = X_new.mul(df.Con, axis=0): multiply row by row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply this idea to our dataset with BlgdType (building type) and \"GrLivArea\": \n",
    "X_2 = pd.get_dummies(df['BldgType'], prefix='Bldg')\n",
    "X_2 = X_2.mul(df['GrLivArea'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts feature engineering example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider the outside surfaces features, we can consider the number of such features each sample have\n",
    "X_3 = pd.DataFrame()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "porchs = [\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"Threeseasonporch\",\"ScreenPorch\"]\n",
    "\n",
    "X_3[\"PorchTypes\"] = X[porchs].gt(0).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking down feature engineering example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['One_Story_1946_and_Newer_All_Styles', 'Two_Story_1946_and_Newer',\n",
       "       'One_Story_PUD_1946_and_Newer',\n",
       "       'One_and_Half_Story_Finished_All_Ages', 'Split_Foyer',\n",
       "       'Two_Story_PUD_1946_and_Newer', 'Split_or_Multilevel',\n",
       "       'One_Story_1945_and_Older', 'Duplex_All_Styles_and_Ages',\n",
       "       'Two_Family_conversion_All_Styles_and_Ages',\n",
       "       'One_and_Half_Story_Unfinished_All_Ages',\n",
       "       'Two_Story_1945_and_Older', 'Two_and_Half_Story_All_Ages',\n",
       "       'One_Story_with_Finished_Attic_All_Ages',\n",
       "       'PUD_Multilevel_Split_Level_Foyer',\n",
       "       'One_and_Half_Story_PUD_All_Ages'], dtype=object)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.MSSubClass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can consider the first word when splitting the data by the string \"_\" as broader and larger class of homes\n",
    "\n",
    "X_4 = pd.DataFrame()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_4['MSClass'] = X['MSSubClass'].apply(lambda x: x.split(\"_\")[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping feature engineering example\n",
    "We can see that a house is highly affected by the type of neighborhood it resides in. Let's consider the median / mean area of houses by neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_5 = pd.DataFrame()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "area_by_neighborhood = X.groupby(\"Neighborhood\").GrLivArea.agg('median')\n",
    "X_5[\"MedNhbdArea\"] = X['Neighborhood'].apply(lambda x: area_by_neighborhood[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check how the model performs with the given new features\n",
    "X20 = X20.join([X_1, X_2, X_3, X_4, X_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X.join([X_1, X_2, X_3, X_4, X_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17451238570779448\n"
     ]
    }
   ],
   "source": [
    "print(score(X_new, y, rf_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Unsupervised learning\n",
    "It is worth noting that we can use unsupervised learning algorithms such as ***K-means*** to a set of features that are interconnected. To hypertune mainly the number of parameters it might be useful to measure the performance of the model with each value of the new cluster feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Encoding\n",
    "This technique empowers the categorical features by converting to numerical values\n",
    "##### Target Encoding\n",
    "Target encoding is any encoding that replace a feature's categories with certain number derived from the target. One popular approach is to replace map each value of the category to the mean when grouping the target on that feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smoothing\n",
    "This technique represents certain risks that should be taken into account. Missing values should be imputed somehow: for a relatively large dataset, this might not be an easy task. Additionally, rare categories could not accurately be represented by practically any statistics. Target encoding in its most basic form might lead to overfitting.  \n",
    "The solution is the technique known as ***smoothing***. It can generally be expressed in pseudo code as follows:\n",
    "$\\begin{align} \n",
    "    encoding = w * (statistics~in~category) + (1 - w) * (statistics~overall)\n",
    "\\end{align}$\n",
    "where the term $w$ mainly is a synthetic metric inspired from the category frequency:\n",
    "$\\begin{align} w = \\frac{n}{n + m}\\end{align}$\n",
    "The choice of $m$ is the result of several considerations: \n",
    "* if the target values within a category are highly variant, it might be a good idea to consider large values of $m$\n",
    "* if the target values vary slightly within a specified range, then smaller values of m would not not hurt.  \n",
    "  \n",
    "  \n",
    "When to consider Target Encoding:\n",
    "* High-cardanility features: the large the number of categories, the more troubelsome it gets. In such situations, label encoding: mapping each category to a random value as well as one-hot encoding are not favorable choices\n",
    "* domain-supported features: certain features can be quite relevant in the prediction even when they might score poorly on feature metrics. a target encoding might bring the feature's usefulness to the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Target encoding it is ***crucial*** to train the encoder on a sample of the training data and not all of it. Otherwise, the model might easily overfit. instead for encoding the values manually, it is advisable to use skelearn encoders described in detail in this [link](https://practicaldatascience.co.uk/machine-learning/how-to-use-category-encoders-to-transform-categorical-variables).  \n",
    "Smoothing can be applied using the ***MEstimateEncoder*** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

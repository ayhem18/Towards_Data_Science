{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl6x8tUYXBeX"
      },
      "source": [
        "### Lab Objectives\n",
        "\n",
        "We will practice understanding Markov Reward Process and Markov Decision Process. Also we will try to apply the direct solution to solve the Bellman Expectation Equation.\n",
        "\n",
        "***\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Before we begin, let's refresh our memory on this question:\n",
        "\n",
        "**What is Markov Chain?**\n",
        "\n",
        "To summarise a Markov chain is defined as follows:\n",
        "* S = set of states\n",
        "* P = state transition probabilities\n",
        "* $S_0$ = starting state\n",
        "\n",
        "\n",
        "![](https://drive.usercontent.google.com/download?id=1vef0Xhpy5OMwTxKLgBBqxrxNTp9bhw9S&authuser=0)\n",
        "\n",
        "\n",
        "![Image Title](https://image-link.com/image.png)\n",
        "\n",
        "\n",
        "[Open this link!](https://setosa.io/blog/2014/07/26/markov-chains/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlkxUbsibai2"
      },
      "source": [
        "**what is the core problem that is being solved for in reinforcement learning?**\n",
        "\n",
        "At the simplest level, the problem we are solving for is to teach the agent to behave *optimally* in a specific environment. An example might be to teach a robot to bounce a ball for some period of time; or program a helicopter to keep the same altitude in unpredictable windy conditions.\n",
        "\n",
        "The numerical definition of what is *optimal* is defined by the **objective function**, which is typically maximized in the context of MDPs (Markov Decision Processes). In the context of MDPs , this objective function is known as the **value function**, aka the *total expected reward*, which is received over sequential state transitions.\n",
        "\n",
        "The goal then, is to teach the agent to maximize the *total expected reward* it earns over some time horizon (theoretically, it is an infinite time horizon) by selecting the best action as dictated by the value function. The agent learns to maximize rewards as it transitions from state to state, taking actions in each state. In a deterministic case, the transitioning process is diagrammed as follows:\n",
        "\n",
        "$$\\text{State 1}\\xrightarrow[Action]{}\\text{State 2} \\xrightarrow[Action]{}\\text{State 3}\\xrightarrow[Action]{}\\dots$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSC3mwCElD4D"
      },
      "source": [
        "### Markov Decision Processes\n",
        "\n",
        "Let's formalize the key components of the RL problem in the context of MDPs:\n",
        "\n",
        "An MDP is defined by: $(S, A, P, R, S_0, \\gamma)$\n",
        "* S = set of states (state-space)\n",
        "* A = set of actions (action-space)\n",
        "* P = state transition probabilities\n",
        "* R = reward for taking an action $a\\in\\text{A}$ in state $s\\in\\text{S}$\n",
        "* $S_0$ = starting state\n",
        "* $\\gamma$ = discount rate\n",
        "    \n",
        "In more detail:\n",
        "* **States** - states can be discrete/finite (imagine cells in a grid world) or continuous/infinite (position on a road).\n",
        "    * Referred to as the *state space* (i.e. discrete state space or continuous state space)\n",
        "* **Actions** - actions can also be discrete (moving up/down/left/right in a grid world cell) or continuous (how many degrees to turn a steering wheel when driving a car).\n",
        "    * Referred to as the *action space* (i.e. discrete action space or continuous action space)\n",
        "* **Rewards** - rewards are issued by a reward function $\\rho : S_t \\times A_t \\rightarrow R$. The reward function is a property of the environment.\n",
        "* **Transition probabilities**. In MDPs, this is denoted by $P_{s,a}$. The transition probability is the probability that, for example, some action $A$ in state $S$ leads to state $S^\\prime$ (prime denotes the next time step) - represented notationally as $P(s^\\prime|s,a)$.\n",
        "* **Discount factor** - the discount factor is a number greater than 0 and less than 1 that is used to discount rewards received over sequential time-steps. It is denoted as $\\gamma \\in [0, 1)$\n",
        "* **Value function** - one of the primary functions learned by the agent: the value function dictates either the value of a state or the value of action. More on this below.\n",
        "* **Policy function** - one of the primary functions learned by the agent: the policy maps states to actions. More below.\n",
        "\n",
        "#### Other useful definitions\n",
        "* **Experience** - $\\big(\\text{State}_{t}$, $\\text{Action}_{t}$, $\\text{Reward}_{t}\\big)$ tuple\n",
        "* **Trajectory** - A sequence of *experiences* through time, represented as: $\\tau$ (tau)\n",
        "* **Episode** - A trajectory that ends in a terminal state\n",
        "\n",
        "\n",
        "### Policy\n",
        "\n",
        "The process of learning for the agent can be thought of as a sequence of mapping states to actions $a = \\pi(s)$ to maximize expected reward over an episode. This is known as the **policy:** $\\pi: S \\rightarrow A$.\n",
        "\n",
        "Note:\n",
        "* The agent needs to explore and interact with its environment to learn where actions earn the maximum rewards.\n",
        "* Actions in the current time step effect rewards in future time steps\n",
        "* There is a trade off between the frequency of sampling the environment and frequency of taking actions\n",
        "* It can be based on discrete state-spaces or continuous state-spaces (and same for action-spaces)\n",
        "\n",
        "### Value functions\n",
        "\n",
        "The **worthiness** of a policy is calculated by the aforementioned *value function*. There are various forms of value functions. First, the value of a state:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1W6LE42KbtNoOs1LKkFxlWhOF6-7wzWxS\" />\n",
        "\n",
        "Second, the value of action:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1I0Df6jgFDH7gBU1djylJ8ZzmJUO36P9_\" />\n",
        "\n",
        "\n",
        "\n",
        "#### <font color=\"#DE008A\">Bellman's optimal ($*$) action-value function</font> :\n",
        "\n",
        "$$Q^*{(s,a)} = \\ R{(s,a)} + \\gamma \\max_{a^\\prime} \\sum_{s^\\prime\\in\\text{S}} \\big[P{(s^\\prime|s,a)}  Q^*{(s^\\prime,a^\\prime)}\\big]$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qKCzSgsKbr7"
      },
      "source": [
        "## Cleaning Robot Problem\n",
        "\n",
        "The main characteristics of this world are the following:\n",
        "\n",
        "- Discrete time and space\n",
        "- Fully observable\n",
        "- Infinite horizon\n",
        "- Known Transition Model\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Yz6xnDuo6StlKzmj4eDqVYvZfOORBlBT\"/>\n",
        "\n",
        "\n",
        "The main goal for the robot in this task is to find the best way to reach the charging station.\n",
        "\n",
        "What does the \"best way\" even mean?\n",
        "It depends on the reward that the robot receives in each intermediate state -> that leads to multiple optimal policies.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1vef0Xhpy5OMwTxKLgBBqxrxNTp9bhw9S\"/>\n",
        "\n",
        "\n",
        "\n",
        "**The Bellman equation**\n",
        "\n",
        "$$Q^*{(s,a)} = \\ R{(s,a)} + \\gamma \\max_{a^\\prime} \\sum_{s^\\prime\\in\\text{S}} \\big[P{(s^\\prime|s,a)}  Q^*{(s^\\prime,a^\\prime)}\\big]$$\n",
        "\n",
        "In this example the reward for each non-terminal state is\n",
        "$$R(s) = −0.04$$\n",
        "\n",
        "\n",
        "BUT, before we begin let's assume that somehow we got the optima optimal policy and utility values generated by the optimal value-function just to help us understand the idea.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1C-hx0MwCi9hNQcM9Q-QIvVJzalbVWypa\"/>\n",
        "\n",
        "In our example we suppose the robot starts from the state (1,1). Using the Bellman equation we have to find the action with the highest utility between UP, LEFT, DOWN and RIGHT. We do not have the optimal policy, but we have the transition model and the utility values for each state. You have to recall the two main rules of our environment: (i) if the robot bounce on the wall it goes back to the previous state, and (ii) the selected action is executed only with a probability of 80% in accordance with the transition model. Instead of dealing with those ugly numbers I want to show you a visual representaion of the possible outcomes:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1_X65joHfWYkFNEIorjSNXmMR0rRGNTjX\"/>\n",
        "\n",
        "For each possible outcome I reported the utility and the probability given by the transition model. This corresponds to the first part of the Bellman equation. The next step is to calculate the product between the utility and the transition probability, then sum up the value for each action.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1lmDRxIXOs0h6cGP6VzzsRjauu-nGJYbO\"/>\n",
        "\n",
        "We found out that for state (1,1) the action UP has the highest value. This is in accordance with the optimal policy we magically got.\n",
        "\n",
        "Now we have all the elements and we can plug the values in the Bellman equation finding the utility of the state (1,1):\n",
        "\n",
        "$$U(s11)=−0.04 + 1.0 × 0.7456= 0.7056$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gtHxLR_EJ1bE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.load('T.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D6HX7UAcRc6c"
      },
      "outputs": [],
      "source": [
        "class MDP:\n",
        "    def __init__(self):\n",
        "        #Starting state vector\n",
        "        #The agent starts from (1, 1)\n",
        "        self.state = np.array([[0.0, 0.0, 0.0, 0.0,\n",
        "                                    0.0, 0.0, 0.0, 0.0,\n",
        "                                    1.0, 0.0, 0.0, 0.0]])\n",
        "        self.rewards = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
        "                                    -0.04,   0.0, -0.04,  -1.0,\n",
        "                                    -0.04, -0.04, -0.04, -0.04])\n",
        "\n",
        "        # Probabilities Transition matrix loaded from file\n",
        "        # 12x12x4 matrix (12 starting states, 12 next states, 4 actions)\n",
        "        self.transits = np.load(\"T.npy\")\n",
        "        #Utility vector genereated by the ((assumed)) optimal value-function\n",
        "        self.values = np.array([[0.812, 0.868, 0.918,   1.0,\n",
        "                                    0.762,   0.0, 0.660,  -1.0,\n",
        "                                    0.705, 0.655, 0.611, 0.388]])\n",
        "        #Assuming that the discount factor is equal to 1.0\n",
        "        self.gamma = 1.0\n",
        "\n",
        "        # self.epsilon = 0.0005\n",
        "\n",
        "    def return_state_utility(self):\n",
        "        \"\"\"Return the state utility (return).\n",
        "\n",
        "        @return the utility (return) of the state\n",
        "        \"\"\"\n",
        "        action_array = np.zeros(4)\n",
        "        \n",
        "        for action in range(0, 4):\n",
        "            \n",
        "            #TODO Calculate action_array\n",
        "            # action_array[action] =\n",
        "            ...\n",
        "\n",
        "        # TODO return the state utility\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj_K-eyiVo-q",
        "outputId": "71ba500e-960c-4df8-920e-66f2b15a206e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utility of state (1,1): 0\n"
          ]
        }
      ],
      "source": [
        "mdp = MDP()\n",
        "u_11 = mdp.return_state_utility()\n",
        "print(\"Utility of state (1,1): \" + str(u_11))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmAx8d2tlOGw"
      },
      "source": [
        "**Direct solution**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1CbWE_KdxuQibsZccX7W7ZWdeni48Vffh\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VvydVKPlZ8G"
      },
      "outputs": [],
      "source": [
        "class MDP:\n",
        "    def __init__(self):\n",
        "        #Starting state vector\n",
        "        #The agent starts from (1, 1)\n",
        "        self.state = np.array([[0.0, 0.0, 0.0, 0.0,\n",
        "                                    0.0, 0.0, 0.0, 0.0,\n",
        "                                    1.0, 0.0, 0.0, 0.0]])\n",
        "        self.rewards = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
        "                                    -0.04,   0.0, -0.04,  -1.0,\n",
        "                                    -0.04, -0.04, -0.04, -0.04])\n",
        "\n",
        "        # Probabilities Transition matrix loaded from file\n",
        "        #(It is too big to write here)\n",
        "        self.transits = np.load(\"T.npy\")\n",
        "        #Generate the first policy randomly\n",
        "        # Nan=Nothing, -1=Terminal, 0=Up, 1=Left, 2=Down, 3=Right\n",
        "        self.policy = np.random.randint(0, 4, size=(12)).astype(np.float32)\n",
        "        self.policy[5] = np.NaN\n",
        "        self.policy[3] = self.policy[7] = -1\n",
        "\n",
        "        #Utility vector\n",
        "        self.values = np.array([0.0, 0.0, 0.0,  0.0,\n",
        "                                0.0, 0.0, 0.0,  0.0,\n",
        "                                0.0, 0.0, 0.0,  0.0])\n",
        "        self.gamma = 0.9\n",
        "\n",
        "        self.epsilon = 0.0001\n",
        "        self.iteration = 0\n",
        "\n",
        "    def policy_evaluation(self, shape=(3,4)):\n",
        "        length = shape[0]*shape[1]\n",
        "        self.values = np.zeros(length)\n",
        "        for s in range(12):\n",
        "            if not np.isnan(self.policy[s]):\n",
        "                action = int(self.policy[s])\n",
        "                self.values[s] = np.linalg.solve(np.identity(length) - self.gamma\\\n",
        "                                                    *self.transits[:,:,action]\\\n",
        "                                                    , self.rewards)[s]\n",
        "\n",
        "        return self.values\n",
        "\n",
        "    def expected_action(self):\n",
        "        \"\"\"Return the expected action.\n",
        "        It returns an action based on the\n",
        "        expected utility of doing a in state s,\n",
        "        according to T and u. This action is\n",
        "        the one that maximize the expected\n",
        "        utility.\n",
        "\n",
        "        @return expected action (int)\n",
        "        \"\"\"\n",
        "        actions_array = np.zeros(4)\n",
        "\n",
        "        for action in range(4):\n",
        "            # Expected utility of doing a in state s, according to T and u.\n",
        "            # TODO calculate actions_array[action]\n",
        "            # actions_array[action] =\n",
        "            pass\n",
        "\n",
        "        return np.argmax(actions_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_policy(p, shape):\n",
        "    \"\"\"Print the policy on the terminal\n",
        "    Using the symbol:\n",
        "    * Terminal state\n",
        "    ^ Up\n",
        "    > Right\n",
        "    v Down\n",
        "    < Left\n",
        "    # Obstacle\n",
        "    \"\"\"\n",
        "\n",
        "    counter = 0\n",
        "    policy_string = \"\"\n",
        "    \n",
        "    for row in range(shape[0]):\n",
        "        for col in range(shape[1]):\n",
        "            if(p[counter] == -1): policy_string += \" *  \"\n",
        "            elif(p[counter] == 0): policy_string += \" ^  \"\n",
        "            elif(p[counter] == 1): policy_string += \" <  \"\n",
        "            elif(p[counter] == 2): policy_string += \" v  \"\n",
        "            elif(p[counter] == 3): policy_string += \" >  \"\n",
        "            elif(np.isnan(p[counter])): policy_string += \" #  \"\n",
        "            counter += 1\n",
        "        policy_string += '\\n'\n",
        "    \n",
        "    print(policy_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVwo-eY9Ym3b"
      },
      "outputs": [],
      "source": [
        "mdp = MDP()\n",
        "u = mdp.values\n",
        "\n",
        "while True:\n",
        "    mdp.iteration += 1\n",
        "    #1- Policy evaluation\n",
        "    u_old = u.copy()\n",
        "    u = mdp.policy_evaluation()\n",
        "\n",
        "    unchanged = True\n",
        "    delta = np.absolute(u - u_old).max()\n",
        "    if (delta < mdp.epsilon * (1 - mdp.gamma) / mdp.gamma) or mdp.iteration > 10000: break\n",
        "    for s in range(12):\n",
        "        if not np.isnan(mdp.policy[s]) and not mdp.policy[s]==-1:\n",
        "            mdp.state = np.zeros((1,12))\n",
        "            mdp.state[0,s] = 1.0\n",
        "            #2- Policy improvement\n",
        "            a = mdp.expected_action()\n",
        "            if a != mdp.policy[s]:\n",
        "                mdp.policy[s] = a\n",
        "                unchanged = False\n",
        "    # print_policy(mdp.policy, shape=(3,4))\n",
        "    if unchanged: break\n",
        "\n",
        "print(\"=================== FINAL RESULT ==================\")\n",
        "print(\"Iterations: \" + str(mdp.iteration))\n",
        "#print(\"Delta: \" + str(delta))\n",
        "print(\"Gamma: \" + str(mdp.gamma))\n",
        "print(\"Epsilon: \" + str(mdp.epsilon))\n",
        "print(\"===================================================\")\n",
        "print(u[0:4])\n",
        "print(u[4:8])\n",
        "print(u[8:12])\n",
        "print(\"===================================================\")\n",
        "print_policy(mdp.policy, shape=(3,4))\n",
        "print(\"===================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf7e75H0eSqC"
      },
      "outputs": [],
      "source": [
        "mdp.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTn-7TeyDBC8"
      },
      "outputs": [],
      "source": [
        "np.dot(mdp.state, mdp.transits[:,:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOb33VbNECe6"
      },
      "outputs": [],
      "source": [
        "mdp.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNPhOZbeEE2u"
      },
      "outputs": [],
      "source": [
        "mdp.transits[:,:,0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

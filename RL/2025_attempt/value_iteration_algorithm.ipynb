{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxOmU8oJZ8cw"
      },
      "source": [
        "FirstName LastName: Ayhem Bouabid \n",
        "\n",
        "Email: a.bouabid@innnopolis.university"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlKIE0U37MsR"
      },
      "source": [
        "## Assignment 1: The value iteration algorithm\n",
        "\n",
        "Your task is to solve the Robot Cleaning problem using the Value Iteration algorithm.\n",
        "\n",
        "1- Based on your understanding write down the pseudo-code for Value Iteration algorithm.\n",
        "\n",
        "2- Complete the implementation for value iteration algorithm. Basically you need to only complete the `MDP.state_utility()` method, comment on your results.\n",
        "\n",
        "3- Make sure to provide your detailed explanation for the result.\n",
        "\n",
        "4- Explain in detail the results you got in comparison with the Policy Iteration algorithm in your own words based on your understanding.\n",
        "\n",
        "\n",
        "**Note:** Your grade will be based on your understanding of the algorithm and your analysis to the results. Cheating will be punished by nullifying.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ss26PsG74-hR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MDP:\n",
        "\tdef __init__(self):\n",
        "\t\t#Starting state vector\n",
        "\t\t#The agent starts from (1, 1)\n",
        "\t\tself.states = np.array([[0.0, 0.0, 0.0, 0.0,\n",
        "\t\t\t\t\t\t0.0, 0.0, 0.0, 0.0,\n",
        "\t\t\t\t\t\t1.0, 0.0, 0.0, 0.0]])\n",
        "\n",
        "\t\tself.rewards = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
        "\t\t\t-0.04,   0.0, -0.04,  -1.0,\n",
        "\t\t\t-0.04, -0.04, -0.04, -0.04])\n",
        "\n",
        "\t\tself.num_states = 12\n",
        "\n",
        "\t\t# Probabilities Transition matrix loaded from file\n",
        "\t\t#(It is too big to write here)\n",
        "\t\tself.transits = np.load(\"T.npy\")\n",
        "\t\t#Generate the first policy randomly\n",
        "\t\t# Nan=Nothing, -1=Terminal, 0=Up, 1=Left, 2=Down, 3=Right\n",
        "\t\tself.policy = np.random.randint(0, 4, size=(12)).astype(np.float32)\n",
        "\t\tself.policy[5] = np.nan\n",
        "\t\tself.policy[3] = self.policy[7] = -1\n",
        "\n",
        "\t\t#Utility vector\n",
        "\t\tself.values = np.array([0.0, 0.0, 0.0,  0.0,\n",
        "\t\t0.0, 0.0, 0.0,  0.0,\n",
        "\t\t0.0, 0.0, 0.0,  0.0])\n",
        "\t\tself.gamma = 0.999\n",
        "\n",
        "\t\tself.epsilon = 0.0001\n",
        "\t\tself.iteration = 0\n",
        "\n",
        "\n",
        "\tdef state_utility(self, s):\n",
        "\t\t\"\"\"Return the state utility.\n",
        "\n",
        "\t\t@return the utility of the state\n",
        "\t\t\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef expected_action(self):\n",
        "\t\t\"\"\"Return the expected action.\n",
        "\n",
        "\t\tIt returns an action based on the\n",
        "\t\texpected utility of doing a in state s,\n",
        "\t\taccording to T and u. This action is\n",
        "\t\tthe one that maximize the expected\n",
        "\t\tutility.\n",
        "\n",
        "\t\t@return expected action (int)\n",
        "\t\t\"\"\"\n",
        "\t\tactions = np.zeros(4)\n",
        "\n",
        "\t\tfor action in range(4):\n",
        "\t\t\t#Expected utility of doing a in state s, according to T and u.\n",
        "\t\t\tactions[action] = np.sum(np.multiply(self.values, np.dot(self.states, self.transits[:,:,action])))\n",
        "\n",
        "\t\treturn np.argmax(actions)\n",
        "\n",
        "\tdef generate_graph(utility_list):\n",
        "\t\t\"\"\"Given a list of utility arrays (one for each iteration)\n",
        "\t\tit generates a matplotlib graph and save it as 'output.jpg'\n",
        "\t\t\"\"\"\n",
        "\t\tname_list = ('(1,3)', '(2,3)', '(3,3)', '+1', '(1,2)', '#', '(3,2)', '-1', '(1,1)', '(2,1)', '(3,1)', '(4,1)')\n",
        "\t\tcolor_list = ('cyan', 'teal', 'blue', 'green', 'magenta', 'black', 'yellow', 'red', 'brown', 'pink', 'gray', 'sienna')\n",
        "\t\tcounter = 0\n",
        "\t\tindex_vector = np.arange(len(utility_list))\n",
        "\n",
        "\t\tfor state in range(12):\n",
        "\t\t\tstate_list = list()\n",
        "\t\t\tfor utility_array in utility_list:\n",
        "\t\t\t\tstate_list.append(utility_array[state])\n",
        "\n",
        "\t\tplt.plot(index_vector, state_list, color=color_list[state], label=name_list[state])\n",
        "\t\tcounter += 1\n",
        "\t\t#Adjust the legend and the axis\n",
        "\t\tplt.legend(loc='upper center', bbox_to_anchor=(0.5, 0.4), ncol=3, fancybox=True, shadow=True)\n",
        "\t\tplt.ylim((-1.1, +1.1))\n",
        "\t\tplt.xlim((1, len(utility_list)-1))\n",
        "\t\tplt.ylabel('Utility', fontsize=15)\n",
        "\t\tplt.xlabel('Iterations', fontsize=15)\n",
        "\t\tplt.savefig(\"./output.jpg\", dpi=500)\n",
        "\n",
        "\tdef print_policy(p, shape):\n",
        "\t\t\"\"\"Printing utility.\n",
        "\n",
        "\t\tPrint the policy actions using symbols:\n",
        "\t\t^, v, <, > up, down, left, right\n",
        "\t\t* terminal states\n",
        "\t\t# obstacles\n",
        "\t\t\"\"\"\n",
        "\t\tcounter = 0\n",
        "\t\tpolicy_string = \"\"\n",
        "\t\tfor row in range(shape[0]):\n",
        "\t\tfor col in range(shape[1]):\n",
        "\t\tif(p[counter] == -1): policy_string += \" *  \"\n",
        "\t\telif(p[counter] == 0): policy_string += \" ^  \"\n",
        "\t\telif(p[counter] == 1): policy_string += \" <  \"\n",
        "\t\telif(p[counter] == 2): policy_string += \" v  \"\n",
        "\t\telif(p[counter] == 3): policy_string += \" >  \"\n",
        "\t\telif(np.isnan(p[counter])): policy_string += \" #  \"\n",
        "\t\tcounter += 1\n",
        "\t\tpolicy_string += '\\n'\n",
        "\t\tprint(policy_string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6gGsywLXVeTU"
      },
      "outputs": [],
      "source": [
        "mdp = MDP()\n",
        "\n",
        "#List containing the data for each iteation\n",
        "graph_list = list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "while True:\n",
        "\tdelta = 0\n",
        "\t# u = mdp.values\n",
        "\tu_old = mdp.values.copy()\n",
        "\tmdp.iteration += 1\n",
        "\tgraph_list.append(u_old)\n",
        "\tfor s in range(12):\n",
        "\tmdp.states = np.zeros((1,mdp.num_states))\n",
        "\tmdp.states[0,s] = 1.0\n",
        "\tmdp.values[s] = mdp.state_utility(s)\n",
        "\tu = mdp.values\n",
        "\tdelta = max(delta, np.abs(u[s] - u_old[s])) #Stopping criteria\n",
        "\tif delta < mdp.epsilon * (1 - mdp.gamma) / mdp.gamma:\n",
        "\t\tprint(\"=================== FINAL RESULT ==================\")\n",
        "\t\tprint(\"Iterations: \" + str(mdp.iteration))\n",
        "\t\tprint(\"Delta: \" + str(delta))\n",
        "\t\tprint(\"Gamma: \" + str(mdp.gamma))\n",
        "\t\tprint(\"Epsilon: \" + str(mdp.epsilon))\n",
        "\t\tprint(\"===================================================\")\n",
        "\t\tprint(u[0:4])\n",
        "\t\tprint(u[4:8])\n",
        "\t\tprint(u[8:12])\n",
        "\t\tprint(\"===================================================\")\n",
        "\t\tfor ss in range(12):\n",
        "\t\t\tif not np.isnan(mdp.policy[ss]) and not mdp.policy[ss]==-1:\n",
        "\t\t\t\tmdp.states = np.zeros((1,12))\n",
        "\t\t\t\tmdp.states[0,ss] = 1.0\n",
        "\t\t\t\t#2- Policy improvement\n",
        "\t\t\t\ta = mdp.expected_action()\n",
        "\t\t\t\tif a != mdp.policy[ss]: mdp.policy[ss] = a\n",
        "\t\t\tprint_policy(mdp.policy, shape=(3,4))\n",
        "\t\tprint(\"===================================================\")\n",
        "\n",
        "\tbreak\n",
        "\tgenerate_graph(graph_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solution\n",
        "\n",
        "Pseudo-code for Value Iteration algorithm:\n",
        "\n",
        "1. denoting the value function of each state by $V(s)$ and $V$ the array containing the value of each state, we randomly initialize $V$ (for example $V(s) = 0$ for all $s$)\n",
        "\n",
        "2. Repeat:\n",
        "    \n",
        "    $\\Delta \\leftarrow 0$ \n",
        "    \n",
        "    For each state $s$ in $S$:     \n",
        "    \n",
        "    $$ V(s) \\leftarrow \\max_{a \\in A} \\sum_{s' \\in \\mathbb{S}, r \\in \\mathbb{R}} P(s', r | s, a) \\cdot [r + \\gamma \\cdot V(s')]$$\n",
        "    \n",
        "    \n",
        "    $$\\Delta \\leftarrow \\max(\\Delta, |V(s) - V'(s)|)$$\n",
        "\n",
        "    Until $ \\Delta < \\epsilon$ ( $\\epsilon$ is a small number)\n",
        "\n",
        "\n",
        "3. Policy extraction:\n",
        "\n",
        "    For each state $s$ in $S$: \n",
        "        \n",
        "    $$\\pi(s) \\leftarrow \\arg\\max_{a \\in A} \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma V(s')]$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

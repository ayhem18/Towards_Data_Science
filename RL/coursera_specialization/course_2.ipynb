{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Sampling Methods\n",
    "Generally any method based on averaging complete returns is referred to as Carlo Monte results. Using complete limits the method's scope to solely episodic tasks. \n",
    "The main idea here is to let our agent learn from experience.\n",
    "## Monte Carlo Prediction \n",
    "### State Value function Prediction\n",
    "Let's start with predicting the value function. There is 1 algorithm with 2 variants: \n",
    "1. First visit prediction\n",
    "2. Every visit prediction\n",
    "Let's start with the first:\n",
    "\n",
    "* Input $\\pi$ to be evaluated\n",
    "1. random initialization of V(s), initialize the returns with empty list for each state $s$\n",
    "2. Loop:\n",
    "    * Generate Episode : $S_0, A_0, R_1, ... S_T, A_T, R_T$\n",
    "    * G = 0\n",
    "    * for all steps in the episode: $ t = T - 1, T - 2 ... 0 $: \n",
    "        * $G \\leftarrow \\gamma \\cdot G + R_{t+1}$\n",
    "        * if $t$ is the first occurence of $S_t$: append $G$ to returns($S_t$)\n",
    "    * V(S_t) = avg(returns($S_t$))\n",
    "\n",
    "The 2nd variant simply appends all returns regardless of their order of appearance.\n",
    "\n",
    "* There are several advantages over $DP$ introduced by the Monte Carlo approach. Unlike DP we do not need the complete dynamic of the environment.\n",
    "* The value function of a given state is computed independently of other states. This is particularly useful if only a subset of states is of interest.\n",
    "* The computation needed to update the value of each state does not depend on the size of the DMP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Value function Prediction\n",
    "In the absence of a model, we cannot use the approach above as there is no states. In this case, we focus on actions or state-action pairs. The algorithm is basically the same. Nevertheless, for a given policy, it is quite possible to have some of state-action pairs to never be visited which breaks the method.  \n",
    "\n",
    "This serious issue is solved by randomly choosing the initial state-action pair which guarantees that any pair will be visited an infite number of times for an infitied number of sequences. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monto Carlo ES Policy Control\n",
    "1. Initialize :\n",
    "    * $\\pi(s)$ a random policy\n",
    "    * random state-action value functions\n",
    "2. Loop:\n",
    "    1. Choose $S_0, A_0$ such that each pair has a probability of being chosen $p > 0$\n",
    "    2. Generate an episode: $S_0, A_0, R_1, S_1, A_1... S_{T - 1}, A_{T - 1}, R_T$\n",
    "    3. loop through each step in the episode: $i = T, T - 1, T - 2 ..., 1, 0$: \n",
    "        * $G \\leftarrow \\gamma \\cdot G + R_t$\n",
    "        * if the pair $S_t, A_t$ appears for the first time\n",
    "            * estimate $Q(S_t, A_t)$ \n",
    "            * Assign the policy $\\pi(S_t) = argmax_a Q(S_t, A_t) $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem with Exploring Starts\n",
    "Even though the idea of having non-zero probabilities for each $action, state$ pair in the environment can be seen as somewhat a solution, it cannot be seen as a general and absolute solution mainly when the agent is interacting with an actual environment. We will consider 2 approachs to tackle such problem\n",
    "### On policy learning\n",
    "In the first few sections, we considered possible techniques to embed exploration in the policy such as $\\epsilon$ greedy methods. The same technique can be applied to the Monte Carlo approach. The updated pseudo-code will be as follows:\n",
    "\n",
    "1. Initialize :\n",
    "    * $\\pi(s)$ a random $\\epsilon$ soft policy \n",
    "    * random state-action value functions\n",
    "\n",
    "2. Loop:\n",
    "    1. Choose $S_0, A_0$ randomly\n",
    "    2. Generate an episode: $S_0, A_0, R_1, S_1, A_1... S_{T - 1}, A_{T - 1}, R_T$\n",
    "    3. loop through each step in the episode: $i = T, T - 1, T - 2 ..., 1, 0$:\n",
    "        * $G \\leftarrow \\gamma \\cdot G + R_t$\n",
    "        * if the pair $S_t, A_t$ appears for the first time\n",
    "            * estimate $Q(S_t, A_t)$ \n",
    "            * update the policy as follows: \n",
    "            $ \\pi(a | S_t) \\leftarrow \\begin{equation}\n",
    "                \\begin{cases}\n",
    "                1 - \\epsilon + \\frac{\\epsilon}{|A(s)|}  && \\text{if } a = A^{*}\\\\\n",
    "                \\frac{\\epsilon}{|A(s)|} && \\text{if } a \\neq A^{*}\n",
    "                \\end{cases}\n",
    "            \\end{equation}\n",
    "            $\n",
    "\n",
    "This approach does not reach the task optimal policy (as such policy is deterministic). Nevertheless, with smaller $\\epsilon$ and more training episodes, the difference in `between the 2 policies cannot be significant which is a small price to pay for overcoming the impracticality of exploring starts. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off policy Control\n",
    "The off policy is still confusing to me at the moment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference LEARNING\n",
    "This is one of the central ideas in RL. First let's consider an improvement over the MC update version as follows:\n",
    "$$V(S_t) = V(S_t) + \\alpha \\cdot (G_t - V(S_t))$$\n",
    "as we would like $V(S_t)$ to convert to the expected value of $G_t$. \n",
    "\n",
    "TD(0) is a powerful algorithm described as follows:\n",
    "* Input: the policy $\\pi$ to be evaluated, step size $\\alpha \\in (0, 1]$\n",
    "1. Initialize $V(s)$ randomly for each state $s$ \n",
    "2. Loop:\n",
    "    1. initialize $S$\n",
    "    2. $A \\leftarrow$ action given by $\\pi$\n",
    "    3. Take action $A$, observe $R$ and next state $S$\n",
    "    4. $V(s) \\leftarrow V(s_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(s_t)) $\n",
    "    5. $S \\leftarrow S^{'}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind $ V(s_t) = V(s_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(s_t))$ is that $R_{t+1} + \\gamma V(S_{t+1})$ is somewhat of an estimation of $G_t$.  \n",
    "\n",
    "Another important identity:  \n",
    "$$G_t - V(S_t) = \\sum_{k = t}^{T - 1} \\gamma ^{k - t} \\delta_k$$\n",
    "where $\\delta_t = R_{t+1} + \\gamma \\cdot V(S_{t+1}) - V(S_t)$. The identity does not hold when $V(S_t)$ is updated as it is in TD, but when the step size $\\alpha$ is small enough, it can be seen as a good approximation.\n",
    "\n",
    "The main advantage introduced by DT is waiting only for the next time step and not for an entire episode which is quite a critical point in real applications. Certain tasks have significanlty long episodes which makes MC methods too slow. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-Control\n",
    "### On policy Control\n",
    "Policy control is usually applied to action state value function as the latter. Thus, we need to derive the update rule for action-state pairs. The derivation is pretty straightforward, so we will include in the main algorithm directly:\n",
    "* Input: policy: $\\pi$, step size: $\\alpha \\in (0, 1]$\n",
    "1. random initialization for $Q(s, a)$ except for pairs $Q(Terminal, s) = 0$\n",
    "2. choose initial pair $A, S$ using policy derived from $Q$: mainly $\\epsilon$-greedy.\n",
    "3. Loop:\n",
    "    1. Take action $A$, observe $R$, $S^{'}$  \n",
    "    2. choose $A^{'}$ and $S'$  \n",
    "    3. $Q(S, A) \\leftarrow Q(S, A) + \\alpha \\cdot [R + \\gamma \\cdot Q(S', A') - Q(S, A)]$\n",
    "    4. $S\\leftarrow S'$ , $A \\leftarrow A'$\n",
    "4. until $S$ is terminal  \n",
    "Having the value of $Q(S, a)$ can be seen as the control, as the final policy can be simply an $\\epsilon$-greedy policy using the values $Q$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off Policy Control\n",
    "Q-learning can be considered as one of the major breakthroughs in Reinforcement Learning. It is somehow based on the bellman optimality equation and thus, estimates $q^{*}$ regardless of the policy being followed:\n",
    "\n",
    "* Input: policy: $\\pi$, step size: $\\alpha \\in (0, 1]$\n",
    "1. random initialization for $Q(s, a)$ except for pairs $Q(Terminal, s) = 0$\n",
    "2. choose initial pair $A, S$ using policy derived from $Q$: mainly $\\epsilon$-greedy.\n",
    "3. Loop:\n",
    "    1. Take action $A$, observe $R$, $S^{'}$    \n",
    "    2. $Q(S, A) \\leftarrow Q(S, A) + \\alpha \\cdot [R + \\gamma \\cdot max_a~Q(S', a) - Q(S, A)]$\n",
    "    3. $S\\leftarrow S'$\n",
    "4. until $S$ is terminal  \n",
    "\n",
    "The main difference here is using the maximum action-state value function based on the next state $S'$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain the reason why Q-learning is consider Off-Policy learning. One main point is that it acts using the given $Q$. Nevertheless, it does not the term $Q(S_{t+1}, A_{t+1})$ in the update. In other words, the target policy is not the same as the behavioral policy.   \n",
    "\n",
    "On the other hand, Sarsa is consider on policy as it updates $Q$ while using the action-state value functions from $Q$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa\n",
    "At least from an algorithmic point of view, the only difference is the update equation:\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha\\cdot (R_{t+1} + \\gamma \\sum_{a' \\in A(S_{t+1})} \\pi(a' | S_{t+1}) \\cdot Q(S_{t+1}, a') - Q(S_t, A_t))$$ \n",
    "\n",
    "The update is much more stable than Sarsa as the updates are of lower variance, but with the downside of increased computation.\n",
    "We can say that expected Sarsa is more robust to larger values of the step size. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Models\n",
    "The term ***model*** should be seen as any mechanism that enables the agent to predict the environment's state and rewards. There are 2 main types of models:\n",
    "1. distribution models: They generate exact probabilites for every transition, providing complete knowledge of the environment\n",
    "2. sample models: The transitions are estimated by aggregating over the samples taken from the environment.\n",
    "\n",
    "## Dyna Q:\n",
    "The term planning should be thought of as acting / choosing a state-action pair based on the model we built for the environment. One path to improve ***Q-Learning*** is to introduce a planning component. How you might ask ? For each episode of interaction with the word, the model saves the transitions, the rewards and different states. Using this knownledge, we can simluate the real experience and update the state action funtion. (A sampling policy should be determined).   \n",
    "Let's formalize this idea with the following algorithm:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Initialize $Q(s, a)$ and $Model(s, a)$ for all $s \\in S$ and $a \\in A$\n",
    "2. Loop:\n",
    "    1. S current state\n",
    "    2. $A \\leftarrow \\epsilon$-greedy action\n",
    "    3. Take action $A$ observer, $R, S'$\n",
    "    4. Update $Q(S, A)$ according to ***Q-Learning*** update: \n",
    "        $$Q(S, A) \\leftarrow Q(S, A) + \\alpha\\cdot (R + \\gamma \\cdot max_{a'} Q(S', a')  - Q(S, A))$$ \n",
    "    5. $Model(S, A) \\leftarrow R, S'$ assuming the environment is deterministic\n",
    "    6. Loop for $n$ times: ($n$ number of planning iterations):\n",
    "        1. $S \\leftarrow$ an action previously seen\n",
    "        2. $A \\leftarrow$ an action seen with $S$\n",
    "        3. $R, S' \\leftarrow Model(A, S)$: $~$ the deterministic outcome of state action pair $A, S$\n",
    "        4. Update $Q(S, A)$ using ***Q-Learning*** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the Dyna Q-Learning better leverages the limited experience the agent acquires from interacting with the environment. This is specially crucial in practical tasks where the interaction can be quite expensive. \n",
    "## Changing Environment\n",
    "Relying heavily on the model raises the natural question: what if the model is unaccurate ?. Well This exact question is just an extension of the exploitation and exploration tradeoff as the model can change with time and the agent should take into account such possibilities.   \n",
    "One way to address this issue is to add an exploitation component in the planning update formula.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Multi Armed Bandits\n",
    "Let's explore the different ideas of Reinforcement Learning.\n",
    "\n",
    "## k-armed bandit problem\n",
    "At each timestamp, we need to choose 1 out of $k$ possible choices. Each option is associated with a reward. This value can be seen as a result of some hidden probability distribution. Let's consider refer to its expected value as the **value** of an action. Assuming $A_t$ is the action taken at timestamp $t$ and $R_t$ the corresponding reward, then \n",
    "$$ q_{*}(a) = E[R_t | a = A_t]$$\n",
    "At each timestamp, we have an estimation of the reward value of a given action, say $Q_t(a)$. There 3 points\n",
    "1. we would like $Q_t(a)$ to be as close as possible to $q_{*}(a)$\n",
    "2. As the number of actions is finite, we can know which action has the most immediate expected / esimated reward. \n",
    "3. Choosing the action with the largest expected estimated reward is referred to as **exploitation**. Otherwise, it is called **exploration**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Action-value methods\n",
    "As mentioned above, among the most important components, aspects of RL is to estimate the rewards of different actions. The most straightforward way is calculating the average of rewards when choosing that exact action: \n",
    "$\\begin{align} \n",
    "Q_t(a) = \\frac{\\sum_{i=1}^{t - 1} R_i \\cdot 1_{A_i = a}}{\\sum_{i=1}^{t - 1}  1_{A_i = a}}\n",
    "\\end{align}$\n",
    "where this is translated as the fraction of the: sum of rewards at time stamps where the action $a$ was taken, over the number of times the action $a$ was taken.   \n",
    "There are different ways to estimate the rewards at a given timestamp. Nevertheless, we would elaborate on that later.  \n",
    "The greedy approach is to choose $a$ such that:\n",
    "$$ A_t = argmax_a Q_t(a)$$ \n",
    "A slightly asymptotically better policy is choosing the greedy option only $1 - \\epsilon$ of the time, while choosing a random option out of all options in the $\\epsilon$ left.   \n",
    "A computational remark: it is enough to save the number of times / steps each action was chosen, and the sum of rewards up till the current timestamp. Introducing some notation, we have the following:\n",
    "1. $Q_n = \\frac{\\sum_{i = 1}^{n - 1} R_i}{n - 1}$, where $R_i$ is the reward at the $i$-th timestamp\n",
    "2. $Q_{n + 1} = \\frac{\\sum_{i = 1}^{n} R_i}{n} = \\frac{R_n + (n - 1) \\cdot \\frac{\\sum_{i = 1}^{n - 1} R_i}{n-1}}{n}$\n",
    "3. $Q_{n + 1} = \\frac{R_n + Q_n (n - 1)}{n}= Q_n + \\frac{R_n - Q_n}{n}$\n",
    "\n",
    "The last equation provides a framework of the efficient computation of rewards and estimated rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sounds like time to code our problems and have some fun with it\n",
    "from typing import Union\n",
    "from _collections_abc import Sequence\n",
    "import numpy as np\n",
    "import random\n",
    "# set the seeds\n",
    "\n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "\n",
    "class kArmBandit:\n",
    "    def __init__(self, k: int=5, rewards_means: list[Union[float, int]]=None, rewards_variances: Union[float, list[float]]=None) -> None:\n",
    "        # number of options\n",
    "        self.k = k\n",
    "        # the means parameters must be a sequence of floats or integers of size k\n",
    "        assert rewards_means is None or (isinstance(rewards_means, Sequence) and len(rewards_means) == k)\n",
    "        # the default values for the mean values are random\n",
    "\n",
    "        if rewards_means is None:        \n",
    "            rewards_means = [np.random.uniform(-5, 5) for _ in range(k)]\n",
    "\n",
    "        if rewards_variances is None:\n",
    "            rewards_variances = [1 for _ in range(k)]\n",
    "\n",
    "        if isinstance(rewards_means, float) or isinstance(rewards_variances, int):\n",
    "            rewards_variances = [rewards_variances for _ in range(k)]\n",
    "\n",
    "        assert isinstance(rewards_variances, Sequence) and len(rewards_variances) == k\n",
    "\n",
    "        self.rewards = [lambda: np.random.normal(mean, variance, 1) for mean, variance in zip(rewards_means, rewards_variances)]\n",
    "\n",
    "    def reward(self, option_number: int):\n",
    "        assert option_number < self.k, 'The option chosen is not available by the arm bandit'\n",
    "        return self.rewards[option_number]()\n",
    "\n",
    "    def reward_and_optimal(self, option_number: int):\n",
    "        # first extract all the possible rewards\n",
    "        all_rewards = [r() for r in self.rewards]\n",
    "        # return the reward chosen and the maximum one\n",
    "        return self.rewards(option_number), max(all_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create an agent class for that\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, k: int, epsilon:float=0, start_estimate:float=0, update_estimate: callable=None) -> None:\n",
    "        self.k = k\n",
    "        # make sure epsilon is in the range [0, 1]\n",
    "        assert 0 <= epsilon <= 1\n",
    "        self.epsilon = epsilon # determines how often does the agent make a random non-greedy choice\n",
    "        self.estimates = [start_estimate for _ in range(self.k)]\n",
    "        self.step_sizes = [1 for _ in range(self.k)]\n",
    "\n",
    "        # the default update estimate value is 1 / step_size\n",
    "        # this is used for average sampling\n",
    "        if update_estimate is None:\n",
    "            update_estimate = lambda step_size: 1 / step_size\n",
    "        \n",
    "        self.update_estimate =  update_estimate  \n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def chooose(self):\n",
    "        p = random.random()\n",
    "        if p < self.epsilon:\n",
    "            # return a random choice\n",
    "            return random.choice(list(range(self.k)))\n",
    "        else:\n",
    "            # choose the option with the highest estimate\n",
    "            return np.argmax(self.estimates)\n",
    "    \n",
    "    def update_estimates(self, option_num:int, option_reward:float):\n",
    "        # make sure to add the reward to the total_reward\n",
    "        self.total_reward += option_reward\n",
    "        # make sure to update the estimated value of the corresponding reward\n",
    "        e = self.estimates[option_num] # writing the entire expression repeatedly is not the best option\n",
    "        self.estimates[option_num] = (e + self.update_estimate(self.step_sizes[option_num]) * (option_reward - e)) \n",
    "        # increment the step size for the corresponding option\n",
    "        self.step_sizes[option_num] += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to create a different class for experimenting with the different settings of the Agent and the ArmBadint\n",
    "class Experiment:\n",
    "    def __init__(self, agent: Agent, arm_bandit: kArmBandit, steps: int=1000, exps:int=100) -> None:\n",
    "        # first make sure they have they operate on the same number of options\n",
    "        assert agent.k == arm_bandit.k, \"The agent and the armbandit must have the same number of options\"\n",
    "        self.agent = agent\n",
    "        self.bandit = arm_bandit\n",
    "        # how many will the agent have to choose an option at each trials\n",
    "        self.steps = steps\n",
    "        # the number of trials\n",
    "        self.exps = exps\n",
    "\n",
    "    def __run_trial(self):\n",
    "        # each trial should calculate the\n",
    "        # 1. the average reward at each step\n",
    "        # 2. the percentage of the optimal rewards so far\n",
    "        pass        \n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values\n",
    "* One approach to driver the agent to explore more is setting optimistic initial action reward estimates. The main reasoning is that the agent will be disappointed with rewards driving him to explore each of the actions several times. This approach is quite limited: \n",
    "1. It only drives exploration in the early steps: The initial values will wash off as the agent continues exploring\n",
    "2. Any approach based on initial values cannot be generalized to non-stationary problems\n",
    "3. settings such ***optimistic*** values might not be known before hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonstationary Problems\n",
    "The above-mentioned approach works only for problems where the expected value of an action is constant. Nevertheless, they are far from perfect for non-stationary rewards. Among the possible approaches is using a constant weight to calculate the new term with respect to the previous estimate. In other words:\n",
    "$$ Q_{n + 1} = Q_n + \\alpha \\cdot (R_n - Q_n)$$\n",
    "The latter can be expanded:\n",
    "$$Q_{n + 1} = (1 - \\alpha) ^ n \\cdot Q_1 + \\sum_{i = 1}^{n} \\alpha \\cdot (1 - \\alpha) ^ {n - i} R_i$$\n",
    "Meaning that the most recent rewards are more significant in the estimation of the next reward.\n",
    "\n",
    "## Upper-confidence Bound action selection\n",
    "Exploration is always needed mainly becuase the estimation of the next actions are always uncertain. The $\\epsilon$-greedy approach might force exploration but undescriminately. Such exploration approach is obviously far from optimal. One way to improve our exploration process is to use the UPPER CONFIDENCE BOUND action selection mechanism:\n",
    "$$ A_t = argmax_a [Q_{t}(a) + c \\cdot \\sqrt{\\frac{\\ln(t)}{N_t(a)}}] $$\n",
    "where the first term is clearly exploitation, while the 2nd is exploration:  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

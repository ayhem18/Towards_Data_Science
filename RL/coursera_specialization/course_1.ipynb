{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Multi Armed Bandits\n",
    "Let's explore the different ideas of Reinforcement Learning.\n",
    "\n",
    "## k-armed bandit problem\n",
    "At each timestamp, we need to choose 1 out of $k$ possible choices. Each option is associated with a reward. This value can be seen as a result of some hidden probability distribution. Let's consider refer to its expected value as the **value** of an action. Assuming $A_t$ is the action taken at timestamp $t$ and $R_t$ the corresponding reward, then \n",
    "$$ q_{*}(a) = E[R_t | a = A_t]$$\n",
    "At each timestamp, we have an estimation of the reward value of a given action, say $Q_t(a)$. There 3 points\n",
    "1. we would like $Q_t(a)$ to be as close as possible to $q_{*}(a)$\n",
    "2. As the number of actions is finite, we can know which action has the most immediate expected / esimated reward. \n",
    "3. Choosing the action with the largest expected estimated reward is referred to as **exploitation**. Otherwise, it is called **exploration**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Action-value methods\n",
    "As mentioned above, among the most important components, aspects of RL is to estimate the rewards of different actions. The most straightforward way is calculating the average of rewards when choosing that exact action: \n",
    "$\\begin{align} \n",
    "Q_t(a) = \\frac{\\sum_{i=1}^{t - 1} R_i \\cdot 1_{A_i = a}}{\\sum_{i=1}^{t - 1}  1_{A_i = a}}\n",
    "\\end{align}$\n",
    "where this is translated as the fraction of the: sum of rewards at time stamps where the action $a$ was taken, over the number of times the action $a$ was taken.   \n",
    "There are different ways to estimate the rewards at a given timestamp. Nevertheless, we would elaborate on that later.  \n",
    "The greedy approach is to choose $a$ such that:\n",
    "$$ A_t = argmax_a Q_t(a)$$ \n",
    "A slightly asymptotically better policy is choosing the greedy option only $1 - \\epsilon$ of the time, while choosing a random option out of all options in the $\\epsilon$ left.   \n",
    "A computational remark: it is enough to save the number of times / steps each action was chosen, and the sum of rewards up till the current timestamp. Introducing some notation, we have the following:\n",
    "1. $Q_n = \\frac{\\sum_{i = 1}^{n - 1} R_i}{n - 1}$, where $R_i$ is the reward at the $i$-th timestamp\n",
    "2. $Q_{n + 1} = \\frac{\\sum_{i = 1}^{n} R_i}{n} = \\frac{R_n + (n - 1) \\cdot \\frac{\\sum_{i = 1}^{n - 1} R_i}{n-1}}{n}$\n",
    "3. $Q_{n + 1} = \\frac{R_n + Q_n (n - 1)}{n}= Q_n + \\frac{R_n - Q_n}{n}$\n",
    "\n",
    "The last equation provides a framework of the efficient computation of rewards and estimated rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sounds like time to code our problems and have some fun with it\n",
    "from typing import Union\n",
    "from _collections_abc import Sequence\n",
    "import numpy as np\n",
    "import random\n",
    "# set the seeds\n",
    "\n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "\n",
    "class kArmBandit:\n",
    "    def __init__(self, k: int=5, rewards_means: list[Union[float, int]]=None, rewards_variances: Union[float, list[float]]=None) -> None:\n",
    "        # number of options\n",
    "        self.k = k\n",
    "        # the means parameters must be a sequence of floats or integers of size k\n",
    "        assert rewards_means is None or (isinstance(rewards_means, Sequence) and len(rewards_means) == k)\n",
    "        # the default values for the mean values are random\n",
    "\n",
    "        if rewards_means is None:        \n",
    "            rewards_means = [np.random.uniform(-5, 5) for _ in range(k)]\n",
    "\n",
    "        if rewards_variances is None:\n",
    "            rewards_variances = [1 for _ in range(k)]\n",
    "\n",
    "        if isinstance(rewards_means, float) or isinstance(rewards_variances, int):\n",
    "            rewards_variances = [rewards_variances for _ in range(k)]\n",
    "\n",
    "        assert isinstance(rewards_variances, Sequence) and len(rewards_variances) == k\n",
    "\n",
    "        self.rewards = [lambda: np.random.normal(mean, variance, 1) for mean, variance in zip(rewards_means, rewards_variances)]\n",
    "\n",
    "    def reward(self, option_number: int):\n",
    "        assert option_number < self.k, 'The option chosen is not available by the arm bandit'\n",
    "        return self.rewards[option_number]()\n",
    "\n",
    "    def reward_and_optimal(self, option_number: int):\n",
    "        # first extract all the possible rewards\n",
    "        all_rewards = [r() for r in self.rewards]\n",
    "        # return the reward chosen and the maximum one\n",
    "        return self.rewards(option_number), max(all_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create an agent class for that\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, k: int, epsilon:float=0, start_estimate:float=0, update_estimate: callable=None) -> None:\n",
    "        self.k = k\n",
    "        # make sure epsilon is in the range [0, 1]\n",
    "        assert 0 <= epsilon <= 1\n",
    "        self.epsilon = epsilon # determines how often does the agent make a random non-greedy choice\n",
    "        self.estimates = [start_estimate for _ in range(self.k)]\n",
    "        self.step_sizes = [1 for _ in range(self.k)]\n",
    "\n",
    "        # the default update estimate value is 1 / step_size\n",
    "        # this is used for average sampling\n",
    "        if update_estimate is None:\n",
    "            update_estimate = lambda step_size: 1 / step_size\n",
    "        \n",
    "        self.update_estimate =  update_estimate  \n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def chooose(self):\n",
    "        p = random.random()\n",
    "        if p < self.epsilon:\n",
    "            # return a random choice\n",
    "            return random.choice(list(range(self.k)))\n",
    "        else:\n",
    "            # choose the option with the highest estimate\n",
    "            return np.argmax(self.estimates)\n",
    "    \n",
    "    def update_estimates(self, option_num:int, option_reward:float):\n",
    "        # make sure to add the reward to the total_reward\n",
    "        self.total_reward += option_reward\n",
    "        # make sure to update the estimated value of the corresponding reward\n",
    "        e = self.estimates[option_num] # writing the entire expression repeatedly is not the best option\n",
    "        self.estimates[option_num] = (e + self.update_estimate(self.step_sizes[option_num]) * (option_reward - e)) \n",
    "        # increment the step size for the corresponding option\n",
    "        self.step_sizes[option_num] += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to create a different class for experimenting with the different settings of the Agent and the ArmBadint\n",
    "class Experiment:\n",
    "    def __init__(self, agent: Agent, arm_bandit: kArmBandit, steps: int=1000, exps:int=100) -> None:\n",
    "        # first make sure they have they operate on the same number of options\n",
    "        assert agent.k == arm_bandit.k, \"The agent and the armbandit must have the same number of options\"\n",
    "        self.agent = agent\n",
    "        self.bandit = arm_bandit\n",
    "        # how many will the agent have to choose an option at each trials\n",
    "        self.steps = steps\n",
    "        # the number of trials\n",
    "        self.exps = exps\n",
    "\n",
    "    def __run_trial(self):\n",
    "        # each trial should calculate the\n",
    "        # 1. the average reward at each step\n",
    "        # 2. the percentage of the optimal rewards so far\n",
    "        pass        \n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values\n",
    "* One approach to driver the agent to explore more is setting optimistic initial action reward estimates. The main reasoning is that the agent will be disappointed with rewards driving him to explore each of the actions several times. This approach is quite limited: \n",
    "1. It only drives exploration in the early steps: The initial values will wash off as the agent continues exploring\n",
    "2. Any approach based on initial values cannot be generalized to non-stationary problems\n",
    "3. settings such ***optimistic*** values might not be known before hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonstationary Problems\n",
    "The above-mentioned approach works only for problems where the expected value of an action is constant. Nevertheless, they are far from perfect for non-stationary rewards. Among the possible approaches is using a constant weight to calculate the new term with respect to the previous estimate. In other words:\n",
    "$$ Q_{n + 1} = Q_n + \\alpha \\cdot (R_n - Q_n)$$\n",
    "The latter can be expanded:\n",
    "$$Q_{n + 1} = (1 - \\alpha) ^ n \\cdot Q_1 + \\sum_{i = 1}^{n} \\alpha \\cdot (1 - \\alpha) ^ {n - i} R_i$$\n",
    "Meaning that the most recent rewards are more significant in the estimation of the next reward.\n",
    "\n",
    "## Upper-confidence Bound action selection\n",
    "Exploration is always needed mainly becuase the estimation of the next actions are always uncertain. The $\\epsilon$-greedy approach might force exploration but undescriminately. Such exploration approach is obviously far from optimal. One way to improve our exploration process is to use the UPPER CONFIDENCE BOUND action selection mechanism:\n",
    "$$ A_t = argmax_a [Q_{t}(a) + c \\cdot \\sqrt{\\frac{\\ln(t)}{N_t(a)}}] $$\n",
    "where the first term is clearly exploitation, while the 2nd is exploration:  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Finite Markov Decision Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Proceeses are formatlization of the RL problem for which, the action and the previous state completely determine the reward as well as the environment's next state. Some math for fun:\n",
    "$$\\begin{align}\n",
    "p(s',r | s, a)  = Pr(S_t = s, R_t = r | S_{t - 1}, A_{t - 1} = a)\n",
    "\\end{align}\n",
    "$$\n",
    "The assumption here is that the sets of rewards, states and actions are finite and therefore this equality holds: \n",
    "$$\\sum_{r \\in R} \\sum_{s' \\in S} p(s', r | s, a) = 1$$\n",
    "\n",
    "Certain environments and RL tasks can be broken into repeatable sections / cycles called episodes. For such problems, we are trying to maximize the mathematical expression:\n",
    "$$ G_t = \\sum_{i}^T R_{t + i}$$\n",
    "Where $T$ is the episode length. The main issue here is that such episodes are not guaranteed to exist. Thus, we introduce a better formulation:\n",
    "\n",
    "The main characteristics of a periodic task is:\n",
    "1. The terminal state where the episode cannot proceed further.\n",
    "2. The different episods are independent and the start state can be one of predefinied states\n",
    "\n",
    "Nevertheless, not all tasks can be modeled as periodic, as they don't break naturally into periods. Therefore, The objective should be changed accordingly:\n",
    "$$ G_t = \\sum_{k = 0}^{\\infin } \\gamma ^ k \\cdot R_{t + k + 1}$$\n",
    "Where the $ 0 \\leq \\gamma < 1$ is simply a discounting factor reflecting the imprtance of long-term rewards. The lower the values, the less far-sighted the model, as the future rewards will soon be practically nullified by the discounting factor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies and Value Functions\n",
    "RL agents generally are built upon values functions: mathematical functions that estimate the quality of a given (current) state. In additional to value functions, we have policies: $\\pi(a|s)$ which determine the probability of a given action $a$ in a given state $s$. The mind of the agent is literally its policy. Learning can manisfest itself in creating a policy that assigns high probabilities to states with high rewards. The mathematical expression of high reward is basically the expected value of the random variable $G_t$ given a state at any point in time.\n",
    "$$ v_{\\pi}(s) = E_{\\pi}[G_t|S_t = s]$$\n",
    "The value of a terminal state if any is always $0$. Extending the definition a bit further, we can define action-value function: \n",
    "$$ q_{\\pi}(s, a) = E_{\\pi}[G_t|S_t = t, A_t = a]$$\n",
    "Such expressions are not of much use as they don't offer (at least immediatly) practical means to calculate them. Let's use one simple identity:\n",
    "$$G_t = R_{t + 1} + \\gamma \\cdot G_{t + 1}$$\n",
    "Replacing this expression in the expression of $v_{\\pi}$, we obtain:\n",
    "\n",
    "$\\begin{align} \n",
    "v_{\\pi}(s) &= E_{\\pi}[G_t|S_t = s] \\\\\n",
    "            &= E_{\\pi}[R_{t + 1} + \\gamma \\cdot G_{t + 1} | S_t = s] \\\\\n",
    "            &= \\sum_{a} \\pi(a | s) \\cdot \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot E_{\\pi}[G_{t + 1} | S_{t + 1} = s']] \\\\\n",
    "v_{\\pi}(s)  &= \\sum_{a} \\pi(a | s) \\cdot \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}(s')]\n",
    "\\end{align}$\n",
    "This formula known as the Bellman equation gives us a way of computing $v_{\\pi}(s)$ in terms of following states $s'$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the same equation but for state-action pair: $q_{\\pi}(s, a) = E_{\\pi}[G_t|(S_t = s, A_t = a)]$.\n",
    "$\\begin{align} \n",
    "q_{\\pi}(s, a) &= E_{\\pi}[G_t|(S_t = s, A_t = a)] \\\\\n",
    "            &= E_{\\pi}[R_{t + 1} + \\gamma \\cdot G_{t + 1} | S_t = s, A_t = a] \\\\\n",
    "            &= \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot E_{\\pi}[G_{t + 1} | S_{t + 1} = s']] \\\\\n",
    "q_{\\pi}(s, a)  &= \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\sum_{a' \\in A_t(s')} \\pi(a'|s')\\cdot q_{\\pi}(s', a')]\n",
    "\\end{align}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further down the line, we will consider different ways, to compute, estimate $v_{\\pi}(s)$ as well as $q_{\\pi}(s, a)$   \n",
    "Solving an RL problems could be seen as maximizing the policy's return on the long run. For a finite MDP, we can define an optimal value policy as follows:   \n",
    "policy $\\pi^{*}$ such as $v_{\\pi^{*}}(s) \\geq v_{\\pi}(s)$ for all states $s$ and all policies $\\pi$.   \n",
    "An intuitive non-formal idea behind the proof of the existence of optimal policy is as follows:\n",
    "1. Assuming policy $\\pi_1$ has a better value function for a set of states $S_1$ while $\\pi_2$ has a better value function of a set of states $S_2$.\n",
    "2. The third policy $\\pi_3$ defined as $\\pi_1$ for $S_1$ and $\\pi_2$ for S_2 is a policy whose value function is larger than all value function for both $\\pi$'s \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given a policy $\\pi$ we can compute the value function of each state by solving a linear system of $|S|$ equations and $|S|$ unknowns. This is not enough to find the optimal policy as the number of possibilities is just untrackable. Nevertheless, the Bellman equations can still help up optimize the process.\n",
    "* Let's consider the following equations:\n",
    "\n",
    "$\\begin{align} \n",
    "v_{\\pi}^{*}(s)  &= \\sum_{a} \\pi(a | s) \\cdot \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}(s')] \\\\\n",
    " &= \\sum_{a} \\pi^{*}(a | s) \\cdot \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}^{*}(s')]  && \\text{The optimal policy should assign 1 to the action with the highest expected reward and 0 otherwise} \\\\\n",
    "v_{\\pi}^{*}(s) &= \\max_{a} \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}^{*}(s')]\n",
    "\\end{align}$\n",
    "Let's elaborate more on the last transition. The expression $S = \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}^{*}(s')]$ depends on $s', s, a, r$. A policy controls the distribution of $a$ given $s$. The choice of $a$ determines the possible values of $r$ and $s'$ and therefore, determine the value of $S$. We can simply find the action $a$ that maximizes $S$ assign it a probability $1$ and $0$ for the other actions assuring optimility.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the optimal state functions, we can easily find the optimal policy: For each state $s$, find all actions $a$ for which $\\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}^{*}(s')]$ is maximal. In other words,\n",
    "Even Better, $$\\pi^{*}(s) = argmax_{a}(\\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}^{*}(s')])$$\n",
    "$$\\pi^{*}(s) = argmax_{a}(q^{*}(a,s))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write $q^{*}(s,a)$ in terms of $v^{*}(s)$ as follows:\n",
    "$$ q^{*}(s,a) = \\sum_{s' \\in S(a), r \\in R} p(s', r |a, s) \\cdot [r + \\gamma \\cdot v^{*}(s')]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chpater 4: DP for Policy Evaluation and Control\n",
    "* policy evaluation is the task of finding the value function $V_{\\pi}$ given a policy $\\pi$. In other words, find $V_{\\pi}(s)$ for every state $s$ in the states' space.\n",
    "* policy control: is the task of improving an existing policy $\\pi$.\n",
    "* Both these tasks can be done if we have access to the dynamics function $p$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "Let's recall the Bellman equation for a state function:\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a | s) \\cdot \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}(s')]$$\n",
    "Simply we can consider $$V_{k + 1}(s) = \\sum_{a} \\pi(a | s) \\cdot \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{k}(s')]$$\n",
    "for every single state $s$.\n",
    "The algorithm is pretty simple:\n",
    "1. randomly initialize two arrays representing the states: $V_{old}$, $V_{new}$\n",
    "2. define the difference between the $2$ arrays\n",
    "3. update each state in $V_old$, when all states are updated copy $V_{new}$ to $V_{old}$\n",
    "4. stop when the difference between "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "Policy control: The task of improving a given policy. The main idea is to use the Bellman optimality equation on a given policy. So given $\\pi$, we will have $\\pi^{'}$ a new greedy policy that chooses the action with the highest value function accroding to $\\pi$\n",
    "$$\\pi^{'}(s) = argmax_a \\sum_{a} \\pi(a | s) \\cdot \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}(s')]$$ \n",
    "According to the policy improvement theorem, $\\pi^{'}$ is better than $\\pi$. In fact strictly better, unless $\\pi$ is already optimal.   \n",
    "Using policy control and policy we can build a general framework: policy iteration as follows:\n",
    "\n",
    "1. Random initialization of $\\pi$ and $v_\\pi$\n",
    "2. Policy Evaluation\n",
    "3. $\\pi_g$ = Policy Improvement($\\pi$)\n",
    "    * if $\\pi_g \\neq \\pi$, $\\pi = \\pi_g$ and go to .2\n",
    "    * else $\\pi$ is already optimal\n",
    "\n",
    "All $\\pi_g$ will be deterministic, the number of deterministic policies is finite, assuring the algorithm will eventually converge. Policy Iteration literally cuts through the search space which is quite powerful computationally.\n",
    "\n",
    "### Value Iteration\n",
    "The policy iteration presented above is quite flexible. and can be modified as needed depending on different scenarios. One way is ***Value Iteration***. Where We perform only one sweep (iteration) in policy evaluation using gradification. In other words, we just use:\n",
    "$$ V(s) = max_a \\sum_{a} \\pi(a | s) \\cdot \\sum_{s'} \\sum_{r} p(s', r | s, a) \\cdot [r + \\gamma \\cdot v_{\\pi}(s')]$$ \n",
    "* Avoiding full sweeps \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

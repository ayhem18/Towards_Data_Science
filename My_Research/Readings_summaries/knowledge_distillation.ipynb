{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "This notebook is mainly designed to save my notes on Knowledge distillation papers.\n",
    "I will use this [survey](https://arxiv.org/pdf/2006.05525.pdf) as my guide to the Knowledge distillation world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response-Based Knownledge\n",
    "This type of KD is based solely on the logits: outputs of the last fully connected layer of the teached model.\n",
    "It can generalized for most computer vision tasks: classification, segmentation, and object detection.\n",
    "\n",
    "## Paper: Vanilla Knowledge Distillation \n",
    "* [link](https://arxiv.org/pdf/1503.02531.pdf)\n",
    "* no code link in the paper\n",
    "\n",
    "* main ideas:\n",
    "    1. The main idea is a random variable with high entropy is quite informative. In other words, if we have an instance $x_i$, associated with probabilities $p_A = 10 ^ {-6}$, $p_B = 8 * 10 ^ {-5}$ and $p_C = 10 ^ {-2}$, if similar distributions are observed across a large number of instances we can say that $A$ and $B$ are quite similar. \n",
    "    2. Basically, soft labels incorporate much more knowledge than hard labels.\n",
    "    3. Cumbersome model (CM) and distilled Model (DM). Train the model on some large training data. leave a holdout dataset generally referred to as the ***transfer dataset***. Minimize entropy between the logits (probabilities are too low to affect the entropy loss) of CM and DM\n",
    "    4. Having some of transfer data labeled (or all of it) with CM's predictions. The loss function can be updated by adding a term where the DM should predict correctly.\n",
    "    5. The mathematical details are not shared, so I can't tell for sure how the process work.\n",
    "    6. The softmax function is defined as: $$p_i = \\frac{exp(\\frac{z_i}{T})}{\\sum_j exp(\\frac{z_j}{T})} $$\n",
    "    7. The value $T$ is referred to as the temperature. The larger the value of $T$, the softer, more uniform the distribution will get.\n",
    "    8. if DM is relatively large, high values of $T$ should be used. For relatively smaller DMs (less neuron units, less layers), use smaller $T$\n",
    "    9. $T$ is used in the softmax predictions of the DM model on the transfer dataset\n",
    "\n",
    "\n",
    "* funny enough (I am still not used to how research papers work), this [FitNet](https://arxiv.org/pdf/1412.6550.pdf) explains the vanilla Knowledge Distillation:\n",
    "    1. So when the labels are not known, the student is trained to minimize $$ L(P^{\\tau}_{T}, P^{\\tau}_{S})$$ where $P^{\\tau}$ is the output of the softmax with temperature $T$\n",
    "    2. when some of the labeled data is known, the loss can be modified as follows: $$ L(y_{true}, P^{\\tau}_{S}) + \\lambda \\cdot L(P^{\\tau}_{T}, P^{\\tau}_{S})$$\n",
    "\n",
    "\n",
    "The paper share a number of ideas concerning distilling ensemble of models, but it does not serve the topic of my research (at least I don't see how, yet).\n",
    "Reading Smooth labeling is a must.\n",
    "\n",
    "* IMPORTANT NOTE: the temperature $T$ is used only in the training phase. The usual softmax with temperature $1$ is used in inference. \n",
    "\n",
    "* There is another similar [paper](https://arxiv.org/pdf/1312.6184.pdf). It experimented mainly with $L_2$ loss as well as introduce a small matrix factorization trick to speed up the training process.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

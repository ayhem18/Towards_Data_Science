{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper1: Reverse Grad\n",
    "\n",
    "* [link](https://arxiv.org/pdf/1409.7495.pdf)\n",
    "* main idea:\n",
    "    1. Assuming the usual setting of ***UDA***, the authors introduce an additional label for each source training sample: $d \\in \\{0, 1\\}$: belongs to source or not\n",
    "    2. Assuming the Neural Network can be split into a feature extractor $G_{f}$ and a label predictor, the idea is to introduce an additional domain predictor (a feed forward block) predicting $d$.\n",
    "    3. The parameters of $G_f$, $\\theta_f$ are optimized to both\n",
    "        * minimize the loss of the label predictor (obtain discriminative features)\n",
    "        * maximize the loss of the domain predictor (learn domain invariant features)\n",
    "    4. Additionally, the parameters $\\theta_d$ are optimized to minimize the loss of the domain predictor\n",
    "\n",
    "* Main intuition:\n",
    "    * According to the [domain shift](https://sci-hub.ru/10.1016/s0378-3758(00)00115-4) assumption, minimizing the discrepancy  distributions: $G_f(x, \\theta_f| x ~ S(x))$, $G_f(x, \\theta_f| x ~ T(x))$ will lead to high performance in both domains (distributions)\n",
    "    * Estimating each of these distributions is non trivial because of high dimensionality, and changing values over the training process\n",
    "    * one way to estimate such discrepancy is the loss of the domain predictor / classifier assuming $\\theta_d$ were optimized to differentiate between them \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 2:\n",
    "* [link](https://arxiv.org/pdf/1412.3474.pdf)\n",
    "* no code\n",
    "* Belongs to the family of approaches that minimizes the difference between the target and source distributions: this paper utilizes the single-kernel version of the Maximum Mean discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 3: Learning Trasferable Features with Deep Adaptation Networks\n",
    "* [link](https://arxiv.org/pdf/1502.02791.pdf)\n",
    "* no code for this paper\n",
    "\n",
    "* main ideas:\n",
    "    1. The main idea is to use a Hilbert Space to estimate the difference between the features of the source and target data\n",
    "    2. using the findings of the paper above (How transferrable are deep features ?), freeze the layers known to produce general features and retrain (fine tune) the other ones\n",
    "    3. fine tune these layers by minimzing the following loss:\n",
    "        $$ \\frac{1}{N} \\sum_{i = 1} ^ {N} L_{CE}(\\theta(x_i), y_i) + \\lambda \\cdot \\sum_{l = l_s} ^ {l = l_e} d^2_k (D_s^l , D_t^l)$$\n",
    "    4. The second component is a sum from $l_s $ to $l_e$ which stands for $l_{start}$: start layer and $l_{end}$ end layer (the fine tuned layers). So the loss captures the dissimilarity between the features of all fine tuned layers\n",
    "    5. The parameters resulting from minimizing such loss function produce embeddings that are domain independent. \n",
    "    6. The paper further introduces algorithmic optimizations to efficiently carry out the necessary calculations\n",
    "\n",
    "* Same family uses: Multiple kernel version of the MMD measure.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

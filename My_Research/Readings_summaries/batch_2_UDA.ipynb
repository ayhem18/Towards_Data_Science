{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 8:\n",
    "* [link](https://arxiv.org/pdf/1409.7495.pdf)\n",
    "* main idea:\n",
    "    1. Assuming the usual setting of ***UDA***, the authors introduce an additional label for each source training sample: $d \\in \\{0, 1\\}$: belongs to source or not\n",
    "    2. Assuming the Neural Network can be split into a feature extractor $G_{f}$ and a label predictor, the idea is to introduce an additional domain predictor (a feed forward block) predicting $d$.\n",
    "    3. The parameters of $G_f$, $\\theta_f$ are optimized to both\n",
    "        * minimize the loss of the label predictor (obtain discriminative features)\n",
    "        * maximize the loss of the domain predictor (learn domain invariant features)\n",
    "    4. Additionally, the parameters $\\theta_d$ are optimized to minimize the loss of the domain predictor\n",
    "\n",
    "* Main intuition:\n",
    "    * According to the [domain shift](https://sci-hub.ru/10.1016/s0378-3758(00)00115-4) assumption, minimizing the discrepancy  distributions: $G_f(x, \\theta_f| x ~ S(x))$, $G_f(x, \\theta_f| x ~ T(x))$ will lead to high performance in both domains (distributions)\n",
    "    * Estimating each of these distributions is non trivial because of high dimensionality, and changing values over the training process\n",
    "    * one way to estimate such discrepancy is the loss of the domain predictor / classifier assuming $\\theta_d$ were optimized to differentiate between them \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 9: How transferrable are deep features\n",
    "* [link]()\n",
    "* main ideas / contributions:\n",
    "    1. the higher the layers, the lower the generality is: the number of layers frozen should be tuned as well\n",
    "    2. The transferability of features is affected not only by the features' specificality but also with codependency between neurons: In other words certain layers (mainly in the middle of the network) might learn fragile features that heavily depend on the previous layers. The transferrability is negatively impacted when only a subset of these co-dependent layers is transferred\n",
    "    3. Using weights pretrained on a different but similar (not sure of the importance of similarity in this particular conclusion) task can improve the performance.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 10: Learning Trasferable Features with Deep Adaptation Networks\n",
    "* [link] (https://arxiv.org/pdf/1502.02791.pdf)\n",
    "* no code for this paper\n",
    "\n",
    "* main ideas:\n",
    "    1. The main idea is to use a Hilbert Space to estimate the difference between the features of the source and target data\n",
    "    2. using the findings of the paper above (How transferrable are deep features ?), freeze the layers known to produce general features and retrain (fine tune) the other ones\n",
    "    3. fine tune these layers by minimzing the following loss:\n",
    "        $$ \\frac{1}{N} \\sum_{i = 1} ^ {N} L_{CE}(\\theta(x_i), y_i) + \\lambda \\cdot \\sum_{l = l_s} ^ {l = l_e} d^2_k (D_s^l , D_t^l)$$\n",
    "    4. The second component is a sum from $l_s $ to $l_e$ which stands for $l_{start}$: start layer and $l_{end}$ end layer (the fine tuned layers). So the loss captures the dissimilarity between the features of all fine tuned layers\n",
    "    5. The parameters resulting from minimizing such loss function produce embeddings that are domain independent. \n",
    "    6. The paper further introduces algorithmic optimizations to efficiently carry out the necessary calculations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 11: Adaptive Batch Normalization\n",
    "* [link](https://arxiv.org/pdf/1603.04779.pdf)\n",
    "* code: no code\n",
    "\n",
    "* The idea is pretty simple: The statistics of batch normalization layers saves domain knownledge (not label knowledge)\n",
    "* Assuming we have a pretrained DNN and a target domain: \n",
    "    1. for each neuron save the response $x_j$ for each image $m$.\n",
    "    2. calculate the associated mean and variance.\n",
    "    3. use these values in the inference time for DNN (instead of the variance and mean estimated from the source data)\n",
    "\n",
    "* very simple (yet powerful ?) approach. can be combined with other UDA technique (or extended for SFDA ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 12: Simlutaneous Deep Transfer Features across Domains\n",
    "* [link](https://arxiv.org/pdf/1510.02192.pdf)\n",
    "\n",
    "* main ideas:\n",
    "    1. the major part is combining 3 different losses, the usual classification loss, confusion loss, and custom soft label loss\n",
    "    2. here is the network's architecture:\n",
    "    ![distribution](../images/joint_cnn.png)\n",
    "    3. a domain classifier is used. The latter is optimized by minimizing the loss:\n",
    "        $$ L_D = -\\sum_{d} (y_D = d) \\cdot \\log(D(x_i))$$\n",
    "        where $D = softmax(x_i, \\theta_D, \\theta_{repr})$\n",
    "    4. on the other hand an additional loss referred to as confusion loss: $$ L_{conf} = -\\sum_d \\frac{1}{D} \\log(D(x))$$\n",
    "    5. Minimizing the first loss, given a feature extractor, means the domain classifier can differentiate between a source and target domain\n",
    "    6. Minimizing the 2nd loss, means the classifier is technically guessing the sample's dataset  randomly \n",
    "    7. As both losses are fixed, the optimization is performed iteratively: optimizing $L_D$ given $\\theta_{ref}$ from previous iteration and  then $L_conf$ with $L_D$\n",
    "    8. The confusion loss helps align both domains: building domain invariant features. Nevertheless, the papers points to an important point: Such alignment doesn't necessarily\n",
    "    mean the alignement of labels between target and source domains. (need to understand this more intuitevely)\n",
    "    9. Hence, the introduction of a 3rd loss. The authors introduce the soft label of a category $k$: the average of distributions of all examples of the source data belonging to $k$. This function can be only used in semi supervised learning where some of the target data has labeled samples (need to verify this)\n",
    "    10. $$L_{soft} = \\sum_i l^{y_T} \\cdot \\log p_i$$ \n",
    "     where $p_i = softmax(\\theta_{rep}, x_{ti}, \\tau)$ as $\\tau$ is the temperature of softmax.\n",
    "\n",
    "    11. The network is trained to optimize the combined loss: $L_c + \\alpha_1 \\cdot L_{conf} + \\alpha_2 \\cdot L_{soft}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 13: Deep Domain Confusion:\n",
    "* [link](https://arxiv.org/pdf/1412.3474.pdf)\n",
    "* no code\n",
    "* Literally just add a function that estimates the disperancy between the representation of target and source domains to the loss function\n",
    "* This family of approaches, simply changes the loss function that quantifies the domain disperancy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 14: Matching Distributions between model and Data: Cross Domain Distillation for UDA\n",
    "* [link](https://aclanthology.org/2021.acl-long.421.pdf)\n",
    "* This paper is one of the first papers to address source free UDA\n",
    "* The suggested setting here is quite similar to Knowledge distillation with the main difference of having the target data's distribution different from the souce data's distribution.\n",
    "* The training objective is as follows:\n",
    "    $$ L_{KD} + \\mu \\cdot L(S(x), T(x))$$\n",
    "where $L_{KD}$ is the standard (Vanilla) KD loss, and the 2nd loss is an estimation of the difference between source and target data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

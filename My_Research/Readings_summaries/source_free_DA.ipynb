{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook I save my notes on Source Free Domain adaptation. I am using this [survey](https://arxiv.org/pdf/2302.11803.pdf) as my guide in this journey.\n",
    "\n",
    "Source Free domain adaptation can be seen as an extension of the well known Unsupervised domain adaptation subfield. The latter relies on the availability of source data which might be considered an unrealistic expectation in practical settings for several reasons such as:\n",
    "\n",
    "1. Extremely large datasets that introduce saving and sharing issues \n",
    "2. Privacy concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Domain roughly speaking represents a dataset $D^s$ with a set of labels $L$ and the data is sampled from a distribution $P(X)$. The source domain is mathematically presented as \n",
    "$$D^s = \\{ \\{X^s, P(X^s), d^s\\}, L^s \\}$$\n",
    "It is available during pretraining.\n",
    "\n",
    "* Target Domain is referred to as a dataset with no labels with a distribution different from the source domain:\n",
    "                $$D^s = \\{ \\{X^t, P(X^t), d^t\\} \\}$$\n",
    "\n",
    "* The typical setting of SFUDA assumes having the same set of possible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Based Approaches\n",
    "This direction itself divides into 2 familities of approaches: \n",
    "1. Reconstruction based: building a representation of the source domain \n",
    "2. Focusing on the unannotated data: identifying clusters in the target domain \n",
    "\n",
    "DATA BASED APPROACHES ARE MORE CHALLENGING. THEY ARE LEFT FOR LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based Approaches:\n",
    "The second research direction is modifying part of the model's parameters. The main 3 sub-directions within this approach are:\n",
    "1. self-training\n",
    "2. entropy minimization\n",
    "3. contrasive learning\n",
    "\n",
    "## Pseudo-Labeling\n",
    "The most widely studied approach so far. Pseudo-labeling can be broken into 3 different parts:\n",
    "1. Prototype generation: Selecting the reliable samples\n",
    "2. Pseudo Labels assignment: assigning a label to each sample\n",
    "3. Pseudo labeling filtering: filter unreliable samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prototype Generation ideas\n",
    "### Domain Adaptation without Data\n",
    "* [link](https://arxiv.org/pdf/2007.01524.pdf)\n",
    "* [code]()\n",
    "\n",
    "* This paper provides references for the reasons why the source data might not be always available. such as the [following](https://www-file.huawei.com/-/media/corporate/pdf/trust-center/ai-security-whitepaper.pdf)\n",
    "Each paper's analysis will be broken down to the 3 steps mentioned above:\n",
    "|\n",
    "* Prototype generation: For each training sample $x_t$, the entropy is calculated: $$H(x_t) = \\frac{1}{\\log(N_c)} \\sum_{c=1}^{N_c} p(x_t|c) \\cdot \\log(p (x_t|c))$$ The reliable samples are chosen according to the following criteria. The lower the entropy, the more confident the model is about its prediction. The threshold is:\n",
    "$$ \\mu = max \\{ min(H_c) |c \\in C \\} $$ \n",
    "and for each class we know have multiple prototypes, the samples whose entropy is less than the threshold.\n",
    "\n",
    "* Pseudo labels assignment: The idea here is simple, calculate the average similarity between the features of the given sample and the prototypes of each class. The sample will be assigned the class with the largest similarity score.\n",
    "\n",
    "* Filtering labels: The process still uses class prototypes. For each sample, they consider the prototype of the labeled class and the prototypes of the 2nd most probable class. The most reliable samples are the ones such that $$max~d(x_t, M_{1p}) \\leq min~d(x_t, M_{2p})$$ In other words, the samples such as the distance to the most dissimilar prototype in $M_{1p}$ is less that the distance to the most similar prototype in $M_{2p}$\n",
    "\n",
    "* Training: training a classifier and a feature extractor using a linear combination of the source pseudo labels, and the filtered ones.\n",
    "\n",
    "Here is the ![pseudocode](../images/pseudo_labeling_training.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised Learning: Pseudo Labeling\n",
    "* [link](https://www.researchgate.net/profile/Dong-Hyun-Lee/publication/280581078_Pseudo-Label_The_Simple_and_Efficient_Semi-Supervised_Learning_Method_for_Deep_Neural_Networks/links/55bc4ada08ae092e9660b776/Pseudo-Label-The-Simple-and-Efficient-Semi-Supervised-Learning-Method-for-Deep-Neural-Networks.pdf)\n",
    "\n",
    "* This paper introduces the semi-supervised training setting to me.\n",
    "* This tiny papers trains a neural network using both labeled and unlabeled data by minimizing the following loss function:\n",
    "    $$L = \\frac{1}{n} \\sum_{m=1}^n \\sum_{i=1}^C L(y_i^m f_i^m) + \\alpha(t) \\cdot \\frac{1}{n^{'}} \\sum_{m=1}^{n^{'}} \\sum_{i=1}^C L(y^{'m}_i, f^{'m}_i)$$\n",
    "\n",
    "* This loss is somehow equivalent to cross entropy regularization, as the $y^{'m}_i$ is the most probable label at the given iteration.\n",
    "* choosing the proper scheduling $\\alpha(t)$ is crucial to take advantage of the unlabeled data without disrupting the training with the labeled one.\n",
    "\n",
    "* The paper above explains that increasing $\\alpha(t)$ mitigates the erros at the early training stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi Supervised Learning: Deep Cluster\n",
    "* [link](https://arxiv.org/pdf/1807.05520.pdf)\n",
    "* [explanation](https://www.youtube.com/watch?v=4MqFR_hHmUE)\n",
    "* This paper introduces clustering into the self-supervised learning picture.\n",
    "\n",
    "* The paper builds on top of a key observation: Training a head classifier on top of a randomly initialized convolutional neural network reaches an accuray of $12\\%$ which is huge relatively to the expected probability of a random classifier: $\\frac{1}{1000} = 0.1$ The latter is an emperical evidence of the extreme discriminative power of convolutional layers. \n",
    "\n",
    "* The general idea is quite simple: iterative training\n",
    "1. Given a feature extractor with fixed weights $\\theta$, we find the cluster centroids as well as assign a cluster (and thus a label) for each sample.\n",
    "2. Given these pseudo labels, we train the classification head and the entire network with backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do We Really Need to Access The Source Data?\n",
    "* [paper](https://arxiv.org/pdf/2002.08546.pdf)\n",
    "\n",
    "* Let's focus on the thought process. \n",
    "1. Assuming the target feature extractor is similar to that of the source feature extractor. Then, having a source domain classifier would work well enough.\n",
    "2. Since we do not have the source distribution, aligning the source data with the target data distributions isn't possible. \n",
    "3. Nevertheless, the authors tackle the problem from a different perspective: What would the target feature extractor look like if the domain shift is mitigated ?\n",
    "4. The authors' answer: high confidence and diversity. Thus, the use of Maximum information loss\n",
    "5. The next step is to use Pseudo Labeling: Building clusters (somehow) and then using them as pseudo labels in the usual loss function with pseudo-labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploiting the intrinsic Neighbordhood Structure for SFDA\n",
    "* [paper](https://arxiv.org/pdf/2110.04202.pdf)\n",
    "\n",
    "* This paper puts forth an important point: The domain shift slightly disturbs the cluster structure in the embedding space. In other words, the parts with the highest density in most clusters are preserved. So the method's main goal is to assign good labels (pseudo-labels) for samples lying in the ambigous clusters\n",
    "\n",
    "* It is important to note that this hypothesis is based on emperical and experimental results without theoretically-proven results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProxyMix: Proxy-Based Mixup training with label refinery \n",
    "* [link](https://arxiv.org/pdf/2205.14566.pdf)\n",
    "* [code]()\n",
    "\n",
    "* The authors explain that generative approachs generally introduce additional parameters. The generative networs end up suffering from the mode collapse problem.\n",
    "* one interesting idea is to consider the weights used just before the output layer as centroids for the given categories.\n",
    "* The other idea is to define a hyperparameter $N$ as to consider the $N$ closest points from the centroids of each class.\n",
    "\n",
    "* AMONG THE IMPORTANT ISSUES ASSOCIATED WITH PSEUDO-LABELING IN a SFUDA setting, is the class imbalance in prototypes and the model's bias towards easier classes.\n",
    "* Their main strategy isn't completely clear. so I will be back to it later. NEvertheless, it is based on so called classical works: \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

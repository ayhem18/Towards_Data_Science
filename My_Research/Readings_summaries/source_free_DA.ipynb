{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook I save my notes on Source Free Domain adaptation. I am using this [survey](https://arxiv.org/pdf/2302.11803.pdf) as my guide in this journey.\n",
    "\n",
    "Source Free domain adaptation can be seen as an extension of the well known Unsupervised domain adaptation subfield. The latter relies on the availability of source data which might be considered an unrealistic expectation in practical settings for several reasons such as:\n",
    "\n",
    "1. Extremely large datasets that introduce saving and sharing issues \n",
    "2. Privacy concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Domain roughly speaking represents a dataset $D^s$ with a set of labels $L$ and the data is sampled from a distribution $P(X)$. The source domain is mathematically presented as \n",
    "$$D^s = \\{ \\{X^s, P(X^s), d^s\\}, L^s \\}$$\n",
    "It is available during pretraining.\n",
    "\n",
    "* Target Domain is referred to as a dataset with no labels with a distribution different from the source domain:\n",
    "                $$D^s = \\{ \\{X^t, P(X^t), d^t\\} \\}$$\n",
    "\n",
    "* The typical setting of SFUDA assumes having the same set of possible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Based Approaches\n",
    "This direction itself divides into 2 familities of approaches: \n",
    "1. Reconstruction based: building a representation of the source domain \n",
    "2. Focusing on the unannotated data: identifying clusters in the target domain \n",
    "\n",
    "DATA BASED APPROACHES ARE MORE CHALLENGING. THEY ARE LEFT FOR LATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based Approaches:\n",
    "The second research direction is modifying part of the model's parameters. The main 3 sub-directions within this approach are:\n",
    "1. self-training\n",
    "2. entropy minimization\n",
    "3. contrasive learning\n",
    "\n",
    "## Pseudo-Labeling\n",
    "The most widely studying approach so far. Pseudo-labeling can be broken into 3 different parts:\n",
    "1. Prototype generation: Selecting the reliable samples\n",
    "2. Pseudo Labels assignment: assigning a label to each sample\n",
    "3. Pseudo labeling filtering: filter unreliable samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prototype Generation ideas\n",
    "### Domain Adaptation without DAta\n",
    "* [link](https://arxiv.org/pdf/2007.01524.pdf)\n",
    "* [code]()\n",
    "\n",
    "* This paper provides references for the reasons why the source data might not be always available. such as the [following](https://www-file.huawei.com/-/media/corporate/pdf/trust-center/ai-security-whitepaper.pdf)\n",
    "Each paper's analysis will be broken down to the 3 steps mentioned above:\n",
    "|\n",
    "* Prototype generation: For each training sample $x_t$, the entropy is calculated: $$H(x_t) = \\frac{1}{\\log(N_c)} \\sum_{c=1}^{N_c} p(x_tc|) \\cdot \\log(p (x_t|c))$$ The reliable samples are chosen according to the following criteria. The lower the entropy, the more confident the model is about its prediction. The threshold is:\n",
    "$$ \\mu = max \\{ min(H_c) |c \\in C \\} $$ \n",
    "and for each class we know have multiple prototypes, the samples whose entropy is less than the threshold.\n",
    "\n",
    "* Pseudo labels assignment: The idea here is simple, calculate the average similarity between the features of the given sample and the prototypes of each class. The sample will be assigned the class with the largest similarity score.\n",
    "\n",
    "* Filtering labels: The process still uses class prototypes. For each sample, they consider the prototype of the labeled class and the prototypes of the 2nd most probable class. The most reliable samples are the ones such that $$max~d(x_t, M_{1p}) \\leq min~d(x_t, M_{2p})$$ In other words, the samples such as the distance to the most dissimilar prototype in $M_{1p}$ is less that the distance to the most similar prototype in $M_{2p}$\n",
    "\n",
    "* Training: training a classifier and a feature extractor using a linear combination of the source pseudo labels, and the filtered ones.\n",
    "\n",
    "Here is the ![pseudocode](../images/pseudo_labeling_training.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook is to save my notes while learning the hugging face library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with The Pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9989687204360962}, {'label': 'POSITIVE', 'score': 0.9997546076774597}, {'label': 'POSITIVE', 'score': 0.9932047128677368}, {'label': 'NEGATIVE', 'score': 0.9978812336921692}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# sentiment analysis it is\n",
    "sent_pipe = pipeline('sentiment-analysis')\n",
    "\n",
    "sents = ['Life does not seem to entail any meaning, my friend', \n",
    "         \"Well, It is what it is\", \n",
    "         \"Man, I have never been this happy\", \n",
    "         \"Well, things could probably be better.\"]\n",
    "\n",
    "res = sent_pipe(sents)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I am so tired. I want to do more as soon as possible. Please forgive your pain and I can never live again.\"'}, {'generated_text': \"I am so tired. I want to go out to work. We've all been together and have been talking since the very beginning but we are doing the same things.\\n\\n\\nIf you've been coming at me for so long, you've given my energy.\\nI don't feel my heart can pick up on what I was doing.\\nIt takes less. I just want to go out and be awesome.\\nI believe in your power, but no sooner can you make it to\"}, {'generated_text': \"I am so tired. I want to feel the feeling of having it all over again. It was nice when my mom asked me to join me, she would tell me to stay in my home. But she told me to let me spend my days with him. Not by any means. I couldn't even keep my mind shut. I had to watch everything. It was hard.\\nI was nervous after what she said and the other people on the show. I had to watch the whole episode\"}]\n"
     ]
    }
   ],
   "source": [
    "# well as we can see the pipeline class is not for final usage.\n",
    "# It is mainly for demonstrative usage\n",
    "\n",
    "# let's try generation\n",
    "\n",
    "gen = pipeline('text-generation', max_length=100, num_return_sequences=3, model='distilgpt2')\n",
    "\n",
    "answer = gen('I am so tired. I want to')\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "Well must check the following:\n",
    "1. [Understand Bert](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "2. [Decoder Models For text generation](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer API\n",
    "The first preprocessing step is tokenizing the input as well as adding any special tokens. To carry out the last step, one needs to better understand the model in question. The information is generally available on the model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start using some models\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2092, 1010, 1045, 2031, 2488, 2477, 2000, 2079, 2005, 2085, 1012,\n",
      "         2156, 2017, 2101,  102],\n",
      "        [ 101, 2079, 2017, 2428, 2228, 1045, 3685, 2663, 2023, 2674, 1029,  102,\n",
      "            0,    0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(['Well, I have better things to do for now. See you later', \"Do you really think I cannot win this match ?\"], \n",
    "                        padding=True,\n",
    "                        truncation=True, \n",
    "                        return_tensors='pt')\n",
    "\n",
    "print(model_inputs['input_ids'])\n",
    "print(model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# now time to use a model\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# let's do some inference\n",
    "inference = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model's output's shape is generally: \n",
    "# 1. The batch size\n",
    "# 2. The sequence's length\n",
    "# 3. The hidden space dimensionality\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "preds = torch.argmax(outputs.logits.detach(), dim=-1).tolist()\n",
    "# extract the map from indices to classe\n",
    "itos = model.config.id2label\n",
    "\n",
    "print([itos[p] for p in preds])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "config= BertConfig()\n",
    "model = BertModel(config)\n",
    "print(config)\n",
    "# using the config file will simply create a BertModel from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\t[101, 2503, 19689, 11303, 1468, 1110, 11519, 1103, 1148, 2585, 1107, 1126, 21239, 2101, 1933, 102]\n",
      "token_type_ids\t[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask\t[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# there a couple of cool tokenizers that we can use\n",
    "from transformers import BertTokenizer\n",
    "BERT_CHECKPOINT = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "\n",
    "sentence = 'fine tuning transformers is basically the first step in an NLP project'\n",
    "\n",
    "tokens_data = bert_tokenizer(sentence)\n",
    "\n",
    "# the forward call will execute the entire tokenization pipeline \n",
    "for k ,v in tokens_data.items():\n",
    "    print(k, v, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine tuning transformers is basically the first step in an NLP project\n"
     ]
    }
   ],
   "source": [
    "# let's break down a bit\n",
    "tokens = bert_tokenizer.tokenize(sentence)\n",
    "# the 2nd step is to convert each of these tokens to their numerical ids\n",
    "ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "# we can actually convert the numerical ids back to text as follows\n",
    "string = bert_tokenizer.decode(ids)\n",
    "print(bert_tokenizer.decode(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional input \n",
    "* It is important to keep in mind that most models expected batched input.  \n",
    "* The default output of the tokenize function is a list. Make sure to use tensor as a return type to account for batching\n",
    "* With great power comes great responsibility. Each element in the batch must have the exact same shape. Here comes padding and the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "bm = AutoModelForSequenceClassification.from_pretrained(BERT_CHECKPOINT)\n",
    "\n",
    "# first let's start with extracting some interesting values\n",
    "\n",
    "pad_id = tokenizer.pad_token_id\n",
    "start_id = tokenizer.sep_token_id \n",
    "cls_id = tokenizer.cls_token_id\n",
    "\n",
    "def prepare_input(sentences: list[str], tokenizer):\n",
    "    # extract the lenght of the longuest sentence\n",
    "    tokens = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(s)) for s in sentences]\n",
    "    max_length = len(max(tokens, key=len))\n",
    "\n",
    "    def pad(tokens: str, num_pads):\n",
    "        res = [tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id] + [tokenizer.pad_token_id] * num_pads\n",
    "        return res\n",
    "    \n",
    "    def attention_mask(padded_ids: list[int]):\n",
    "        return [int(t != tokenizer.pad_token_id) for t in padded_ids]\n",
    "    pads = [pad(s, max_length - len(s)) for s in tokens]\n",
    "\n",
    "    return {\"input_ids\": torch.tensor(pads) , \"attention_mask\": torch.tensor([attention_mask(s) for s in pads])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101,  146, 1821,  170, 1363, 1825,  102,    0],\n",
       "         [ 101,  146, 1567, 5464, 5679, 1114, 6831,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = ['I am a good person', 'I love drinking tea with milk']\n",
    "i1, m1 = (prepare_input(sens, tokenizer).values())\n",
    "i1, m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101,  146, 1821,  170, 1363, 1825,  102,    0],\n",
       "         [ 101,  146, 1567, 5464, 5679, 1114, 6831,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o =  tokenizer(sens, padding=True, return_tensors='pt')\n",
    "o['input_ids'], o['attention_mask']\n",
    "# well nice we got the hang of the process apparently !!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization API: More Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method __call__ in module transformers.tokenization_utils_base:\n",
      "\n",
      "__call__(text: Union[str, List[str], List[List[str]]] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]]] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.bert.tokenization_bert_fast.BertTokenizerFast instance\n",
      "    Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      "    sequences.\n",
      "    \n",
      "    Args:\n",
      "        text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "        text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "        text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "        text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    \n",
      "        add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to encode the sequences with the special tokens relative to their model.\n",
      "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "            Activates and controls padding. Accepts the following values:\n",
      "    \n",
      "            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "              sequence if provided).\n",
      "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "              acceptable input length for the model if that argument is not provided.\n",
      "            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "              lengths).\n",
      "        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "            Activates and controls truncation. Accepts the following values:\n",
      "    \n",
      "            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "              to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "              sequences (or a batch of pairs) is provided.\n",
      "            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "              greater than the model maximum admissible input size).\n",
      "        max_length (`int`, *optional*):\n",
      "            Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "    \n",
      "            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "        stride (`int`, *optional*, defaults to 0):\n",
      "            If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "            returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "            argument defines the number of overlapping tokens.\n",
      "        is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "            which it will tokenize. This is useful for NER or token classification.\n",
      "        pad_to_multiple_of (`int`, *optional*):\n",
      "            If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "            `>= 7.5` (Volta).\n",
      "        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "            If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "    \n",
      "            - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "            - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "    \n",
      "        return_token_type_ids (`bool`, *optional*):\n",
      "            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "            the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "    \n",
      "            [What are token type IDs?](../glossary#token-type-ids)\n",
      "        return_attention_mask (`bool`, *optional*):\n",
      "            Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "    \n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      "            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      "            of returning overflowing tokens.\n",
      "        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return special tokens mask information.\n",
      "        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return `(char_start, char_end)` for each token.\n",
      "    \n",
      "            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      "            Python's tokenizer, this method will raise `NotImplementedError`.\n",
      "        return_length  (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return the lengths of the encoded inputs.\n",
      "        verbose (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to print more information and warnings.\n",
      "        **kwargs: passed to the `self.tokenize()` method\n",
      "    \n",
      "    Return:\n",
      "        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      "    \n",
      "        - **input_ids** -- List of token ids to be fed to a model.\n",
      "    \n",
      "          [What are input IDs?](../glossary#input-ids)\n",
      "    \n",
      "        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      "          if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      "    \n",
      "          [What are token type IDs?](../glossary#token-type-ids)\n",
      "    \n",
      "        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      "    \n",
      "          [What are attention masks?](../glossary#attention-mask)\n",
      "    \n",
      "        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      "          `return_overflowing_tokens=True`).\n",
      "        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      "          `return_overflowing_tokens=True`).\n",
      "        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      "        - **length** -- The length of the inputs (when `return_length=True`)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's import a tokenizer really quick\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "\n",
    "# there are several arguments in the api call that might be of use\n",
    "# 1. padding: whether and how to bad\n",
    "help(tokenizer.__call__)\n",
    "# we can see here that padding is either True of False. The default strategy is padding according to the longest sentence in the batch\n",
    "# but we can use max_length as the maximum length of a sequence input\n",
    "\n",
    "# 1.padding\n",
    "# 2.max_length\n",
    "# 3.truncation: whether the longer sequences (those exceeding max_length value) should be split into different parts\n",
    "# 4.return_tensor ['np', 'tf', 'pt'], the default value will make the call return a list \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Fine Tune Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/bouab/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a06b18f533456ba0236c17aac39ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n"
     ]
    }
   ],
   "source": [
    "# surprisingly, the hugging face HUB doesn't only contain models but datasets as well\n",
    "# let's start with a minimalistic dataset for experimenting\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset('glue', 'mrpc')\n",
    "train_ds, val_ds, test_ds = ds['train'], ds['validation'], ds['test']\n",
    "for d in train_ds:\n",
    "    print(d['sentence1'])\n",
    "    print(d['sentence2'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\t[101, 1124, 1110, 170, 1250, 3354, 14987, 102, 1124, 1110, 1684, 1185, 1831, 102]\n",
      "token_type_ids\t[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "attention_mask\t[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# this dataset is for the task of paraphrasing: comparing if 2 textual sequences are equivalent\n",
    "# the input is expected to be 2 phrases\n",
    "# let's start with a tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "BERT_CHECKPOINT = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "pair = tokenizer('He is a workaholic', 'He is working no stop')\n",
    "for k, v in pair.items():\n",
    "    print(k, v, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'He',\n",
       " 'is',\n",
       " 'a',\n",
       " 'work',\n",
       " '##ah',\n",
       " '##olic',\n",
       " '[SEP]',\n",
       " 'He',\n",
       " 'is',\n",
       " 'working',\n",
       " 'no',\n",
       " 'stop',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we haven't consider the token_type_ids before\n",
    "# the latter determines which tokens belongs to which sentence\n",
    "tokenizer.convert_ids_to_tokens(pair['input_ids'])\n",
    "# so as we can see: the tokenizer return [CLS] sentence 1 [SEP] sentence2 [SEP], as well as token_type_ids to let the model know the boarders of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\bouab\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-44839de31f1bb325.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5415bbd0e7054324b34d499e36b54a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\bouab\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-175c3b5777158902.arrow\n"
     ]
    }
   ],
   "source": [
    "# working with the Datasets library is needed for efficiency:\n",
    "# a single example of the Dataset object cannot be feeded directly to a model and thus some preprocessing is needed\n",
    "# it is important to define such steps in a function\n",
    "\n",
    "def preprocess(example):\n",
    "    # all we need this time is to tokeniz\n",
    "    return tokenizer(example['sentence1'], example['sentence2'], truncation=True, padding=True) \n",
    "\n",
    "# then we use the map function, as well as the batched=True argument for batched preprocessing\n",
    "tokenized_ds = ds.map(preprocess, batched=True)\n",
    "# the map function will keep the original features in the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1\tThe Nasdaq composite index increased 10.73 , or 0.7 percent , to 1,514.77 .\n",
      "sentence2\tThe Nasdaq Composite index , full of technology stocks , was lately up around 18 points .\n",
      "label\t0\n",
      "idx\t12\n",
      "input_ids\t[101, 1109, 11896, 1116, 1810, 4426, 14752, 7448, 2569, 1275, 119, 5766, 117, 1137, 121, 119, 128, 3029, 117, 1106, 122, 117, 4062, 1527, 119, 5581, 119, 102, 1109, 11896, 1116, 1810, 4426, 3291, 24729, 13068, 7448, 117, 1554, 1104, 2815, 17901, 117, 1108, 10634, 1146, 1213, 1407, 1827, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "token_type_ids\t[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask\t[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# let's check some of the individual examples\n",
    "ex1 = tokenized_ds['train'][11]\n",
    "for k, v in ex1.items():\n",
    "    print(k, v, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "# this function will convert \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "samples = [train_ds[i]['sentence1'] for i in range(10)]\n",
    "s = data_collator({'input_ids': tokenizer(samples)['input_ids']})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/bouab/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739cc64cd9f449d5bee1383723ed3b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\bouab\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-393aa7e5374d03aa.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa6d55b9c284888b6f1859e18625cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\bouab\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-590f93adf0413883.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0, 'input_ids': [101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 11336, 6732, 3384, 1106, 1140, 1112, 1178, 107, 1103, 7737, 107, 117, 7277, 2180, 5303, 4806, 1117, 1711, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# to save all the preprocessing steps in one cell:\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "# 1st import the data\n",
    "data = load_dataset('glue', 'mrpc')\n",
    "# we can easily extract the different components as follows\n",
    "data_train, data_val, data_test = data['train'], data['validation'], data['test']\n",
    "\n",
    "# to proceed one needs to understand the nature of the tast and the data itself\n",
    "# each two sentences should be tokenized together\n",
    "# let's first create the tokenizer\n",
    "paraphrase_tokenizer = AutoTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "\n",
    "# we define a function to process a given input, but first let's understand one sample of the dataset\n",
    "train_sample = data_train[0]\n",
    "print(train_sample)\n",
    "\n",
    "def process_paraphrase_sentence(train_sample, tokenizer):\n",
    "    return tokenizer(train_sample['sentence1'], train_sample['sentence2'], truncation=True, padding=True)   # the default will be the longest sequence in the batch\n",
    "\n",
    "# let's apply this function to each batch usig the map function \n",
    "# but keep in mind that the function passed to the map function should have no argument\n",
    "# partial is here for the rescue: \n",
    "process_function = partial(process_paraphrase_sentence, tokenizer=paraphrase_tokenizer)\n",
    "tokenized_data = data.map(process_function, batched=True)\n",
    "\n",
    "# we need an collator to make use of dynamic padding (I still don't get this part that much but that should be good enough for now)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=paraphrase_tokenizer)\n",
    "# let'see our tokenized data\n",
    "tokenized_train_sample = tokenized_data['train'][0]\n",
    "print(tokenized_train_sample)\n",
    "# we can see the inputs_ids and the masks fields were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/bouab/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dafe53627c42dabf1d77b85ce623ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\bouab\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-bb21e6423b980722.arrow\n",
      "Loading cached processed dataset at C:\\Users\\bouab\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-d1e8c90b5d349f7a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4057c289bd1a47e280d9fa4cd08013fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9681f9d80b3f4fc7aeca46236c37dcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\bouab\\DEV\\ds_env_new\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab2cb51f3724c1fb47d6ebf53cebb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.518, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3407, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n",
      "{'train_runtime': 225.2463, 'train_samples_per_second': 48.853, 'train_steps_per_second': 6.113, 'train_loss': 0.36598954100321407, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.36598954100321407, metrics={'train_runtime': 225.2463, 'train_samples_per_second': 48.853, 'train_steps_per_second': 6.113, 'train_loss': 0.36598954100321407, 'epoch': 3.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# this means that the model will discard the output layer used in pretraining and add a Fully Conncted layer with 2 labels as specified.\n",
    "# the last layer will be randomly initialized\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# let's the model train baby\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e345eafc24d1441e99af2a916bd16490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_ds['validation'])\n",
    "predictions, labels = predictions.predictions, predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current values in the predictions are just logits\n",
    "# we need argmax function  (well layer) to convert them to predicitons\n",
    "import evaluate\n",
    "import numpy as np\n",
    "preds = np.argmax(predictions, axis=-1)\n",
    "# we extract the metrics used in the dataset\n",
    "metric = evaluate.load('glue', 'mrpc')\n",
    "metric.compute(preditions=preds, refernces=labels) # for some reason, this statement gives an error\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(1),\n",
       " 'input_ids': tensor([  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
       "          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
       "          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
       "          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
       "          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before proceeding with using Pytorch directly, we need to prepare the data for training\n",
    "token_ds = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx']).rename_column('label', 'labels')\n",
    "token_ds.set_format('torch')\n",
    "token_ds['train'][0] # converting the list to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(token_ds['train'], \n",
    "                          shuffle=True, \n",
    "                          batch_size=8, \n",
    "                          collate_fn=data_collator)\n",
    "\n",
    "val_loader = DataLoader(token_ds['validation'], \n",
    "                         shuffle=False, \n",
    "                         batch_size=8, \n",
    "                         collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\ttorch.Size([8])\n",
      "input_ids\ttorch.Size([8, 68])\n",
      "token_type_ids\ttorch.Size([8, 68])\n",
      "attention_mask\ttorch.Size([8, 68])\n"
     ]
    }
   ],
   "source": [
    "for k, v in batch.items():\n",
    "    print(k, v.shape, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

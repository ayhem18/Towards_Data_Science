{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook is to save my notes while learning the hugging face library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with The Pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9989687204360962}, {'label': 'POSITIVE', 'score': 0.9997546076774597}, {'label': 'POSITIVE', 'score': 0.9932047128677368}, {'label': 'NEGATIVE', 'score': 0.9978812336921692}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# sentiment analysis it is\n",
    "sent_pipe = pipeline('sentiment-analysis')\n",
    "\n",
    "sents = ['Life does not seem to entail any meaning, my friend', \n",
    "         \"Well, It is what it is\", \n",
    "         \"Man, I have never been this happy\", \n",
    "         \"Well, things could probably be better.\"]\n",
    "\n",
    "res = sent_pipe(sents)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I am so tired. I want to say I could Step It up in the game and give it time I'm in like half-sleep, and it definitely is one of the reasons I decided to spend half my off-season at the start of next year. I don't know how it would help with any of the other things, so if I made my decisions so this would be something you'll appreciate as a bonus for me to go down and play against the best teams there has ever been\"}, {'generated_text': \"I am so tired. I want to die. For years of my life, I thought he couldn't find a place in my life. At age 31, I couldn't find anyone else with the resources to get me back in the U.S... and just go home happy. I also couldn't find someone to give me a phone call...\\nWhy stop trying to get me back in the U.S.?\\nI've been missing since July 2015. It's too good an\"}, {'generated_text': \"I am so tired. I want to live alone. Please let me know. I hate all of my life. I hate all I've heard about so far. I hate making a decision.\\n\\n\\nFollow me on Twitter and Facebook\\nThis post is for the most part in the news with updates in the comments.\\n-\\nMy new boyfriend and I got together in a house in the middle of the night and we stayed at the living room.\\nWe got off and we were both\"}]\n"
     ]
    }
   ],
   "source": [
    "# well as we can see the pipeline class is not for final usage.\n",
    "# It is mainly for demonstrative usage\n",
    "\n",
    "# let's try generation\n",
    "\n",
    "gen = pipeline('text-generation', max_length=100, num_return_sequences=3, model='distilgpt2')\n",
    "\n",
    "answer = gen('I am so tired. I want to')\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "Well must check the following:\n",
    "1. [Understand Bert](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "2. [Decoder Models For text generation](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer API\n",
    "The first preprocessing step is tokenizing the input as well as adding any special tokens. To carry out the last step, one needs to better understand the model in question. The information is generally available on the model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start using some models\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2092, 1010, 1045, 2031, 2488, 2477, 2000, 2079, 2005, 2085, 1012,\n",
      "         2156, 2017, 2101,  102],\n",
      "        [ 101, 2079, 2017, 2428, 2228, 1045, 3685, 2663, 2023, 2674, 1029,  102,\n",
      "            0,    0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(['Well, I have better things to do for now. See you later', \"Do you really think I cannot win this match ?\"], \n",
    "                        padding=True,\n",
    "                        truncation=True, \n",
    "                        return_tensors='pt')\n",
    "\n",
    "print(model_inputs['input_ids'])\n",
    "print(model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# now time to use a model\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# let's do some inference\n",
    "inference = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model's output's shape is generally: \n",
    "# 1. The batch size\n",
    "# 2. The sequence's length\n",
    "# 3. The hidden space dimensionality\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "preds = torch.argmax(outputs.logits.detach(), dim=-1).tolist()\n",
    "# extract the map from indices to classe\n",
    "itos = model.config.id2label\n",
    "\n",
    "print([itos[p] for p in preds])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "config= BertConfig()\n",
    "model = BertModel(config)\n",
    "print(config)\n",
    "# using the config file will simply create a BertModel from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\t[101, 2503, 19689, 11303, 1468, 1110, 11519, 1103, 1148, 2585, 1107, 1126, 21239, 2101, 1933, 102]\n",
      "token_type_ids\t[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask\t[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# there a couple of cool tokenizers that we can use\n",
    "from transformers import BertTokenizer\n",
    "BERT_CHECKPOINT = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "\n",
    "sentence = 'fine tuning transformers is basically the first step in an NLP project'\n",
    "\n",
    "tokens_data = bert_tokenizer(sentence)\n",
    "\n",
    "# the forward call will execute the entire tokenization pipeline \n",
    "for k ,v in tokens_data.items():\n",
    "    print(k, v, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine tuning transformers is basically the first step in an NLP project\n"
     ]
    }
   ],
   "source": [
    "# let's break down a bit\n",
    "tokens = bert_tokenizer.tokenize(sentence)\n",
    "# the 2nd step is to convert each of these tokens to their numerical ids\n",
    "ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "# we can actually convert the numerical ids back to text as follows\n",
    "string = bert_tokenizer.decode(ids)\n",
    "print(bert_tokenizer.decode(ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional input \n",
    "* It is important to keep in mind that most models expected batched input.  \n",
    "* The default output of the tokenize function is a list. Make sure to use tensor as a return type to account for batching\n",
    "* With great power comes great responsibility. Each element in the batch must have the exact same shape. Here comes padding and the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "bm = AutoModelForSequenceClassification.from_pretrained(BERT_CHECKPOINT)\n",
    "\n",
    "# first let's start with extracting some interesting values\n",
    "\n",
    "pad_id = tokenizer.pad_token_id\n",
    "start_id = tokenizer.sep_token_id \n",
    "cls_id = tokenizer.cls_token_id\n",
    "\n",
    "def prepare_input(sentences: list[str], tokenizer):\n",
    "    # extract the lenght of the longuest sentence\n",
    "    tokens = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(s)) for s in sentences]\n",
    "    max_length = len(max(tokens, key=len))\n",
    "\n",
    "    def pad(tokens: str, num_pads):\n",
    "        res = [tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id] + [tokenizer.pad_token_id] * num_pads\n",
    "        return res\n",
    "    \n",
    "    def attention_mask(padded_ids: list[int]):\n",
    "        return [int(t != tokenizer.pad_token_id) for t in padded_ids]\n",
    "    pads = [pad(s, max_length - len(s)) for s in tokens]\n",
    "\n",
    "    return {\"input_ids\": torch.tensor(pads) , \"attention_mask\": torch.tensor([attention_mask(s) for s in pads])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101,  146, 1821,  170, 1363, 1825,  102,    0],\n",
       "         [ 101,  146, 1567, 5464, 5679, 1114, 6831,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = ['I am a good person', 'I love drinking tea with milk']\n",
    "i1, m1 = (prepare_input(sens, tokenizer).values())\n",
    "i1, m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101,  146, 1821,  170, 1363, 1825,  102,    0],\n",
       "         [ 101,  146, 1567, 5464, 5679, 1114, 6831,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o =  tokenizer(sens, padding=True, return_tensors='pt')\n",
    "o['input_ids'], o['attention_mask']\n",
    "# well nice we got the hang of the process apparently !!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization API: More Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import a tokenizer really quick\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "\n",
    "# there are several arguments in the api call that might be of use\n",
    "# 1. padding: whether and how to bad\n",
    "help(tokenizer.__call__)\n",
    "# we can see here that padding is either True of False. The default strategy is padding according to the longest sentence in the batch\n",
    "# but we can use max_length as the maximum length of a sequence input\n",
    "\n",
    "# 1.padding\n",
    "# 2.max_length\n",
    "# 3.truncation: whether the longer sequences (those exceeding max_length value) should be split into different parts\n",
    "# 4.return_tensor ['np', 'tf', 'pt'], the default value will make the call return a list \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Fine Tune Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\DEV\\ds_env_new\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (C:/Users/bouab/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 299.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# surprisingly, the hugging face HUB doesn't only contain models but datasets as well\n",
    "# let's start with a minimalistic dataset for experimenting\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset('glue', 'mrpc')\n",
    "train_ds, val_ds, test_ds = ds['train'], ds['validation'], ds['test']\n",
    "for d in train_ds:\n",
    "    print(d['sentence1'])\n",
    "    print(d['sentence2'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\t[101, 1124, 1110, 170, 1250, 3354, 14987, 102, 1124, 1110, 1684, 1185, 1831, 102]\n",
      "token_type_ids\t[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "attention_mask\t[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# this dataset is for the task of paraphrasing: comparing if 2 textual sequences are equivalent\n",
    "# the input is expected to be 2 phrases\n",
    "# let's start with a tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "BERT_CHECKPOINT = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_CHECKPOINT)\n",
    "pair = tokenizer('He is a workaholic', 'He is working no stop')\n",
    "for k, v in pair.items():\n",
    "    print(k, v, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'He',\n",
       " 'is',\n",
       " 'a',\n",
       " 'work',\n",
       " '##ah',\n",
       " '##olic',\n",
       " '[SEP]',\n",
       " 'He',\n",
       " 'is',\n",
       " 'working',\n",
       " 'no',\n",
       " 'stop',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we haven't consider the token_type_ids before\n",
    "# the latter determines which tokens belongs to which sentence\n",
    "tokenizer.convert_ids_to_tokens(pair['input_ids'])\n",
    "# so as we can see: the tokenizer return [CLS] sentence 1 [SEP] sentence2 [SEP], as well as token_type_ids to let the model know the boarders of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\bouab\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-5b59d8dacba58b79.arrow\n",
      "Loading cached processed dataset at C:\\Users\\bouab\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-175c3b5777158902.arrow\n"
     ]
    }
   ],
   "source": [
    "# working with the Datasets library is needed for efficiency:\n",
    "# a single example of the Dataset object cannot be feeded directly to a model and thus some preprocessing is needed\n",
    "# it is important to define such steps in a function\n",
    "\n",
    "def preprocess(example):\n",
    "    # all we need this time is to tokeniz\n",
    "    return tokenizer(example['sentence1'], example['sentence2'], truncation=True, padding=True) \n",
    "\n",
    "# then we use the map function, as well as the batched=True argument for batched preprocessing\n",
    "tokenized_ds = ds.map(preprocess, batched=True)\n",
    "# the map function will keep the original features in the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1\tThe Nasdaq composite index increased 10.73 , or 0.7 percent , to 1,514.77 .\n",
      "sentence2\tThe Nasdaq Composite index , full of technology stocks , was lately up around 18 points .\n",
      "label\t0\n",
      "idx\t12\n",
      "input_ids\t[101, 1109, 11896, 1116, 1810, 4426, 14752, 7448, 2569, 1275, 119, 5766, 117, 1137, 121, 119, 128, 3029, 117, 1106, 122, 117, 4062, 1527, 119, 5581, 119, 102, 1109, 11896, 1116, 1810, 4426, 3291, 24729, 13068, 7448, 117, 1554, 1104, 2815, 17901, 117, 1108, 10634, 1146, 1213, 1407, 1827, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "token_type_ids\t[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask\t[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# let's check some of the individual examples\n",
    "ex1 = tokenized_ds['train'][11]\n",
    "for k, v in ex1.items():\n",
    "    print(k, v, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "# this function will convert \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "samples = [train_ds[i]['sentence1'] for i in range(10)]\n",
    "s = data_collator({'input_ids': tokenizer(samples)['input_ids']})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  2%|▏         | 25/1377 [03:21<3:01:20,  8.05s/it]\n",
      "c:\\Users\\bouab\\DEV\\ds_env_new\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 36%|███▋      | 500/1377 [04:46<08:31,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4971, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1000/1377 [09:43<03:44,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2465, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1377/1377 [13:27<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 807.1518, 'train_samples_per_second': 13.633, 'train_steps_per_second': 1.706, 'train_loss': 0.2944130381554387, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.2944130381554387, metrics={'train_runtime': 807.1518, 'train_samples_per_second': 13.633, 'train_steps_per_second': 1.706, 'train_loss': 0.2944130381554387, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's train the model, we will use the Trainer class provided by the HG library\n",
    "from transformers import TrainingArguments\n",
    "# honestly, I will just copy paste this part until I get to wrap my head around the 450-line doc string.\n",
    "training_args = TrainingArguments('test-trainer') \n",
    "# the number of labels used for the upcoming task is different from the one used in pretraining\n",
    "# so let's set that number in the model's definition\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "bert = AutoModelForSequenceClassification.from_pretrained(BERT_CHECKPOINT, num_labels=2)\n",
    "\n",
    "# this means that the model will discard the output layer used in pretraining and add a Fully Conncted layer with 2 labels as specified.\n",
    "# the last layer will be randomly initialized\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    bert,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# let's the model train baby\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

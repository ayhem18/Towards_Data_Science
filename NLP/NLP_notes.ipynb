{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week1\n",
    "Basically introduction to the course and main applications of NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2 \n",
    "Let's define some terminology:\n",
    "1. type is an element of the vocabulary. The vocabulary is predefined.\n",
    "2. Token: is an instance of that type in the text: for example cat is a type, cats is a token\n",
    "\n",
    "* Heaps Law determines a relation between the number of types in a vocabulary and the number of tokens:\n",
    "Assuming $N$ tokens and a vocabulary $V$ of size $|V|$, then: \n",
    "$\\begin{align} \n",
    "|V| = K\\cdot N ^ {\\beta}\n",
    "\\end{align}$ \n",
    "where $0.67<\\beta< 0.75$\n",
    "This is an emperical result: The idea is that the number of tokens in signficantly larger for several reasons: each type can be repeated as it is and even in different forms: a verb can be converted to a noun, adjective, or even an adverb. The latter leads to the explosion of the number of tokens. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "* word classes define the role certain words play in speech. They are also referred to as  Part Of Speech tag (POS tag). \n",
    "* Several word classes were defined several centruies before including: noun, verb, prooun, preposition...\n",
    "* Modern day linguistics define multiple tags: certain languages might introduce more than 150 tags\n",
    "* Closed classes: for which no more words can be added: the ***preposition*** class is one example.\n",
    "* Open classes: for which new words are being added over time: Think of the **verb** ***google*** for example\n",
    "* There are two main appproaches when categorizing / creating classes:\n",
    "    1. syntatic / morphological (morpheme is the least mearning-breaning unit in a language) distributional properties\n",
    "    2. semtantic function: words bearing similar meanings\n",
    "* The syntatic rules / properties tend to be favored over semantic ones.\n",
    "* word classes are quite useful for a number of reasons: \n",
    "    1. a word class can significantly limit the possible affixes that word can take\n",
    "    2. a word class can facilitate parsing\n",
    "    3. the word classes are correlated with one another: they reflect context to a certain extent\n",
    "\n",
    "* Considering the language as a set of unique words, it would be safe to state that around $85\\%$ of words are  \"unambigous\" class-wise. Considering words' repetitions, the $15\\%$ left represent more than $60\\%$ of the word tokens. \n",
    "\n",
    "* why is POS tagging challenging ? Well, for starters there is no one-to-one map between a words and POS tag. A word's POS tag varies depending on the sentence. It is not always obvious how to determine a POS tag from the context. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Break\n",
    "LEt's consider one of the most interesting and elegant mathematical models. Hidden states Markov Models. Assuming the following\n",
    "1. We have a hidden (non-directly observable) random variable H, we know the probabilities of transitioning from one variable value to another\n",
    "2. We have an observable random variable R, we know the emissions matrix: the values of the conditional probabilities: $P(R|H)$\n",
    "3. Assuming we have a sequence of values of the random variable $R$: $r_1, r_2, ... r_n$. The model introduces a mechanism to find another sequence $h_1, h_2, .. h_n$ for which the probability: \n",
    "$\n",
    "\\begin{align} \n",
    "P = P(r_1, r_2, ..., r_n, h_1, h_2, ... h_n)\n",
    "\\end{align}\n",
    "$\n",
    "is maximized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The matrix of conditional probabilities of hidden states is called transitions matrix\n",
    "* The matrix of conditional probabilities of observable states on hidden states is called emissions matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well Give me a break for god's sake here. We literally have a probability of $2n$ values, right? Well hold your horses a bit dude. Let's break this expression down:\n",
    "$\n",
    "\\begin{align}\n",
    "P = P(r_n | r_1, r_2, ... r_{n - 1}, h_1, h_2, ... h_n) \\cdot P(r_{n - 1} | r_1, r_2, ... r_{n - 2}, h_1, h_2, ... h_n) ... \\cdot P(r_1 |h_n, h_{n-1}, ... h_1) \\cdot P(h_n|h_1, h_2, ... h_{n - 1}) \\cdot P(h_{n - 1}|h_1, h_2, ... h_{n - 2}) ... \\cdot P(h_2 | h_1)\n",
    "\\end{align}\n",
    "$\n",
    "Well the expression isn't getting any prettier old man!! Care to explain. Well, the markovian hypothesis comes to the rescue: $P(r = r_i)$ is only dependent on the value of $r_{i - 1}$. Thus, the expression simplifies to:\n",
    "$\\begin{align}\n",
    "P = \\prod_{j=1}^{n} P(r_j | h_j) \\cdot \\prod_{j=n}^{2} P(h_j | h_{j - 1}) \\cdot P(h_1 | S)\n",
    "\\end{align}$\n",
    "where $P(h|S)$ is the initial probability distribution of the hidden states.\n",
    "Each term of the $2n$ terms in this simplified expression of $P$ is available and thus the combination of hidden states maximizing can be easily found. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM IN POS TAGGING\n",
    "well HMM can be applied to POS tagging, let's see how that turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to implement it\n",
    "# let's first download data with tagged speech\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\"\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "PATH = os.path.join(os.getcwd(), 'POS_corpus.txt')\n",
    "\n",
    "# let's save the data to a text\n",
    "with open(PATH, 'w') as f:\n",
    "    f.write(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence NN\n",
      "in IN\n",
      "the DT\n",
      "pound NN\n",
      "is VBZ\n",
      "widely RB\n",
      "expected VBN\n",
      "to TO\n",
      "take VB\n",
      "another DT\n",
      "sharp JJ\n",
      "dive NN\n",
      "if IN\n",
      "trade NN\n",
      "figures NNS\n",
      "for IN\n",
      "September NNP\n",
      ", ,\n",
      "due JJ\n",
      "for IN\n"
     ]
    }
   ],
   "source": [
    "# let's what our text have for us\n",
    "with open(PATH, 'r') as f:\n",
    "    for line in f.readlines()[:20]:\n",
    "        print(line[:-1])\n",
    "# great !! so each line represents pair of word and tag\n",
    "# we will take into consideration building the transitions and emissions matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's build a generator from the file in question\n",
    "pairs = (pair[:-1] for pair in open(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build the emission matrix: the probability of having a certain word given its tag\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "# each pair will be mapped to word + \" \" + tag\n",
    "# the matrix can be rebuild from the counter easily\n",
    "\n",
    "\n",
    "def build_emissions_matrix(pairs) -> pd.DataFrame: \n",
    "    \"\"\"Given an iterable of pairs (word and tag), this functions returns the emissions matrix as used in Viterbi algorithm. The function makes use of\n",
    "    the computational enhancements of the pandas package\n",
    "    \n",
    "    Args:\n",
    "        pairs any iterable: an iterable where each item represents a pair: word and associated tag\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: the emissions matrix calculated and saved in a dataframe\n",
    "    \"\"\"\n",
    "    emissions_counter = Counter()\n",
    "\n",
    "    for pair in pairs:\n",
    "        # first make sure to lower case the words\n",
    "        p = re.split(r'\\s+', pair)\n",
    "        if len(p) == 2 and p[0].isalnum():\n",
    "            emissions_counter.update([p[0].lower() + \" \" + p[1]]) # make sure to pass the argument as a list\n",
    "\n",
    "    # the next step is to build the matrix out of the counter\n",
    "    # let's extract the unique set of names\n",
    "    # since the tags' number is quite small\n",
    "    tags_unique = [pair.split()[0] for pair in emissions_counter.keys()]\n",
    "\n",
    "    # create an empty data frame \n",
    "    emissions_df = pd.DataFrame(data=[], index=tags_unique)\n",
    "\n",
    "    # the great thing about using dataframes is that adding a column in not computationally expensive\n",
    "    for pair, pair_count in emissions_counter.items():\n",
    "        # extract the word and the tag\n",
    "        word, tag = pair.split()\n",
    "\n",
    "        # add the column for the word, if the word has not been encountered yet.\n",
    "        if word not in emissions_df.columns: \n",
    "            emissions_df[word] = 0\n",
    "            \n",
    "        # update the value in both cases\n",
    "        emissions_df.at[tag, word] += pair_count\n",
    "\n",
    "    # now emission_df represents tabular data where the columns are words and the rows are the tags\n",
    "    # we want for each cell ('t', 'w') to be the conditional probability P(Word=w|Tag=t)\n",
    "    # in other words the count of the pair ('t', 'w') divided by the total count of the entire occurences of the tag 't' \n",
    "\n",
    "    # calculate the total occurences of each tag\n",
    "    tag_sums = emissions_df.sum()\n",
    "\n",
    "    emissions = emissions_df / tag_sums\n",
    "\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_matrix = build_emissions_matrix(pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, HMM, despite its elegance and simplicity, comes with a number of limitations:\n",
    "1. It (over?)simplies the actual dependencies between the elements of the sequences where it solely considers the transitions between two consecutive elements\n",
    "2. Both transmission and emission probabilities are static. They are invariant with respect to the elements' positions.\n",
    "\n",
    "Unlike HMM, conditional random fields are discriminative models: in other worlds, they are more descriptive rather explanative (or more formally generative as the case with HMM). A CRF can consider any connection between any elements of the sequences. Considering a more extensive set of connections introduces computational overhead. Thus, generally the model used is the Linear Chain CRF as a tradeoff between complexity and computational overhead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "* Introducing grammaticality, A word is grammarly correct if it follows a certain set of rules (a mechanism). This set of rules represents the grammar of language. It is said that grammar generates the language. A syntax makes language useful for communication. IT limits the number of possible forms of a sentence which in turn limits the possible meanings a single sentence can bear.\n",
    "\n",
    "* Some Pragmatic points: \n",
    "1. $N$ is a set of ***non-terminal*** symbols\n",
    "2. $\\Sigma$ is a set of ***terminal*** symbols\n",
    "3. $R$ represents a set of possible productions rules of the form: \n",
    "\n",
    "* More concrete stuff: SO a Phrase Structure Grammar consists mainly of two components:\n",
    "1. a Lexicon: a word class, a category of words\n",
    "2. phrase structure rules: determine how lexicon can be grouped together to build larger phrases\n",
    "\n",
    "* To be verified: a group of at least 2 lexicons is referred to as a ***phrase*** ???\n",
    "* Phrases belonging to the same category exhibit similar statistical distributions and thus similar behavior in terms of context and grammar\n",
    "* The distribution's similarity is tested by the means of substitution: \n",
    "    * Consider the same sentence with two different strings (phrases). Both phrases should have the same state grammarly: both (un)/grammatical "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Probabilistic Models: N-grams\n",
    "DISCLAIMER: My notes are mainly taken from [standford references](https://web.stanford.edu/~jurafsky/slp3/3.pdf).   \n",
    "Why would anyone want to predict, the upcoming words in a given sentence ? That's a great question? Well cause Natural language is inherently ambiguous. The mathematical (objective) approach to handle ambinguity(and hence incertainty) is probability. Such approach is the most effective (currently) which systems such as speech recognition, spelling correction... A \"good\" probabilistic model should understand that the user's statement is much more likely ***The student has been sick*** than ***The student husband sick***\n",
    "### Mathematical Break\n",
    "So how can we model the probability of the following sentence: \"Such happening did seem, in such momoment of crippling despair, beyond the realms of possibility\" ? Good question. Well, No matter how large the corpus is, this very particular sentence (and generally any sentence of certain complexity) wouldn't be repeated enough times for us to build a statistically significant representation. One approach is based on the chain rule of of probability theory: \n",
    "$\n",
    "\\begin{align}\n",
    "P = P(w_1, w_2, ... w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_2, w_1) ... \\cdot P(w_n|w_{n-1}...w_1)\n",
    "\\end{align}\n",
    "$\n",
    "This rule doesn't seem to add much, right ?. Well technically it doesn't, but that's what engineering is about. Approximating phenomena well enough to achieve meaningful results. The main assumption of $N-grams$ model is:\n",
    "$\n",
    "\\begin{align}\n",
    "P(w_n|w_{n - 1}, w_{n - 2}, .. w_1) = P(w_n|w_{n - 1}... w_{n - N + 1})\n",
    "\\end{align}\n",
    "$\n",
    "For the case of a Bi-gram with additional end and start tokens, the following equalities hold:\n",
    "$\n",
    "\\begin{align}\n",
    "P(w_n | w_{n - 1}) = \\frac{C(w_{n - 1}w_n)}{\\sum_{w} C(w_{n - 1}w)} = \\frac{C(w_{n - 1}w_n)}{C(w_{n - 1})}\n",
    "\\end{align}\n",
    "$\n",
    "The same equation can be extended for N-grams in general."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Evaluation\n",
    "There are two main approaches to evaluate a probabilistic model:\n",
    "1. extrinsic: how well the model in question performs on the task at hand using a test dataset (unseen data): how good is the translation / classification...\n",
    "2. intrinsic: perplixity: assuming a sequence $W$ of $N$ words: $w_1, w_2, ... w_{N}$:\n",
    "$\n",
    "\\begin{align}\n",
    "P(W) = P(w_1, w_2, ... w_n) ^ {-\\frac{1}{N}}\n",
    "\\end{align}\n",
    "$\n",
    "It is important to note that comparing two models using intrinsic criteria requires training the two models on the same training vocabulary while having absolutely no prior knowledge of the test dataset. An increase in the **instrinsic** metric **does not guarantee** an improvement in the **extrinsic** one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization and Zeroes\n",
    "* statistical models are quite sensitive to the training corpus. In order to have a concrete improvement, The test and training corpuses must of the same genre (distribution). This dependency is quite heavy in NLP in general.\n",
    "* what about sparsity ? Well a perfectly reasonable N-gram might not be present in a certain training dataset but present in the training one (even with two splits with extremely similar distribution). Such detail introduces two issues:\n",
    "1. It reflects that the model is not general enough, in the sense that it under estimates the probability of certain N-grams: hurting the performance in general\n",
    "2. Cannot mathematically compute the perplexity as a probability of a test set with an unknown word eventually evaluates to $0$. Spoiler alert we can't divide by $0$\n",
    "\n",
    "This issue has several manifestations that should be probably tackled separately:\n",
    "### Completely unknown words\n",
    "This specific issue can be tackled by a number of different approachs:\n",
    "1. build a vocabulary with a predetermined size $|V|$, converting any word out of vocabulary to a predetermined token $UNK$ for example.\n",
    "2. Either choosing a small number $n$ and assigning any word with a frequency lower than $n$ to a $UNK$ token, or probably pick the $V$ most frequent words and assign the rest the $UNK$ token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "### Laplace Smoothing\n",
    "Well, let's first consider the simple case of a unigram with $N$ tokens in the corpus\n",
    "$\n",
    "\\begin{align}\n",
    "P(w_i) = \\frac{c_i}{N}\n",
    "\\end{align}\n",
    "$\n",
    "At least from a mathematical perspective, Laplace smoothing is about converting $0$ counts to $1$, This can be done as follows:\n",
    "$\\begin{align}\n",
    "P(w_i) = \\frac{c_i + 1}{N + V}\n",
    "\\end{align}$\n",
    "Where $V$ is the size of the vocabulary. The same technique can be extended for N-gram in general.  \n",
    "Laplace's smoothing is sometimes referred to as **add-one smoothing**. A natural extension would be **add-K smoothing**:\n",
    "$\n",
    "\\begin{align} \n",
    "P(w_i) = \\frac{c_i + k}{N + k\\cdot V}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification With Naive Bayes\n",
    "Disclaimer: These notes are taken from the 4th chapter of [Stanford Notes](https://web.stanford.edu/~jurafsky/slp3/4.pdf)  \n",
    "classification in general and text classification in particular are quite popular and important tasks. Sentiment analysis, spam classification and even category classification. The general process is simple:\n",
    "* take a piece of text\n",
    "* process it\n",
    "* extract features out of it\n",
    "* pass it through a classifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "Recurrent Neural Networks raised to solve time-series problems. Assuming the input is a sequence of values in time, then the model can use the output of the previous neuron to predict the current output.   \n",
    "The introduction of timve-variant input required several changes, reflected mathematically as follows:\n",
    "$$h_t = g(U \\cdot h_{t - 1} + W \\cdot x_t)$$\n",
    "$$y_t = f(V\\cdot h_t)$$\n",
    "Assuming the following architecture hyperparameters:\n",
    "1. output dimensions: $n_o$\n",
    "2. hidden state dimensions: $n_h$\n",
    "3. input dimensions: $n_i$\n",
    "Then we have:\n",
    "1. V = ($n_o$, $n_h$ )\n",
    "2. U = ($n_h$, $n_h$)\n",
    "3. W = ($n_h$, $n_i$)\n",
    "\n",
    "In addition to the presence of bias units, we can determine the exact number of trainable parameters of an RNN model.\n",
    "\n",
    "The nature of the input requires an incremenet inference mechanism. Nevertheless, with few mathematical manipulations, any intermediate output or hidden state can be written in terms of previous input values $x_{i \\leq t}$. Such experssion can be quite complex\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Rnn as a Language model\n",
    "Since RNN are not bound by the vocabulary and the word occurence in the corpus as it is the case with statistical models: n-grams, and the fixed length of a context as with Forward Neural networks. Therefore, RNN seems like the perfect candidates for Language modeling. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some code please. The code below is taken from the amazing [blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectionel RNNs: are an improvement of the original RNNS. The last hidden state output is generally associated with much more information from the last part of the sequence. To overcome such impediment, it is possible to apply the same architecture but in the opposite order resulting in 2 outputs at each input in the sequence. Each of these outputs are concatenated into a single vector saving much more of the context at each point. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layers in the RNN architecture try to tackle 2 tasks at the time.\n",
    "1. decoding  extracting information useful for the current local part / output of the sequence\n",
    "2. extract useful / important information for the future parts in the sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though theoretically, RNN models are indeed turing complete and theoretically capable of solving the complex problems they are expected to solve, they do not acheive the desired performance in practice. A simple reason is ***vanishing gradient*** meaning the the signal generated from terms far away in the past is lost. The model, thus might not be able to capture connections between elements further away in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the code is already encapsulated in the built-in classes in Pytorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb7bdd38c1fc06c6d44e1bb0cbbf89a8632a48322598c4ff70629b1138073807"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

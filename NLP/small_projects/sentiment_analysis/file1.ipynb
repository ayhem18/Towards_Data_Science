{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preeliminary: imports and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "DATA_PATH = os.path.join(HOME, 'IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all the data at once\n",
    "all_data = pd.read_csv(DATA_PATH)\n",
    "# let's see if data was read correctly\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = all_data['review'], all_data['sentiment']\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11, test_size=0.15, stratify=y) # keep the classes distributions between the two splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(all_data, random_state=11, test_size=0.15, stratify=all_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our two operations led to equivalent results\n",
    "assert (train_df['review'] == X_train).all()\n",
    "assert (test_df['review'] == X_test).all()\n",
    "# save the data\n",
    "train_df.to_csv(os.path.join(HOME, 'train.csv'))\n",
    "test_df.to_csv(os.path.join(HOME, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time for the heavy machinery: Pytorch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "\n",
    "GOOD_PUNCT = list(\"',.?!:\\\"-\")\n",
    "from string import punctuation\n",
    "import re\n",
    "unwanted_regex_comb = r'[#$%&*+\\/;<=>@[\\\\\\]\\^_`{\\|}~]+?[\\w\\s]+[#$%&()*+\\/;<=>@[\\\\\\]^_`{\\|}~]+?' \n",
    "unwanted_punc_regex = r'[#$%&*+\\/;<=>@[\\\\\\]^_`{\\|}~]+'\n",
    "\n",
    "def process_sentence(sentence: str):\n",
    "    sentence = re.sub(unwanted_regex_comb, '', sentence)\n",
    "    sentence = re.sub(unwanted_punc_regex , '', sentence)\n",
    "    # the next point is to remove extra spaces\n",
    "    return re.sub('\\s+', \" \", sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bouab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "NLP = spacy.load(\"en_core_web_md\")\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "english_sw = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def reduce_sentence(sentence: str, nlp_object=None):\n",
    "    if nlp_object is None:\n",
    "        nlp_object = NLP\n",
    "    # tokenize using spacy\n",
    "    document = nlp_object(sentence)                  \n",
    "    embedding = np.array([token.vector for token in document if re.sub( r'\\W+', '',token.text.strip().lower()) not in english_sw])\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence =  X_train[0]\n",
    "sentence = process_sentence(sentence)\n",
    "embeddings = reduce_sentence(sentence)\n",
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, file_path: Union[Path, str], labels_dict:dict, train:bool=True, embedding_length: int=300, sequence_length:int=300):\n",
    "        # need to call the super class constructor\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        # read the data\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "        # a boolean flag to determine whether labels will be returned or not\n",
    "        self.train = train \n",
    "        \n",
    "        # the length of the embedding of a single token\n",
    "        self.embedding_length = embedding_length\n",
    "        \n",
    "        # the length of the input sequence to the model\n",
    "        self.sequence_length = sequence_length\n",
    "        # a varible to map a non necessarily numeric representation to a numerical encoding of the target variable \n",
    "        self.labels_dict = labels_dict\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get the index-th row from the data \n",
    "        review_raw = self.data.iloc[index, :]['review']\n",
    "\n",
    "        if self.train:\n",
    "            label = self.data.iloc[index, :]['sentiment']\n",
    "            \n",
    "        assert isinstance(review_raw, str)\n",
    "        # make sure to process the data\n",
    "        embedded_review = reduce_sentence(process_sentence(review_raw))\n",
    "\n",
    "        # in the light of the length's constraint some additional manipulation is needed for longer sequences         \n",
    "        # embedded_review = np.mean(\n",
    "        #      np.array(\n",
    "        #             [embedded_review[i * self.sequence_length : (i + 1) * self.sequence_length] \n",
    "        #                 for i in range(int(ceil(len(embedded_review) / self.sequence_length)))])\n",
    "        #       , axis=0)    \n",
    "        \n",
    "        # let's define a model as follows\n",
    "        \n",
    "        # take into account the lenght's contrainst\n",
    "        embedded_review = embedded_review[:self.sequence_length]    \n",
    "        # make sure to pad the rest\n",
    "        padding = np.array([[0] * len(embedded_review[0]) for _ in range(0, max(0, self.sequence_length  - len(embedded_review)))])\n",
    "\n",
    "        # concatenate the 2 arrays into a single one\n",
    "        embedded_review = np.concatenate((embedded_review, padding), axis=0)\n",
    "\n",
    "        # next step is to the embedding to a tensor\n",
    "        embedding = torch.from_numpy(embedded_review)\n",
    "\n",
    "        assert torch.is_tensor(embedding)  and embedding.size()[0] == self.sequence_length and embedding.size()[1] == self.embedding_length # make sure the types and shapes are as expected\n",
    "\n",
    "        if self.train:\n",
    "                return embedding, self.labels_dict[label] \n",
    "\n",
    "        return embedding    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = os.path.join(HOME, 'train.csv')\n",
    "TEST_FILE = os.path.join(HOME, 'test.csv')\n",
    "\n",
    "train_dataset = ReviewDataset(file_path=TRAIN_FILE, train=True, labels_dict = {\"positive\":1, \"negative\":0}, sequence_length=400)\n",
    "test_dataset = ReviewDataset(file_path=TEST_FILE, train=False, labels_dict = {\"positive\":1, \"negative\":0}, sequence_length=400)\n",
    "\n",
    "# let's get our data \n",
    "x, y = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to iterate through the dataset\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ReviewLSTM(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_state_dim: int, num_layers: int, dropout_prob: float=0.2, num_classes:int=2):\n",
    "        super.__init__()\n",
    "        self.input_dim = input_dim # the length of the embedding (pretty much determined by the embedding method)\n",
    "        self.hidden_state_dim = hidden_state_dim # the dimension of the hidden state inside of the LSTM\n",
    "        self.num_layers = num_layers # the number of layers stacked on one another in the LSTM\n",
    "        \n",
    "        self.lstm =  nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_state_dim, num_layers=self.num_layers, batch_first=True) # to make sure that the output\n",
    "        # is of the shape (batch, embedding dimension, fixed sequence length)\n",
    "        # determine the number of output units\n",
    "        self.num_outputs = (num_classes if num_classes > 2 else 1)\n",
    "        self.dropout = nn.Dropout(p=max(dropout_prob, 0)) # in case the input is negative\n",
    "        self.output = nn.Linear(in_features=hidden_state_dim, out_features=self.num_outputs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # so first let's pass the data through the LSTM\n",
    "        lstm_out, _ = self.lstm(x) # the outputs arguments return all the final hidden state at each of the intermediate steps in the sequence \n",
    "\n",
    "        # just to make we are extracting the information as it is supposed to be\n",
    "        assert len(lstm_out.shape) == 3 and lstm_out.shape[1] == self.input_dim and lstm_out.shape[2] == self.hidden_state_dim, \\\n",
    "        \"MAKE SURE THE ASSUMPTIONS ABOUT THE LSTM OUTPUT ARE CORRECT !!\"\n",
    "        \n",
    "        #  for simplicity we will solely use the last one\n",
    "        lstm_out = lstm_out[:, -1, :] # the sequence length is the 2nd dimension\n",
    "        \n",
    "        return self.output(self.dropout(lstm_out))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

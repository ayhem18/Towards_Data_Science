{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook was created to save and make notes of the fabulous NLP book:  **Handbook Of Natural Language Processing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifical Approaches\n",
    "## The classical toolkit\n",
    "The classical techniques first tackled a nature language by first decomposing the processing process into different steps / stages:\n",
    "1. tokenization\n",
    "2. lexical analysis\n",
    "3. syntactic analysis\n",
    "4. semantic analysis\n",
    "5. pragmatic analysis\n",
    "\n",
    "From the last layer, we proceed to extract the speaker's intended meaning.  \n",
    "\n",
    "The book dives into each of these $5$ steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "This is defined as the skill of converting text (boiled down to a sequence of bits) into a totally different sequence of meaningful linguistic units. Among the main techniques there is Word Segmentation: dividing a piece of text into different words, this raises the question of what determines the boundaries of a word, sentence segmentation: dividing a piece of text into different sentences. These two techniques are interdependent.  \n",
    "Text preprocessing should consider different aspect of the language such as the writing system. There are $3$ main system:\n",
    "1. logographic: a word is built out of a large number of symbols (some can reach $1000$ symbols)\n",
    "2. syllabic: a symbol represents a syllable (at least two different sounds combined)\n",
    "3. alphabetic: a symbol represents a sound.  \n",
    "\n",
    "Due to the cultural exchange taking place in the modern age, no language is purely based on a single writing system. nevertheless, it is safe to state that English is mostly alphabetic.   \n",
    "### Language identification: The Language dependence Challenge\n",
    "Well this task does not require a heavy machinery of NLP techniques. The first step is to consider the range of characters in the used language (mainly the mapping of the characters to their numerical values in the encoding system). The first step already narrows down the choice. The second part is a bit trickier as several language could share the same alphabet: Arabic and Persian, Swedish and Danish, european languages in general. This further distinction is mainly based on the distribution of the frequencies of these chracters in the specific set.\n",
    "\n",
    "### Corpus dependence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robustness represented a major challenge for the earlier NLP systems. With the explosion of content on the internet, textual data is found in abandance. Nevertheless, the quality did not follow along with the quantity as most piece of text do not follow grammars, punctuations as well as formatting rules that some of the earlier NLP were built upon. It is evident that NLP requires robust algorithms capable of addressing a large number of irregularities associated with 1. difficulty to actually set and formally define rules for producing text 2. the slim likelihood that individuals will actually follow these rules if they ever formalized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## application dependence\n",
    "There is no universal criteria determining what constitutes a word, as the possible definitions of such entity can be greatly vary with respect to the language, context, writing system or even the system actually processing the text. One main example is the contraction: I am $\\rightarrow$ I'm. Different corpora might treat this linguistic entity differently depending on several factors. Therefore, tokens' representation highly depends on the final processing purpose: speech recognition, classification tasks, text generation... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "The challenges associated with segmentation can be explained by two main factors:\n",
    "1. writing system: I have already elaborated on that above\n",
    "2. The type of the language: how words are constructed in terms of the most basic sound / components of the language\n",
    "\n",
    "Ambiguities arise maily when the exact same character: delimiters mainly, have significantly different semantic meaning. The full stop can denote an end of a sentence, an abbreviation or even a part of a numerical representation. Such ambiguities make the tokenization such a challenging task.\n",
    "\n",
    "* Tokenizating over white spaces is not the most reasonable approach as it does not take punctuation marks into account. The latters should be considered in most cases as seperate tokens regardless of their relative position to a weight space.\n",
    "\n",
    "* Even though punctuation marks should be considered as stand-alone tokens, they might as well be attached to certain other tokens. One main example of such exception is the full stop used in abbreviations, the contraction such as ***doesn't*** and ***can't***. One apparent solution is to consider a list of possible abbreviations. Such approach fails in the face of reproducibility of abbreviations as any expression lengthy enough can be abbreviated under certain circumstances. Furthermore, the same abbreviation could stand for several expressions: such as St: saint or street or even state. \n",
    "\n",
    "* One of the most ambigious characters is the \" ' \": apostrophe as it has multiple usages such as expressing the genetive case, contractions: he's doesn't or even the plural of abbreviated expressions: I.D's.\n",
    "* White space is uninformative when tokenizing agglutinative or multi-part words. The hyphen character can also be used to build multi-part words. This is not the sole use case of the hyphen character which introduces ambiguity.\n",
    "* In Unsegmented languages, A 'good' tokenization is even harder to reach as there is not predefined standard for what constitutes a word. Such disagreements manisfest even between native speakers who might have different definitions of the notion of a 'word'. The best-effort approach rely on a deeper understanding of the language in question alongside an extensive list of 'words' in that language might lead to relatively satisfactory results. Nevertheless, the accuracy is highly affected by out-of-vocabulary expressions.\n",
    "\n",
    "* Among the approaches to tackle the tokenization of unsegmented languages, is the maximum length greedy algorithm. The latter is quite effective for languages such as chinease where most words do not exceed 3 characters in length. starting from the first letter, consider the longuest sequence that belong to the corpus starting with that particular character. The procedure continues from the next character after the end of the matching word. Another suggestion is the Inverse maximum length algorithm which is quite efficient itself.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "* This processing step is by no-means less complex or problematic than tokenization. The fewer punctuation marks, the more challenging this step is. Even with language with relatively rich punctuation system, sentence segmentation is quite problematic. Defining a set of punctuation marks as sentence boundaries is only a step towards the solution as the same fullstop could represent the end of a sentence, a part of an abbreviation or even part of a numerical representation. Identifying abbreviation is quite challenging task as it is corpus-dependent.\n",
    "* In well-behaved corpora, rule based approach could lead to pretty good results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding: BPE Tokenizer\n",
    "less talk, more code... The code below is for better understanding and does not represent a general implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from copy import copy\n",
    "\n",
    "# first let's define the define the initial vocabulary produced from the input text / corpus.\n",
    "def build_initial_vocab(text: str):\n",
    "    # the given text will be first tokenized by spaces\n",
    "    tokens = re.split(r'\\s+', text)\n",
    "\n",
    "    # convert each token to lower case, remove extra spaced and filter empty tokens\n",
    "    tokens = [list(t.lower().strip()) for t in tokens if t.lower().strip()]\n",
    "    \n",
    "    # the vocabulary would be a set\n",
    "    vocab = set()\n",
    "    for t in tokens:\n",
    "        vocab.update(t)\n",
    "    \n",
    "    return vocab, tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_new_token(token: list, pair: str):\n",
    "    if len(token) <= 1:\n",
    "        return token\n",
    "    \n",
    "    if token[0] + token[1] == pair:\n",
    "        return [pair] + _get_new_token(token[2:])\n",
    "\n",
    "    pass \n",
    "\n",
    "def next_merge(current_vocab: set, tokens: list, equal_freq=None):\n",
    "    \"\"\"this function adds the most frequent contigent pair of characters to the current vocabulary\n",
    "\n",
    "    Args:\n",
    "        current_vocab (set): the current vocabulary\n",
    "        tokens (list): the corpus represented as a list of tokens\n",
    "        equal_freq: a callable that sorts the most frequent pairs. The pair returned will be the first one.\n",
    "    \"\"\"\n",
    "\n",
    "    # let's create a counter to keep the occurences of different pairs\n",
    "    pairs_counters = Counter()\n",
    "    for t in tokens: \n",
    "        for i in range(len(t) - 1):\n",
    "            pairs_counters.update(t[i] + t[i + 1])\n",
    "    \n",
    "    # determine the most frequent pair(s):\n",
    "    max_count = pairs_counters.most_common()[0][1]\n",
    "    most_freq = []\n",
    "    for v in pairs_counters.most_common():\n",
    "        if v[1] < max_count:\n",
    "            break\n",
    "        else:\n",
    "            most_freq.append(v[0])\n",
    "    \n",
    "    # sort the pairs according to the equal_freq criteria\n",
    "    most_freq = sorted(most_freq, key=equal_freq) # if equal_freq is None, the lexicographical order will be used\n",
    "\n",
    "    # save the chosen pair\n",
    "    final_pair = most_freq[-1]\n",
    "\n",
    "    # merge the frequence pair in the occurences of the tokens\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_tokenizer_num_merges(ini_vocab: set, tokens: list, num_merges:int):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        ini_vocab (set): the initial vocabulary\n",
    "        tokens (list): the tokens extracted from the text\n",
    "        num_merges (int): the number of merges needed\n",
    "    \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ds_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb0ce5cb6b092cde9f0ba713d915425207ed6ea08d3ede97530b87c251a3aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
